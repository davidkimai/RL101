# 2.2 Environment State

PBRFT operates with static, single-shot prompt states that terminate immediately after response generation. Agentic RL models dynamic, evolving world states with partial observations, enabling agents to gather feedback over time, maintain context, and adapt to changing conditions through persistent interaction loops.

## Key Takeaways
- **State Evolution**: Static prompt-response vs dynamic environment tracking
- **Partial Observability**: Agents see subset of full environment state
- **Context Persistence**: Multi-turn memory vs session-independent interactions
- **State Representation**: Structured data vs simple text prompts

## Prerequisites Check

```bash
# Verify environment setup
python -c "import json, uuid, time; print('State management libraries ready')"
python -c "import collections; print('Data structures available')"

# Conceptual prerequisites
echo "Do you understand JSON data structures?"
echo "Are you familiar with state machines or game loops?"
```

## Hands-On: State Management Comparison

### PBRFT: Static State Management
```python
import time
import json
from typing import Dict, Optional

class PBRFTState:
    """Static state for preference-based fine-tuning"""
    
    def __init__(self, prompt: str):
        self.prompt = prompt
        self.timestamp = time.time()
        self.session_id = None  # No session persistence
        self.is_terminal = False
    
    def get_state(self) -> Dict:
        """Return current state - just the prompt"""
        return {
            'type': 'PBRFT',
            'prompt': self.prompt,
            'timestamp': self.timestamp,
            'terminal': self.is_terminal
        }
    
    def update(self, response: str) -> None:
        """PBRFT ends after single response"""
        self.is_terminal = True
        # No state evolution - episode terminates
    
    def get_context_for_model(self) -> str:
        """Model context is just the original prompt"""
        return self.prompt

# Demo: PBRFT state lifecycle
pbrft_state = PBRFTState("What is the capital of France?")
print("Initial state:", json.dumps(pbrft_state.get_state(), indent=2))

# Single interaction
response = "The capital of France is Paris."
pbrft_state.update(response)
print("After response:", json.dumps(pbrft_state.get_state(), indent=2))
print("Context for next interaction:", pbrft_state.get_context_for_model())
print("Session continues:", not pbrft_state.is_terminal)
```

### Agentic RL: Dynamic State Management
```python
import uuid
from collections import deque
from dataclasses import dataclass, field
from typing import List, Dict, Any

@dataclass
class AgenticState:
    """Dynamic state for agentic reinforcement learning"""
    
    # Environment components
    user_query: str
    session_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    step_count: int = 0
    
    # Agent memory and context
    conversation_history: List[Dict] = field(default_factory=list)
    working_memory: Dict = field(default_factory=dict)
    long_term_memory: List[Dict] = field(default_factory=list)
    
    # Tool and environment state
    available_tools: List[str] = field(default_factory=list)
    tool_results: Dict = field(default_factory=dict)
    environment_variables: Dict = field(default_factory=dict)
    
    # Task tracking
    current_task: Optional[str] = None
    task_progress: Dict = field(default_factory=dict)
    is_terminal: bool = False

class AgenticEnvironment:
    """Dynamic environment with evolving state"""
    
    def __init__(self, max_memory_length: int = 50):
        self.max_memory_length = max_memory_length
        self.state = None
        
    def reset(self, initial_query: str) -> AgenticState:
        """Initialize new episode with dynamic state"""
        self.state = AgenticState(
            user_query=initial_query,
            available_tools=['calculator', 'web_search', 'memory_store'],
            current_task='problem_solving',
            environment_variables={
                'difficulty': 'medium',
                'context': 'educational',
                'safety_level': 'high'
            }
        )
        return self.state
    
    def update_state(self, action: Dict, result: Any) -> AgenticState:
        """Update environment state based on agent action"""
        self.state.step_count += 1
        
        # Update conversation history
        interaction = {
            'step': self.state.step_count,
            'action': action,
            'result': result,
            'timestamp': time.time()
        }
        self.state.conversation_history.append(interaction)
        
        # Manage memory length
        if len(self.state.conversation_history) > self.max_memory_length:
            # Move old interactions to long-term memory
            old_interaction = self.state.conversation_history.pop(0)
            self.state.long_term_memory.append(old_interaction)
        
        # Update working memory based on action type
        if action['type'] == 'tool_use':
            tool_name = action['tool']
            self.state.tool_results[tool_name] = result
            self.state.working_memory[f'last_{tool_name}_result'] = result
            
        elif action['type'] == 'reasoning':
            self.state.working_memory['current_reasoning'] = action['content']
            
        # Update task progress
        if action['type'] == 'task_completion':
            self.state.task_progress['completed'] = True
            self.state.is_terminal = True
        
        # Environment variables can change based on agent behavior
        if action.get('unsafe_content', False):
            self.state.environment_variables['safety_level'] = 'warning'
            
        return self.state
    
    def get_partial_observation(self) -> Dict:
        """Return partial observation of full state (POMDP)"""
        # Agent doesn't see everything - partial observability
        return {
            'current_query': self.state.user_query,
            'step': self.state.step_count,
            'recent_history': self.state.conversation_history[-3:],  # Last 3 interactions
            'available_tools': self.state.available_tools,
            'working_memory': self.state.working_memory,
            'task_status': self.state.task_progress,
            'environment_hints': {
                'safety_level': self.state.environment_variables['safety_level']
            }
        }
    
    def get_full_state_for_debug(self) -> Dict:
        """Return full state for debugging/analysis"""
        return {
            'session_id': self.state.session_id,
            'user_query': self.state.user_query,
            'step_count': self.state.step_count,
            'conversation_history': self.state.conversation_history,
            'working_memory': self.state.working_memory,
            'long_term_memory': self.state.long_term_memory,
            'tool_results': self.state.tool_results,
            'environment_variables': self.state.environment_variables,
            'current_task': self.state.current_task,
            'task_progress': self.state.task_progress,
            'is_terminal': self.state.is_terminal
        }

# Demo: Agentic state evolution
env = AgenticEnvironment()
state = env.reset("Help me calculate the compound interest on $1000 at 5% for 3 years")

print("=== Initial State ===")
print("Partial observation:", json.dumps(env.get_partial_observation(), indent=2))

# Simulate multi-step interaction
actions = [
    {'type': 'tool_use', 'tool': 'calculator', 'content': 'compound interest calculation'},
    {'type': 'reasoning', 'content': 'Using compound interest formula A = P(1+r)^t'},
    {'type': 'tool_use', 'tool': 'calculator', 'content': '1000 * (1.05)^3'},
    {'type': 'task_completion', 'content': 'Final answer: $1157.63'}
]

for i, action in enumerate(actions, 1):
    print(f"\n=== Step {i}: {action['type']} ===")
    
    # Simulate result
    if action['type'] == 'tool_use' and action['tool'] == 'calculator':
        result = "1157.63" if '1000' in action['content'] else "calculation performed"
    else:
        result = "reasoning completed"
    
    # Update environment
    env.update_state(action, result)
    
    # Show partial observation (what agent sees)
    obs = env.get_partial_observation()
    print("Agent observes:")
    print(f"  Step: {obs['step']}")
    print(f"  Working memory: {obs['working_memory']}")
    print(f"  Recent history: {len(obs['recent_history'])} interactions")
    print(f"  Task status: {obs['task_status']}")

print(f"\n=== Final State ===")
final_state = env.get_full_state_for_debug()
print(f"Session persisted: {final_state['session_id']}")
print(f"Total interactions: {len(final_state['conversation_history'])}")
print(f"Long-term memory: {len(final_state['long_term_memory'])} items")
print(f"Episode completed: {final_state['is_terminal']}")
```

## State Representation Patterns

### Simple vs Complex State Structures
```python
# PBRFT: Simple state (just text)
pbrft_state_example = {
    'type': 'prompt',
    'content': 'User question here',
    'metadata': {
        'timestamp': '2024-01-01T00:00:00Z',
        'model': 'gpt-3.5-turbo'
    }
}

# Agentic RL: Complex structured state
agentic_state_example = {
    'session': {
        'id': 'sess_12345',
        'user_id': 'user_789',
        'start_time': '2024-01-01T00:00:00Z'
    },
    'conversation': {
        'turn_count': 5,
        'history': [
            {'speaker': 'user', 'content': '...', 'timestamp': '...'},
            {'speaker': 'agent', 'action': '...', 'result': '...', 'timestamp': '...'}
        ]
    },
    'agent_state': {
        'working_memory': {
            'current_task': 'math_problem',
            'intermediate_results': ['step1', 'step2'],
            'confidence': 0.85
        },
        'long_term_memory': [
            {'type': 'fact', 'content': 'user prefers detailed explanations'},
            {'type': 'skill', 'content': 'good at algebra problems'}
        ]
    },
    'environment': {
        'available_tools': ['calculator', 'web_search', 'code_interpreter'],
        'tool_states': {
            'calculator': {'last_result': '42', 'error_count': 0},
            'web_search': {'rate_limit_remaining': 98}
        },
        'external_context': {
            'time_of_day': 'morning',
            'user_timezone': 'PST',
            'suggested_difficulty': 'intermediate'
        }
    },
    'task_tracking': {
        'primary_goal': 'solve_math_problem',
        'sub_goals': ['understand_problem', 'plan_solution', 'execute_calculation'],
        'completion_status': {'understand_problem': True, 'plan_solution': True},
        'estimated_remaining_steps': 2
    }
}

print("PBRFT state size:", len(str(pbrft_state_example)))
print("Agentic state size:", len(str(agentic_state_example)))
print("Complexity ratio:", len(str(agentic_state_example)) / len(str(pbrft_state_example)))
```

### State Transition Patterns
```python
def demonstrate_state_transitions():
    """Show how states evolve differently in PBRFT vs Agentic RL"""
    
    print("=== PBRFT State Transitions ===")
    print("State_0 (prompt) -> State_terminal (done)")
    print("No intermediate states, no memory, no context evolution")
    
    print("\n=== Agentic RL State Transitions ===") 
    print("State_0 -> State_1 -> State_2 -> ... -> State_n")
    print("Rich state evolution with memory accumulation:")
    
    # Simulate state evolution
    states = []
    base_state = {
        'step': 0,
        'memory_items': 0,
        'tools_used': 0,
        'task_progress': 0.0
    }
    
    for step in range(1, 6):
        new_state = base_state.copy()
        new_state.update({
            'step': step,
            'memory_items': step * 2,  # Memory accumulates
            'tools_used': min(step, 3),  # Tools get utilized
            'task_progress': min(step / 5.0, 1.0)  # Progress toward goal
        })
        states.append(new_state)
        print(f"  Step {step}: memory={new_state['memory_items']}, "
              f"tools={new_state['tools_used']}, "
              f"progress={new_state['task_progress']:.1%}")
    
    return states

demonstrate_state_transitions()
```

## Partial Observability in Practice

### What the Agent Sees vs Full State
```python
class PartialObservabilityDemo:
    """Demonstrate partial observability in agentic environments"""
    
    def __init__(self):
        # Full environment state (hidden from agent)
        self.full_state = {
            'user_true_intent': 'learn_about_reinforcement_learning',
            'user_background': 'beginner_programmer',
            'user_mood': 'curious_but_impatient',
            'system_load': 0.3,
            'rate_limits': {'api_calls_remaining': 847},
            'safety_flags': {'inappropriate_content': False},
            'conversation_context': 'educational_setting',
            'time_constraints': {'max_session_length': '30_minutes'},
            'available_resources': ['docs', 'examples', 'tutorials'],
            'system_capabilities': ['text_gen', 'code_exec', 'web_search']
        }
    
    def get_agent_observation(self) -> Dict:
        """What the agent actually observes (partial state)"""
        return {
            'user_query': 'Can you explain reinforcement learning?',
            'conversation_turn': 1,
            'available_tools': ['web_search', 'code_interpreter'],
            'system_status': 'normal',
            'context_hint': 'educational'
            # Note: Missing user_mood, true_intent, rate_limits, etc.
        }
    
    def agent_must_infer(self) -> List[str]:
        """What agent must infer from limited observations"""
        return [
            "User's background level",
            "User's true intent and goals",
            "User's preferred learning style",
            "Time constraints and urgency",
            "System resource limitations",
            "Appropriate response complexity"
        ]

# Demo partial observability
demo = PartialObservabilityDemo()
print("=== What Agent Observes ===")
print(json.dumps(demo.get_agent_observation(), indent=2))

print("\n=== What Agent Must Infer ===")
for item in demo.agent_must_infer():
    print(f"  - {item}")

print("\n=== Hidden Full State ===")
print("Full state has", len(demo.full_state), "dimensions")
print("Agent sees", len(demo.get_agent_observation()), "dimensions")
print("Observability ratio:", 
      len(demo.get_agent_observation()) / len(demo.full_state))
```

## ASCII Diagrams: State Evolution

```
PBRFT State Lifecycle:
┌─────────────┐    ┌──────────────┐
│   Prompt    │───►│   Terminal   │
│  (Static)   │    │   (Done)     │
└─────────────┘    └──────────────┘
     t=0                t=1

Agentic RL State Evolution:
┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐
│  State₀ │───►│  State₁ │───►│  State₂ │───►│  State₃ │
│ memory=∅│    │ memory≠∅│    │ memory++│    │ memory++│
│ tools=∅ │    │ tools=1 │    │ tools=2 │    │ tools=n │
└─────────┘    └─────────┘    └─────────┘    └─────────┘
    t=0           t=1           t=2           t=n

Memory Evolution:
t=0: []
t=1: [interaction₁]  
t=2: [interaction₁, interaction₂]
t=3: [interaction₁, interaction₂, interaction₃, ...]

Tool State Evolution:
t=0: {tools_available: [calc, web, mem]}
t=1: {calc: {used: true, result: "42"}, web: unused, mem: unused}
t=2: {calc: cached, web: {used: true, results: [...]}, mem: unused}
```

## State Management Best Practices

### Secure State Handling
```python
class SecureStateManager:
    """Production-ready state management with security"""
    
    def __init__(self):
        self.state_validation_rules = {
            'max_memory_items': 1000,
            'max_conversation_length': 50,
            'allowed_tools': ['calculator', 'web_search', 'memory'],
            'forbidden_keys': ['password', 'secret', 'private_key']
        }
    
    def validate_state_update(self, state_update: Dict) -> bool:
        """Validate state updates for security and bounds"""
        # Check memory bounds
        if 'memory' in state_update:
            if len(state_update['memory']) > self.state_validation_rules['max_memory_items']:
                return False
        
        # Check for forbidden content
        state_str = str(state_update).lower()
        for forbidden in self.state_validation_rules['forbidden_keys']:
            if forbidden in state_str:
                return False
                
        return True
    
    def sanitize_state(self, state: Dict) -> Dict:
        """Remove sensitive information from state"""
        safe_state = state.copy()
        
        # Remove any keys containing sensitive info
        sensitive_patterns = ['password', 'token', 'key', 'secret']
        for key in list(safe_state.keys()):
            if any(pattern in key.lower() for pattern in sensitive_patterns):
                safe_state.pop(key, None)
        
        return safe_state

# Demo secure state management
secure_mgr = SecureStateManager()

# Safe state update
safe_update = {'memory': ['interaction1', 'interaction2'], 'step': 3}
print("Safe update valid:", secure_mgr.validate_state_update(safe_update))

# Unsafe state update  
unsafe_update = {'memory': ['data'] * 2000, 'api_key': 'secret123'}
print("Unsafe update valid:", secure_mgr.validate_state_update(unsafe_update))

print("Sanitized state:", secure_mgr.sanitize_state(unsafe_update))
```

## Key Differences Summary

| Aspect | PBRFT State | Agentic RL State |
|--------|-------------|------------------|
| **Structure** | Simple text prompt | Complex nested dictionary |
| **Evolution** | Static (no change) | Dynamic (continuous updates) |
| **Memory** | None | Episodic + working + long-term |
| **Tools** | No tool state | Tool status and results |
| **Persistence** | Single interaction | Multi-turn sessions |
| **Observability** | Full (prompt visible) | Partial (hidden environment) |
| **Context** | Request-only | Rich contextual tracking |

## Practical Exercises

```python
# Exercise 1: Implement state evolution
def exercise_state_evolution():
    """Create a simple state that evolves over 5 steps"""
    # Your implementation here
    pass

# Exercise 2: Design partial observability
def exercise_partial_observability():
    """Create full state and partial observation function"""
    # Your implementation here
    pass

# Exercise 3: Memory management
def exercise_memory_management():
    """Implement memory with size limits and persistence"""
    # Your implementation here
    pass
```

## Resources

- **Survey Reference**: [Section 2.2, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **POMDP Tutorial**: [Partial Observability Concepts](https://www.pomdp.org/tutorial/)
- **State Management**: [Gymnasium Environment API](https://gymnasium.farama.org/api/env/)
- **Implementation Guide**: [OpenAI Gym Custom Environments](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)

## Next Steps

- **[2.3 Action Space](2.3_Action_Space.md)**: Learn hybrid action spaces (text + tools)
- **Practice**: Implement a simple stateful environment for your domain
- **Deep Dive**: Study POMDP literature for advanced partial observability techniques

---

*Dynamic state management is fundamental to agentic behavior. The rich state representations enable memory, planning, and tool coordination that static prompt-response cannot achieve.*
