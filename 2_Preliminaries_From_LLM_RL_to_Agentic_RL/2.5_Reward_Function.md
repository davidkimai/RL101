# 2.5 Reward Function

PBRFT relies on final preference scores from verifiers or human raters, providing single terminal signals. Agentic RL designs multi-step reward functions combining sparse task outcomes with dense process rewards and auxiliary learning signals, enabling fine-grained credit assignment across long trajectories while avoiding reward hacking.

## Key Takeaways
- **Single vs Multi-step**: Terminal preference scores vs trajectory-based reward sequences
- **Reward Density**: Sparse goal achievement + dense process guidance + auxiliary signals
- **Credit Assignment**: Precise attribution of success/failure across multi-step decisions
- **Reward Hacking Prevention**: Robust reward design that captures true objectives

## Prerequisites Check

```bash
# Verify numerical libraries
python -c "import numpy as np; print('NumPy ready for reward calculations')"
python -c "import matplotlib.pyplot as plt; print('Plotting ready for reward visualization')"

# Conceptual check
echo "Do you understand the difference between sparse and dense rewards?"
echo "Are you familiar with credit assignment problems in RL?"
```

## Hands-On: Reward Design Comparison

### PBRFT: Terminal Preference Rewards
```python
import numpy as np
from typing import Dict, List, Tuple

class PBRFTRewardFunction:
    """Terminal preference-based rewards for PBRFT"""
    
    def __init__(self):
        self.reward_type = "terminal_preference"
        self.calls_made = 0
    
    def compute_reward(self, prompt: str, response: str) -> float:
        """Single reward at episode end based on preference model"""
        self.calls_made += 1
        
        # Simulate preference model scoring
        reward_components = {
            'helpfulness': self.score_helpfulness(response),
            'harmlessness': self.score_harmlessness(response), 
            'honesty': self.score_honesty(response)
        }
        
        # Weighted combination
        weights = {'helpfulness': 0.4, 'harmlessness': 0.4, 'honesty': 0.2}
        total_reward = sum(weights[k] * v for k, v in reward_components.items())
        
        return total_reward
    
    def score_helpfulness(self, response: str) -> float:
        """Simplified helpfulness scoring"""
        if len(response) < 10:
            return 0.0
        if "I don't know" in response:
            return 0.3
        if any(word in response.lower() for word in ['help', 'assist', 'solve']):
            return 0.8
        return 0.5
    
    def score_harmlessness(self, response: str) -> float:
        """Basic safety scoring"""
        harmful_patterns = ['hack', 'illegal', 'dangerous', 'hurt']
        if any(pattern in response.lower() for pattern in harmful_patterns):
            return 0.0
        return 1.0
    
    def score_honesty(self, response: str) -> float:
        """Truthfulness estimation"""
        uncertain_phrases = ["I'm not sure", "might be", "probably"]
        if any(phrase in response for phrase in uncertain_phrases):
            return 0.9  # Reward uncertainty acknowledgment
        return 0.7

# Demo PBRFT rewards
pbrft_rewards = PBRFTRewardFunction()

examples = [
    ("What's 2+2?", "2+2 equals 4."),
    ("How do I hack a computer?", "I can't help with illegal activities."),
    ("Explain photosynthesis", "I'm not sure about the exact details, but photosynthesis involves plants converting sunlight to energy.")
]

print("=== PBRFT Reward Examples ===")
for prompt, response in examples:
    reward = pbrft_rewards.compute_reward(prompt, response)
    print(f"Prompt: {prompt[:30]}...")
    print(f"Response: {response[:50]}...")
    print(f"Terminal Reward: {reward:.3f}\n")
```

### Agentic RL: Multi-Step Reward Design
```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class AgenticStep:
    """Single step in agentic trajectory"""
    action_type: str
    action_content: str
    tool_used: Optional[str] = None
    tool_success: bool = True
    progress_made: float = 0.0
    step_number: int = 0

class AgenticRewardFunction:
    """Multi-component reward for agentic trajectories"""
    
    def __init__(self):
        self.reward_components = {
            'task_completion': 0.5,    # Sparse, high-value
            'process_quality': 0.3,    # Dense, guidance
            'tool_efficiency': 0.1,    # Auxiliary signal
            'safety_compliance': 0.1   # Safety constraint
        }
        
    def compute_step_reward(self, step: AgenticStep, context: Dict) -> Dict[str, float]:
        """Compute reward for single step in trajectory"""
        rewards = {}
        
        # Task completion (sparse)
        if step.progress_made >= 0.9:
            rewards['task_completion'] = 100.0  # High sparse reward
        elif step.progress_made > context.get('previous_progress', 0):
            rewards['task_completion'] = 10.0   # Progress reward
        else:
            rewards['task_completion'] = 0.0
            
        # Process quality (dense)
        rewards['process_quality'] = self.score_process_quality(step)
        
        # Tool efficiency (auxiliary)
        rewards['tool_efficiency'] = self.score_tool_efficiency(step, context)
        
        # Safety compliance (constraint)
        rewards['safety_compliance'] = self.score_safety(step)
        
        return rewards
    
    def score_process_quality(self, step: AgenticStep) -> float:
        """Dense process rewards for good reasoning"""
        base_score = 1.0
        
        if step.action_type == 'reasoning':
            if len(step.action_content) > 50:  # Detailed reasoning
                base_score += 2.0
            if 'because' in step.action_content.lower():  # Causal reasoning
                base_score += 1.0
                
        elif step.action_type == 'tool_use':
            if step.tool_success:
                base_score += 3.0  # Successful tool use
            else:
                base_score -= 1.0  # Failed tool use penalty
                
        elif step.action_type == 'text_response':
            if any(word in step.action_content.lower() for word in ['step', 'first', 'then']):
                base_score += 1.5  # Structured response
                
        return base_score
    
    def score_tool_efficiency(self, step: AgenticStep, context: Dict) -> float:
        """Auxiliary rewards for efficient tool use"""
        if step.action_type != 'tool_use':
            return 0.0
            
        efficiency_score = 0.0
        tools_used = context.get('tools_used_count', {})
        
        # Reward appropriate tool selection
        if step.tool_used == 'calculator' and 'math' in context.get('task_type', ''):
            efficiency_score += 2.0
        elif step.tool_used == 'web_search' and 'research' in context.get('task_type', ''):
            efficiency_score += 2.0
            
        # Penalize redundant tool use
        tool_count = tools_used.get(step.tool_used, 0)
        if tool_count > 2:
            efficiency_score -= 1.0
            
        return efficiency_score
    
    def score_safety(self, step: AgenticStep) -> float:
        """Safety constraint rewards"""
        safety_violations = [
            'delete', 'rm -rf', 'format', 'hack', 'exploit',
            'password', 'private', 'confidential'
        ]
        
        content = step.action_content.lower()
        if any(violation in content for violation in safety_violations):
            return -10.0  # Strong penalty for safety violations
            
        return 0.0  # Neutral for safe actions
    
    def compute_trajectory_reward(self, trajectory: List[AgenticStep]) -> Dict:
        """Compute total reward for complete trajectory"""
        total_rewards = {component: 0.0 for component in self.reward_components}
        step_rewards = []
        
        context = {
            'previous_progress': 0.0,
            'tools_used_count': {},
            'task_type': 'math_problem'
        }
        
        for step in trajectory:
            step_reward = self.compute_step_reward(step, context)
            step_rewards.append(step_reward)
            
            # Update context
            context['previous_progress'] = max(context['previous_progress'], step.progress_made)
            if step.tool_used:
                context['tools_used_count'][step.tool_used] = \
                    context['tools_used_count'].get(step.tool_used, 0) + 1
            
            # Accumulate weighted rewards
            for component, value in step_reward.items():
                if component in total_rewards:
                    total_rewards[component] += value * self.reward_components[component]
        
        # Final trajectory reward
        final_reward = sum(total_rewards.values())
        
        return {
            'total_reward': final_reward,
            'component_rewards': total_rewards,
            'step_rewards': step_rewards,
            'trajectory_length': len(trajectory)
        }

# Demo Agentic RL rewards
agentic_rewards = AgenticRewardFunction()

# Example trajectory: solving math problem
trajectory = [
    AgenticStep('reasoning', 'I need to solve 15% of 240. Let me break this down.', 
                progress_made=0.1, step_number=1),
    AgenticStep('tool_use', 'calculator', 'calculator', True, 
                progress_made=0.5, step_number=2),
    AgenticStep('reasoning', 'The calculation gives 36 because 0.15 * 240 = 36', 
                progress_made=0.8, step_number=3),
    AgenticStep('text_response', 'The answer is 36. Here\'s how: 15% = 0.15, and 0.15 × 240 = 36.', 
                progress_made=1.0, step_number=4)
]

result = agentic_rewards.compute_trajectory_reward(trajectory)

print("=== Agentic RL Reward Breakdown ===")
print(f"Total Reward: {result['total_reward']:.2f}")
print(f"Trajectory Length: {result['trajectory_length']} steps")
print("\nComponent Breakdown:")
for component, value in result['component_rewards'].items():
    print(f"  {component}: {value:.2f}")

print("\nStep-by-Step Rewards:")
for i, step_reward in enumerate(result['step_rewards'], 1):
    print(f"  Step {i}: {sum(step_reward.values()):.2f}")
```

## Reward Design Patterns

### Reward Shaping Strategies
```python
class RewardShapingStrategies:
    """Common patterns for designing multi-step rewards"""
    
    @staticmethod
    def sparse_plus_dense(base_reward: float, progress: float) -> float:
        """Combine sparse task reward with dense progress signals"""
        if progress >= 1.0:
            return base_reward + 100  # Large sparse completion bonus
        else:
            return base_reward + (progress * 10)  # Dense progress reward
    
    @staticmethod
    def curriculum_rewards(step_number: int, difficulty: str) -> float:
        """Adjust rewards based on curriculum progression"""
        difficulty_multipliers = {'easy': 0.5, 'medium': 1.0, 'hard': 2.0}
        base = difficulty_multipliers.get(difficulty, 1.0)
        
        # Early steps get higher rewards for encouragement
        early_bonus = max(0, (10 - step_number) * 0.1)
        return base + early_bonus
    
    @staticmethod
    def safety_constrained_rewards(base_reward: float, safety_score: float) -> float:
        """Apply safety constraints to reward signals"""
        if safety_score < 0:
            return safety_score  # Safety violations override all other rewards
        return base_reward * (1 + safety_score * 0.1)  # Safety bonus
    
    @staticmethod
    def exploration_bonuses(action: str, state_visits: Dict[str, int]) -> float:
        """Reward exploration of new states/actions"""
        state_key = f"action_{action}"
        visit_count = state_visits.get(state_key, 0)
        
        # Diminishing exploration bonus
        if visit_count == 0:
            return 5.0  # First time bonus
        elif visit_count < 3:
            return 2.0  # Early exploration
        else:
            return 0.0  # No bonus for repeated actions

# Demo reward shaping
print("=== Reward Shaping Examples ===")
shaping = RewardShapingStrategies()

# Example scenarios
scenarios = [
    {'progress': 0.3, 'difficulty': 'easy', 'step': 2, 'safety': 0.1},
    {'progress': 1.0, 'difficulty': 'hard', 'step': 8, 'safety': 0.0},
    {'progress': 0.7, 'difficulty': 'medium', 'step': 15, 'safety': -5.0}
]

for i, scenario in enumerate(scenarios, 1):
    print(f"\nScenario {i}:")
    
    sparse_dense = shaping.sparse_plus_dense(1.0, scenario['progress'])
    curriculum = shaping.curriculum_rewards(scenario['step'], scenario['difficulty'])
    safety = shaping.safety_constrained_rewards(sparse_dense, scenario['safety'])
    
    print(f"  Progress: {scenario['progress']:.1f}, Difficulty: {scenario['difficulty']}")
    print(f"  Sparse+Dense: {sparse_dense:.2f}")
    print(f"  Curriculum: {curriculum:.2f}") 
    print(f"  Safety-Constrained: {safety:.2f}")
```

### Reward Hacking Prevention
```python
class RewardHackingPrevention:
    """Strategies to prevent reward hacking in agentic systems"""
    
    def __init__(self):
        self.validation_checks = [
            'reward_magnitude_check',
            'action_diversity_check', 
            'progress_consistency_check',
            'safety_violation_check'
        ]
    
    def validate_reward_trajectory(self, rewards: List[float], actions: List[str]) -> Dict:
        """Detect potential reward hacking patterns"""
        validation_results = {}
        
        # Check for suspiciously high rewards
        validation_results['magnitude_check'] = self.reward_magnitude_check(rewards)
        
        # Check for action exploitation patterns
        validation_results['diversity_check'] = self.action_diversity_check(actions)
        
        # Check for inconsistent progress patterns  
        validation_results['consistency_check'] = self.progress_consistency_check(rewards)
        
        # Overall assessment
        validation_results['likely_hacking'] = any(
            result.get('suspicious', False) for result in validation_results.values()
        )
        
        return validation_results
    
    def reward_magnitude_check(self, rewards: List[float]) -> Dict:
        """Detect abnormally high rewards"""
        if not rewards:
            return {'suspicious': False, 'reason': 'no_rewards'}
            
        mean_reward = np.mean(rewards)
        max_reward = max(rewards)
        
        # Suspicious if max reward is >5x mean or >100 absolute
        magnitude_suspicious = (max_reward > 5 * mean_reward) or (max_reward > 100)
        
        return {
            'suspicious': magnitude_suspicious,
            'mean_reward': mean_reward,
            'max_reward': max_reward,
            'reason': 'abnormally_high_rewards' if magnitude_suspicious else 'normal'
        }
    
    def action_diversity_check(self, actions: List[str]) -> Dict:
        """Detect repetitive action exploitation"""
        if not actions:
            return {'suspicious': False, 'reason': 'no_actions'}
            
        from collections import Counter
        action_counts = Counter(actions)
        most_common_action, max_count = action_counts.most_common(1)[0]
        
        # Suspicious if >70% of actions are the same
        diversity_suspicious = (max_count / len(actions)) > 0.7
        
        return {
            'suspicious': diversity_suspicious,
            'most_common_action': most_common_action,
            'repetition_rate': max_count / len(actions),
            'reason': 'excessive_repetition' if diversity_suspicious else 'diverse'
        }
    
    def progress_consistency_check(self, rewards: List[float]) -> Dict:
        """Check for implausible progress patterns"""
        if len(rewards) < 3:
            return {'suspicious': False, 'reason': 'insufficient_data'}
            
        # Look for sudden reward spikes without gradual buildup
        reward_changes = [rewards[i+1] - rewards[i] for i in range(len(rewards)-1)]
        large_jumps = [change for change in reward_changes if change > 20]
        
        # Suspicious if >2 large jumps in short trajectory
        consistency_suspicious = len(large_jumps) > 2
        
        return {
            'suspicious': consistency_suspicious,
            'large_jumps': len(large_jumps),
            'max_jump': max(reward_changes) if reward_changes else 0,
            'reason': 'inconsistent_progress' if consistency_suspicious else 'consistent'
        }

# Demo reward hacking detection
hack_detector = RewardHackingPrevention()

# Test cases
test_trajectories = [
    # Normal trajectory
    {
        'rewards': [1.0, 1.5, 2.0, 3.5, 8.0, 15.0],
        'actions': ['reason', 'tool_use', 'reason', 'tool_use', 'text', 'complete']
    },
    # Suspicious: repetitive high rewards
    {
        'rewards': [1.0, 50.0, 50.0, 50.0, 50.0, 50.0],
        'actions': ['tool_use', 'tool_use', 'tool_use', 'tool_use', 'tool_use', 'tool_use']
    }
]

print("=== Reward Hacking Detection ===")
for i, trajectory in enumerate(test_trajectories, 1):
    print(f"\nTrajectory {i}:")
    validation = hack_detector.validate_reward_trajectory(
        trajectory['rewards'], trajectory['actions']
    )
    print(f"  Likely Hacking: {validation['likely_hacking']}")
    
    for check_name, result in validation.items():
        if check_name != 'likely_hacking' and isinstance(result, dict):
            print(f"  {check_name}: {result['reason']} (suspicious: {result['suspicious']})")
```

## ASCII Diagram: Reward Design Comparison

```
PBRFT Rewards:
┌─────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Prompt    │───►│     Response     │───►│ Terminal Reward │
│     +       │    │        +         │    │  (Preference)   │
│ Response    │    │ Preference Model │    │     R_final     │
└─────────────┘    └──────────────────┘    └─────────────────┘
                            │
                            ▼
                   Single reward signal
                   No temporal structure

Agentic RL Rewards:
┌─────┐    ┌─────┐    ┌─────┐    ┌─────┐    ┌─────┐
│ S₁  │───►│ S₂  │───►│ S₃  │───►│ S₄  │───►│ S₅  │
└─────┘    └─────┘    └─────┘    └─────┘    └─────┘
   │          │          │          │          │
   ▼          ▼          ▼          ▼          ▼
  R₁         R₂         R₃         R₄         R₅
   │          │          │          │          │
Dense      Process    Auxiliary   Safety    Sparse
Progress   Quality   Efficiency  Check   Completion

Multi-Component Reward Structure:
┌─────────────────┐    ┌────────────────┐    ┌─────────────────┐
│  Task Progress  │    │ Process Quality│    │ Safety & Aux    │
│   (Sparse)      │    │    (Dense)     │    │  (Constraints)  │
├─────────────────┤    ├────────────────┤    ├─────────────────┤
│• Goal achieved  │    │• Tool use      │    │• Safety check   │
│• Milestone      │    │• Reasoning     │    │• Efficiency     │
│  reached        │    │• Structure     │    │• Exploration    │
└─────────────────┘    └────────────────┘    └─────────────────┘
        │                      │                      │
        └──────────────────────┼──────────────────────┘
                               ▼
                    ┌─────────────────────┐
                    │  Weighted Final     │
                    │  Trajectory Reward  │
                    │  R = Σᵢ wᵢ × Rᵢ(t)  │
                    └─────────────────────┘
```

## Key Differences Summary

| Aspect | PBRFT Rewards | Agentic RL Rewards |
|--------|---------------|---------------------|
| **Timing** | Terminal only | Multi-step sequence |
| **Density** | Single sparse signal | Sparse + dense + auxiliary |
| **Components** | Preference score | Task + process + safety + efficiency |
| **Credit Assignment** | N/A (single step) | Complex temporal attribution |
| **Hacking Prevention** | Content filtering | Multi-layered validation |
| **Learning Signal** | Response quality | Sequential decision quality |
| **Exploration** | Not applicable | Exploration bonuses included |

## Practical Exercises

```python
# Exercise 1: Design domain-specific rewards
def exercise_domain_rewards():
    """Create reward function for coding assistant"""
    # Components: code correctness, style, efficiency, safety
    pass

# Exercise 2: Implement reward shaping  
def exercise_reward_shaping():
    """Add curriculum and exploration bonuses"""
    pass

# Exercise 3: Build hacking detection
def exercise_hacking_detection():
    """Create validation pipeline for your domain"""
    pass
```

## Resources

- **Survey Reference**: [Section 2.5, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **Reward Shaping**: [Ng, Harada, Russell - Policy Invariance Under Reward Transformations](https://people.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf)
- **Reward Hacking**: [Amodei et al. - Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565)
- **Multi-Objective RL**: [Roijers, Vamplew - A Survey of Multi-Objective Sequential Decision-Making](https://jair.org/index.php/jair/article/view/10987)

## Next Steps

- **[2.6 Learning Objective](2.6_Learning_Objective.md)**: Long-term vs immediate optimization
- **Practice**: Design multi-component reward for your specific application
- **Deep Dive**: Study reward hacking cases in production RL systems

---

*Multi-step reward design is crucial for agentic behavior. The balance of sparse, dense, and auxiliary signals guides agents toward robust, safe, and efficient problem-solving strategies.*
