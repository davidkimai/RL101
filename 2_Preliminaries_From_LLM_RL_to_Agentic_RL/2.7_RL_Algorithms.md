# 2.7 RL Algorithms

Core algorithm families bridge the gap between PBRFT and Agentic RL objectives. This module covers policy gradients (REINFORCE), PPO variants, DPO-style preference optimization, and GRPO derivatives, examining how each handles the tradeoffs between stability, compute efficiency, and signal design critical for tool integration, memory, and multi-turn planning.

## Key Takeaways
- **Algorithm Families**: Policy gradients, actor-critic, preference optimization, and group-relative methods
- **Stability vs Efficiency**: PPO provides stable updates while DPO eliminates reward model dependency  
- **Multi-step Adaptation**: Algorithms must handle temporal credit assignment and tool integration
- **Practical Implementation**: Focus on runnable examples for each major algorithm class

## Prerequisites Check

```bash
# Verify ML libraries
python -c "import torch; print('PyTorch ready for policy optimization')"
python -c "import numpy as np; print('NumPy ready for algorithm implementation')"

# Conceptual check
echo "Do you understand basic gradient descent and policy gradients?"
echo "Are you familiar with the bias-variance tradeoff in RL?"
```

## Hands-On: Algorithm Family Implementations

### REINFORCE: Basic Policy Gradient
```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Dict, Tuple

class SimplePolicy(nn.Module):
    """Simple policy network for REINFORCE"""
    def __init__(self, vocab_size: int, hidden_size: int = 128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.output = nn.Linear(hidden_size, vocab_size)
        
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        logits = self.output(lstm_out)
        return logits

class REINFORCEAgent:
    """REINFORCE algorithm for policy optimization"""
    
    def __init__(self, vocab_size: int, lr: float = 0.01):
        self.policy = SimplePolicy(vocab_size)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.saved_log_probs = []
        self.rewards = []
        
    def select_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:
        """Select action and save log probability"""
        logits = self.policy(state.unsqueeze(0))
        probs = torch.softmax(logits[0, -1], dim=0)
        action = torch.multinomial(probs, 1)
        log_prob = torch.log(probs[action])
        
        self.saved_log_probs.append(log_prob)
        return action.item(), log_prob
    
    def update_policy(self, gamma: float = 0.99):
        """REINFORCE policy update"""
        if not self.rewards:
            return 0.0
            
        # Compute discounted returns
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + gamma * R
            returns.insert(0, R)
        
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # Normalize
        
        # Compute policy gradient
        policy_loss = []
        for log_prob, R in zip(self.saved_log_probs, returns):
            policy_loss.append(-log_prob * R)
        
        policy_loss = torch.stack(policy_loss).sum()
        
        # Update policy
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()
        
        # Clear episode data
        loss_val = policy_loss.item()
        self.saved_log_probs.clear()
        self.rewards.clear()
        
        return loss_val

# Demo REINFORCE
print("=== REINFORCE Algorithm ===")
agent = REINFORCEAgent(vocab_size=1000)

# Simulate simple episode
state = torch.randint(0, 1000, (5,))  # Simple state representation
rewards = [1.0, 2.0, 3.0, 10.0]  # Episode rewards

for reward in rewards:
    action, log_prob = agent.select_action(state)
    agent.rewards.append(reward)
    print(f"Action: {action}, Log prob: {log_prob.item():.4f}, Reward: {reward}")

loss = agent.update_policy()
print(f"Policy loss: {loss:.4f}")
```

### PPO: Proximal Policy Optimization  
```python
class PPOAgent:
    """PPO with clipping for stable policy updates"""
    
    def __init__(self, vocab_size: int, lr: float = 0.001, clip_epsilon: float = 0.2):
        self.policy = SimplePolicy(vocab_size)
        self.value_net = nn.Sequential(
            nn.Linear(vocab_size, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)
        self.clip_epsilon = clip_epsilon
        
    def compute_gae(self, rewards: List[float], values: List[float], 
                    gamma: float = 0.99, lam: float = 0.95) -> Tuple[List[float], List[float]]:
        """Compute Generalized Advantage Estimation"""
        advantages = []
        returns = []
        
        gae = 0
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1]
                
            delta = rewards[i] + gamma * next_value - values[i]
            gae = delta + gamma * lam * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[i])
            
        return advantages, returns
    
    def update_policy(self, states: torch.Tensor, actions: torch.Tensor, 
                     old_log_probs: torch.Tensor, advantages: torch.Tensor,
                     returns: torch.Tensor, epochs: int = 4):
        """PPO policy update with clipping"""
        
        for _ in range(epochs):
            # Current policy evaluation
            logits = self.policy(states)
            probs = torch.softmax(logits, dim=-1)
            current_log_probs = torch.log(probs.gather(-1, actions.unsqueeze(-1))).squeeze()
            
            # Ratio of new to old policy
            ratio = torch.exp(current_log_probs - old_log_probs)
            
            # Clipped policy objective
            obj1 = ratio * advantages
            obj2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            policy_loss = -torch.min(obj1, obj2).mean()
            
            # Value function loss
            values = self.value_net(states.float()).squeeze()
            value_loss = nn.MSELoss()(values, returns)
            
            # Update networks
            self.policy_optimizer.zero_grad()
            policy_loss.backward(retain_graph=True)
            self.policy_optimizer.step()
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()
        
        return policy_loss.item(), value_loss.item()

# Demo PPO  
print("\n=== PPO Algorithm ===")
ppo_agent = PPOAgent(vocab_size=1000)

# Simulate batch of experience
batch_size = 4
states = torch.randint(0, 1000, (batch_size, 5))
actions = torch.randint(0, 1000, (batch_size,))
old_log_probs = torch.randn(batch_size) * 0.1
advantages = torch.randn(batch_size)
returns = torch.randn(batch_size) + 5

policy_loss, value_loss = ppo_agent.update_policy(states, actions, old_log_probs, advantages, returns)
print(f"Policy loss: {policy_loss:.4f}, Value loss: {value_loss:.4f}")
```

### DPO: Direct Preference Optimization
```python
class DPOAgent:
    """Direct Preference Optimization - no reward model needed"""
    
    def __init__(self, vocab_size: int, lr: float = 0.001, beta: float = 0.1):
        self.policy = SimplePolicy(vocab_size)
        self.reference_policy = SimplePolicy(vocab_size)  # Fixed reference
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.beta = beta
        
        # Initialize reference policy same as policy
        self.reference_policy.load_state_dict(self.policy.state_dict())
        
    def compute_log_likelihood(self, model: nn.Module, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:
        """Compute log likelihood of actions under model"""
        logits = model(states)
        log_probs = torch.log_softmax(logits, dim=-1)
        return log_probs.gather(-1, actions.unsqueeze(-1)).squeeze().sum(dim=-1)
    
    def dpo_loss(self, states: torch.Tensor, preferred_actions: torch.Tensor, 
                 rejected_actions: torch.Tensor) -> torch.Tensor:
        """DPO loss function"""
        
        # Log likelihoods under current policy  
        preferred_logp = self.compute_log_likelihood(self.policy, states, preferred_actions)
        rejected_logp = self.compute_log_likelihood(self.policy, states, rejected_actions)
        
        # Log likelihoods under reference policy (fixed)
        with torch.no_grad():
            preferred_ref_logp = self.compute_log_likelihood(self.reference_policy, states, preferred_actions)
            rejected_ref_logp = self.compute_log_likelihood(self.reference_policy, states, rejected_actions)
        
        # DPO objective
        preferred_ratio = preferred_logp - preferred_ref_logp
        rejected_ratio = rejected_logp - rejected_ref_logp
        
        loss = -torch.log(torch.sigmoid(self.beta * (preferred_ratio - rejected_ratio))).mean()
        
        return loss
    
    def update_policy(self, states: torch.Tensor, preferred_actions: torch.Tensor, rejected_actions: torch.Tensor):
        """Update policy using DPO"""
        loss = self.dpo_loss(states, preferred_actions, rejected_actions)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()

# Demo DPO
print("\n=== DPO Algorithm ===") 
dpo_agent = DPOAgent(vocab_size=1000)

# Simulate preference data
states = torch.randint(0, 1000, (2, 5))
preferred = torch.randint(0, 1000, (2, 3))  # Preferred responses
rejected = torch.randint(0, 1000, (2, 3))   # Rejected responses

loss = dpo_agent.update_policy(states, preferred, rejected)
print(f"DPO loss: {loss:.4f}")
```

### GRPO: Group Relative Policy Optimization
```python
class GRPOAgent:
    """Group Relative Policy Optimization for multi-agent scenarios"""
    
    def __init__(self, vocab_size: int, num_agents: int = 4, lr: float = 0.001):
        self.policies = [SimplePolicy(vocab_size) for _ in range(num_agents)]
        self.optimizers = [optim.Adam(policy.parameters(), lr=lr) for policy in self.policies]
        self.num_agents = num_agents
        
    def compute_group_relative_rewards(self, individual_rewards: torch.Tensor) -> torch.Tensor:
        """Convert individual rewards to group-relative rewards"""
        # individual_rewards shape: (num_agents, episode_length)
        
        # Compute group average at each step
        group_avg = individual_rewards.mean(dim=0, keepdim=True)
        
        # Group-relative reward = individual - group average
        relative_rewards = individual_rewards - group_avg
        
        return relative_rewards
    
    def update_group_policies(self, group_experiences: List[Dict]) -> Dict[str, float]:
        """Update all agent policies using group-relative rewards"""
        losses = {}
        
        # Extract group rewards
        all_rewards = torch.stack([torch.tensor(exp['rewards']) for exp in group_experiences])
        relative_rewards = self.compute_group_relative_rewards(all_rewards)
        
        # Update each agent's policy
        for i, (policy, optimizer, experience) in enumerate(zip(self.policies, self.optimizers, group_experiences)):
            states = torch.stack(experience['states'])
            actions = torch.stack(experience['actions'])
            log_probs = torch.stack(experience['log_probs'])
            
            # Use group-relative rewards for this agent
            agent_rewards = relative_rewards[i]
            
            # Compute returns
            returns = []
            R = 0
            for r in reversed(agent_rewards.tolist()):
                R = r + 0.99 * R
                returns.insert(0, R)
            returns = torch.tensor(returns)
            
            # Policy gradient update
            policy_loss = -(log_probs * returns).mean()
            
            optimizer.zero_grad()
            policy_loss.backward()
            optimizer.step()
            
            losses[f'agent_{i}'] = policy_loss.item()
        
        return losses

# Demo GRPO
print("\n=== GRPO Algorithm ===")
grpo_agent = GRPOAgent(vocab_size=1000, num_agents=3)

# Simulate group experience
group_experiences = []
for agent_id in range(3):
    experience = {
        'states': [torch.randint(0, 1000, (5,)) for _ in range(4)],
        'actions': [torch.randint(0, 1000, (1,)) for _ in range(4)],
        'log_probs': [torch.randn(1) * 0.1 for _ in range(4)],
        'rewards': [1.0 + agent_id, 2.0 + agent_id, 3.0 + agent_id, 5.0 + agent_id]
    }
    group_experiences.append(experience)

losses = grpo_agent.update_group_policies(group_experiences)
for agent, loss in losses.items():
    print(f"{agent}: {loss:.4f}")
```

## Algorithm Comparison and Selection Guide

### Algorithm Selection Matrix
```python
def algorithm_selection_guide():
    """Practical guide for choosing RL algorithms"""
    
    algorithms = {
        'REINFORCE': {
            'best_for': ['Simple tasks', 'Baseline implementations', 'Educational purposes'],
            'strengths': ['Simple to implement', 'Unbiased gradients', 'Works with any differentiable policy'],
            'weaknesses': ['High variance', 'Sample inefficient', 'Slow convergence'],
            'when_to_use': 'Proof of concept, simple environments, learning RL basics',
            'code_complexity': 'Low'
        },
        'PPO': {
            'best_for': ['Most agentic RL applications', 'Stable training', 'Long episodes'],
            'strengths': ['Stable updates', 'Sample efficient', 'Proven in practice'],
            'weaknesses': ['More complex', 'Hyperparameter sensitive', 'Requires value function'],
            'when_to_use': 'Production systems, multi-step tasks, tool integration',
            'code_complexity': 'Medium'
        },
        'DPO': {
            'best_for': ['Preference learning', 'Human feedback', 'Alignment tasks'],
            'strengths': ['No reward model', 'Direct preference optimization', 'Stable training'],
            'weaknesses': ['Requires preference data', 'Less exploration', 'Domain specific'],
            'when_to_use': 'Human alignment, preference-based tasks, safety applications',
            'code_complexity': 'Medium'
        },
        'GRPO': {
            'best_for': ['Multi-agent systems', 'Competitive environments', 'Relative performance'],
            'strengths': ['Handles multi-agent credit assignment', 'Relative performance', 'Group learning'],
            'weaknesses': ['Complex setup', 'Requires multiple agents', 'Limited applicability'],
            'when_to_use': 'Multi-agent scenarios, competitive training, group optimization',
            'code_complexity': 'High'
        }
    }
    
    print("=== Algorithm Selection Guide ===")
    for algo, details in algorithms.items():
        print(f"\n{algo}:")
        print(f"  Best for: {', '.join(details['best_for'])}")
        print(f"  When to use: {details['when_to_use']}")
        print(f"  Code complexity: {details['code_complexity']}")

algorithm_selection_guide()
```

## ASCII Diagram: Algorithm Family Tree

```
RL Algorithm Families for Agentic Systems

Policy Gradient Methods
├── REINFORCE (Vanilla)
│   ├── Simple implementation
│   ├── High variance
│   └── Educational baseline
│
├── Actor-Critic
│   ├── PPO (Proximal Policy Optimization)
│   │   ├── Clipped objective
│   │   ├── Value function baseline  
│   │   └── Industry standard
│   │
│   └── A2C/A3C (Advantage Actor-Critic)
│       ├── Advantage estimation
│       ├── Parallel training
│       └── Reduced variance
│
├── Preference-Based
│   ├── DPO (Direct Preference)
│   │   ├── No reward model
│   │   ├── Human alignment
│   │   └── Stable training
│   │
│   └── RLHF (Human Feedback)
│       ├── Reward model training
│       ├── Policy optimization
│       └── Human preference data
│
└── Multi-Agent
    ├── GRPO (Group Relative)
    │   ├── Relative rewards
    │   ├── Group coordination
    │   └── Competitive scenarios
    │
    └── MADDPG/MAPPO
        ├── Centralized training
        ├── Decentralized execution
        └── Multi-agent coordination

Algorithm Selection Flow:
Single Agent? ──Yes──► Simple Task? ──Yes──► REINFORCE
     │                      │
     │                     No
     │                      ▼
     │                 PPO (Recommended)
     │
    No
     ▼
Multi-Agent? ──Yes──► GRPO or MAPPO
     │
    No
     ▼
Preference Data? ──Yes──► DPO
     │
    No  
     ▼
Default: PPO
```

## Key Differences Summary

| Algorithm | Update Rule | Stability | Sample Efficiency | Best Use Case |
|-----------|-------------|-----------|-------------------|---------------|
| **REINFORCE** | ∇J = E[∇log π(a\|s) × R] | Low | Low | Learning/Baseline |
| **PPO** | Clipped ratio × advantage | High | Medium | Production Systems |
| **DPO** | Direct preference ranking | High | Medium | Human Alignment |
| **GRPO** | Group-relative rewards | Medium | Medium | Multi-Agent |

## Implementation Best Practices

### Production Considerations
```python
def production_best_practices():
    """Key practices for deploying RL algorithms"""
    
    practices = {
        'stability': [
            'Use gradient clipping (norm < 0.5)',
            'Learning rate scheduling',
            'Proper initialization',
            'Regularization (entropy bonus)'
        ],
        'efficiency': [
            'Vectorized environments', 
            'Batch processing',
            'GPU acceleration',
            'Experience replay where applicable'
        ],
        'monitoring': [
            'Track policy entropy',
            'Monitor gradient norms',
            'Log reward statistics',
            'Validate convergence metrics'
        ],
        'safety': [
            'Action space constraints',
            'Reward clipping',
            'Safe exploration policies',
            'Rollback mechanisms'
        ]
    }
    
    print("=== Production Best Practices ===")
    for category, items in practices.items():
        print(f"\n{category.upper()}:")
        for item in items:
            print(f"  - {item}")

production_best_practices()
```

## Practical Exercises

```python
# Exercise 1: Implement algorithm comparison
def exercise_algorithm_comparison():
    """Compare REINFORCE vs PPO on simple task"""
    # Implement both algorithms on same environment
    # Compare convergence speed and stability
    pass

# Exercise 2: Tune hyperparameters  
def exercise_hyperparameter_tuning():
    """Systematic hyperparameter search for PPO"""
    # Grid search over learning rates, clip epsilon, epochs
    # Evaluate on your specific domain
    pass

# Exercise 3: Build custom algorithm
def exercise_custom_algorithm():
    """Combine best features for your specific use case"""
    # Mix elements from different algorithms
    # Implement domain-specific modifications
    pass
```

## Resources

- **Survey Reference**: [Section 2.7, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **PPO Paper**: [Schulman et al. - Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)
- **DPO Paper**: [Rafailov et al. - Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
- **Implementation Guide**: [Stable Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- **GRPO Reference**: [Group Relative Policy Optimization](https://arxiv.org/abs/2402.14739)

## Next Steps

- **[3. Agentic Capabilities](../3_Agentic_RL_Capability_Perspective/)**: Apply algorithms to planning, tool use, memory
- **Practice**: Implement PPO for your specific agentic task
- **Deep Dive**: Study advanced policy gradient methods and their theoretical foundations

---

*Algorithm choice shapes learning dynamics. PPO provides the best balance for most agentic applications, while DPO excels for preference learning and GRPO handles multi-agent scenarios. Start with PPO for production systems.*
