# 2.1 Markov Decision Processes

Learn the formal MDP/POMDP setup that distinguishes PBRFT (single-step preference learning) from Agentic RL (multi-step decision processes). This mathematical foundation shapes what an LLM "does" during training and deployment, enabling sequential decision-making, memory utilization, and strategic planning in dynamic environments.

## Key Takeaways
- **Single-step vs Multi-step**: PBRFT uses degenerate 1-step MDPs; Agentic RL uses full multi-step POSMDPs
- **State Evolution**: PBRFT has static states; Agentic RL has evolving, partially observable states
- **Decision Horizons**: Immediate preference optimization vs long-term trajectory optimization
- **Mathematical Framework**: Formal definitions enable precise algorithm design and evaluation

## Prerequisites Check

```bash
# Verify mathematical libraries
python -c "import numpy as np; print('NumPy ready for MDP calculations')"
python -c "import matplotlib.pyplot as plt; print('Matplotlib ready for visualizations')"

# Conceptual check
echo "Do you understand basic probability (P(A|B))?"
echo "Are you familiar with state machines or finite automata?"
```

## Hands-On: MDP in Action

### Traditional PBRFT: Degenerate Single-Step MDP
```python
import numpy as np
from typing import Dict, List, Tuple

class PBRFTEnvironment:
    """Degenerate single-step MDP for preference-based fine-tuning"""
    
    def __init__(self):
        self.state = "initial_prompt"  # Static state
        self.episode_length = 1        # Always terminates after one step
    
    def step(self, action: str) -> Tuple[str, float, bool, Dict]:
        """Single step: action -> reward -> done"""
        # State never changes in PBRFT
        next_state = "terminal"
        
        # Preference-based reward (human/AI feedback)
        reward = self.preference_model(self.state, action)
        
        # Always terminal after one step
        done = True
        info = {"episode_length": 1, "type": "PBRFT"}
        
        return next_state, reward, done, info
    
    def preference_model(self, prompt: str, response: str) -> float:
        """Simplified preference scoring"""
        # In practice: human raters or trained reward model
        base_score = len(response.split()) * 0.1  # Reward verbosity
        helpfulness = 0.5 if "helpful" in response.lower() else 0
        return base_score + helpfulness

# Demo usage
env = PBRFTEnvironment()
response = "This is a helpful response to your question."
next_state, reward, done, info = env.step(response)
print(f"PBRFT Step: state=terminal, reward={reward:.2f}, done={done}")
```

### Agentic RL: Multi-Step POMDP
```python
class AgenticEnvironment:
    """Multi-step POMDP for agentic reinforcement learning"""
    
    def __init__(self, max_steps: int = 10):
        self.max_steps = max_steps
        self.current_step = 0
        self.state = self.reset()
        self.memory = []
        
    def reset(self) -> Dict:
        """Initialize environment state"""
        self.current_step = 0
        self.memory = []
        initial_state = {
            'user_query': "Help me solve a math problem: What's 15% of 240?",
            'available_tools': ['calculator', 'memory', 'web_search'],
            'agent_memory': [],
            'step': self.current_step
        }
        return initial_state
    
    def step(self, action: Dict) -> Tuple[Dict, float, bool, Dict]:
        """Multi-step interaction with environment evolution"""
        self.current_step += 1
        
        # Process agent action
        reward = 0
        if action['type'] == 'tool_use':
            if action['tool'] == 'calculator':
                result = self.use_calculator(action['args'])
                reward += 10  # Tool use reward
                
                # Update environment state
                next_state = {
                    'user_query': self.state['user_query'],
                    'available_tools': self.state['available_tools'],
                    'agent_memory': self.state['agent_memory'] + [
                        {'action': action, 'result': result, 'step': self.current_step}
                    ],
                    'tool_result': result,
                    'step': self.current_step
                }
        else:
            # Text response action
            next_state = self.state.copy()
            next_state['step'] = self.current_step
            reward += 1  # Small step reward
        
        # Check if problem solved
        if self.problem_solved(action, next_state):
            reward += 50  # Task completion bonus
            
        # Episode termination conditions
        done = (self.current_step >= self.max_steps or 
                self.problem_solved(action, next_state))
        
        self.state = next_state
        info = {
            'step': self.current_step,
            'type': 'AgenticRL',
            'tools_used': len([m for m in next_state.get('agent_memory', []) 
                              if 'tool' in str(m)])
        }
        
        return next_state, reward, done, info
    
    def use_calculator(self, args: str) -> str:
        """Simple calculator tool"""
        try:
            # Secure eval for basic math (production: use math parser)
            if all(c in '0123456789+-*/.%() ' for c in args):
                result = eval(args)
                return f"Calculator result: {result}"
        except:
            return "Calculator error: invalid expression"
        return "Calculator error"
    
    def problem_solved(self, action: Dict, state: Dict) -> bool:
        """Check if math problem is correctly solved"""
        # Simplified: check if correct answer (36) appears in response
        if action['type'] == 'text_response':
            return '36' in action.get('text', '')
        return False

# Demo usage: Multi-step interaction
env = AgenticEnvironment()
state = env.reset()
total_reward = 0

# Step 1: Use calculator tool
action1 = {
    'type': 'tool_use',
    'tool': 'calculator', 
    'args': '0.15 * 240'
}
state, reward, done, info = env.step(action1)
total_reward += reward
print(f"Step 1: reward={reward}, total={total_reward}, done={done}")

# Step 2: Provide answer based on tool result
action2 = {
    'type': 'text_response',
    'text': 'Based on the calculation, 15% of 240 is 36.'
}
state, reward, done, info = env.step(action2)
total_reward += reward
print(f"Step 2: reward={reward}, total={total_reward}, done={done}")
print(f"Tools used: {info['tools_used']}")
```

## Mathematical Formalization

### PBRFT MDP Structure
```python
class PBRFTMath:
    """Mathematical formalization of PBRFT as degenerate MDP"""
    
    @staticmethod
    def state_space():
        """S = {s_prompt}: Single prompt state"""
        return {"single_prompt_state"}
    
    @staticmethod
    def action_space():
        """A = text responses (continuous/discrete depending on tokenization)"""
        return "natural_language_responses"
    
    @staticmethod
    def transition_function(state, action):
        """T(s'|s,a) = 1 for s' = terminal, 0 otherwise"""
        if state == "prompt":
            return {"terminal": 1.0}  # Deterministic transition to terminal
        return {}
    
    @staticmethod
    def reward_function(state, action):
        """R(s,a): Preference model score"""
        # In practice: reward model trained on human preferences
        return "preference_model_score(prompt, response)"
    
    @staticmethod
    def horizon():
        """H = 1: Single decision step"""
        return 1

# Visualization
def visualize_pbrft_mdp():
    """ASCII representation of PBRFT MDP"""
    print("""
PBRFT MDP Structure:
    
S₀ (Prompt) ──action──► S₁ (Terminal)
     │                      │
     └──── R(s₀,a) ─────────┘
     
H = 1 (single step)
T(s₁|s₀,a) = 1 (deterministic termination)
    """)

visualize_pbrft_mdp()
```

### Agentic RL POMDP Structure  
```python
class AgenticRLMath:
    """Mathematical formalization of Agentic RL as POMDP"""
    
    @staticmethod
    def state_space():
        """S = environment_state × agent_memory × tools_state"""
        return {
            'environment': 'dynamic_world_state',
            'memory': 'agent_episodic_memory', 
            'tools': 'available_tools_and_context'
        }
    
    @staticmethod  
    def action_space():
        """A = A_text ∪ A_tools: Hybrid action space"""
        return {
            'text_actions': 'natural_language_generation',
            'tool_actions': 'structured_tool_invocations',
            'memory_actions': 'memory_read_write_operations'
        }
    
    @staticmethod
    def observation_function(state):
        """O(o|s): Partial observability mapping"""
        return "observable_subset_of_full_state"
    
    @staticmethod
    def transition_function(state, action):
        """T(s'|s,a): Stochastic environment evolution"""
        return "probabilistic_state_transitions"
    
    @staticmethod
    def reward_function(state, action, timestep):
        """R(s,a,t): Multi-step reward design"""
        return {
            'task_completion': 'sparse_goal_achievement',
            'tool_efficiency': 'dense_process_rewards', 
            'memory_usage': 'auxiliary_learning_signals'
        }
    
    @staticmethod
    def horizon():
        """H > 1: Multi-step episodes"""
        return "variable_length_episodes"

# Visualization
def visualize_agentic_pomdp():
    """ASCII representation of Agentic POMDP"""
    print("""
Agentic RL POMDP Structure:

    S₀ ──a₀──► S₁ ──a₁──► S₂ ──a₂──► ... ──aₜ──► Sₜ₊₁
    │          │          │                      │
    │          │          │                      │
    O₀ ◄──────┘   O₁ ◄──────┘                   Oₜ₊₁
    │               │                             │
    │               │                             │
    π(a₀|o₀,h₀)    π(a₁|o₁,h₁)                 π(aₜ|oₜ,hₜ)
    
    H >> 1 (multi-step)
    T(s'|s,a) ~ stochastic transitions
    O(o|s) ~ partial observability
    π(a|o,h) ~ policy depends on observation + history
    """)

visualize_agentic_pomdp()
```

## Policy Learning Comparison

### PBRFT Policy Optimization
```python
def pbrft_policy_learning():
    """PBRFT optimizes immediate response quality"""
    
    def objective_function(policy, dataset):
        """Maximize expected preference score"""
        total_reward = 0
        for prompt, preferred_response in dataset:
            generated = policy.generate(prompt)
            reward = preference_model(prompt, generated)
            total_reward += reward
        return total_reward / len(dataset)
    
    print("PBRFT Objective: E[R(s,a)] - immediate reward")
    print("No temporal credit assignment needed")
    print("Single-step gradient updates")

# Demo PBRFT learning step
pbrft_policy_learning()
```

### Agentic RL Policy Optimization  
```python
def agentic_policy_learning():
    """Agentic RL optimizes long-term returns"""
    
    def objective_function(policy, environment, num_episodes):
        """Maximize discounted trajectory returns"""
        gamma = 0.99  # Discount factor
        total_returns = []
        
        for episode in range(num_episodes):
            state = environment.reset()
            episode_return = 0
            t = 0
            
            while not done:
                action = policy.act(state)
                next_state, reward, done, _ = environment.step(action)
                
                # Discounted reward accumulation
                episode_return += (gamma ** t) * reward
                state = next_state
                t += 1
                
            total_returns.append(episode_return)
            
        return np.mean(total_returns)
    
    print("Agentic Objective: E[Σₜ γᵗ R(sₜ,aₜ)] - discounted returns")
    print("Temporal credit assignment required")
    print("Multi-step trajectory optimization")

# Demo Agentic RL learning
agentic_policy_learning()
```

## Key Differences Summary Table

| Aspect | PBRFT | Agentic RL |
|--------|-------|------------|
| **State Space** | Single prompt | Dynamic environment + memory |
| **Action Space** | Text only | Text + tools + memory ops |
| **Transitions** | Deterministic terminal | Stochastic continuing |
| **Horizon** | H = 1 | H > 1, variable |
| **Rewards** | Immediate preference | Multi-step, shaped |
| **Learning** | Response optimization | Trajectory optimization |
| **Memory** | None | Episodic + working memory |
| **Tools** | None | External tool integration |

## Mathematical Exercises

Test your understanding:

```python
# Exercise 1: Implement MDP components
def exercise_mdp_components():
    """Define MDP tuple (S, A, T, R, γ) for simple task"""
    pass  # Your implementation

# Exercise 2: Compare value functions  
def exercise_value_functions():
    """Calculate V(s) for 1-step vs multi-step policies"""
    pass  # Your implementation

# Exercise 3: Design reward function
def exercise_reward_design():
    """Create multi-step reward for coding task"""
    pass  # Your implementation
```

## Resources

- **Survey Section**: [Section 2.1, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **MDP Reference**: Sutton & Barto, "Reinforcement Learning: An Introduction", Chapter 3
- **POMDP Tutorial**: [Introduction to POMDPs](https://www.pomdp.org/tutorial/)
- **Implementation**: [Gymnasium MDP environments](https://gymnasium.farama.org/)

## Next Steps

- **[2.2 Environment State](2.2_Environment_State.md)**: State representation in dynamic environments
- **Practice**: Implement your own MDP/POMDP comparison for a simple task
- **Deep Dive**: Read survey Section 2.1 for formal mathematical treatment

---

*This MDP foundation is essential for understanding how agentic capabilities emerge from multi-step optimization. The mathematical formalism guides algorithm design and evaluation throughout the course.*
