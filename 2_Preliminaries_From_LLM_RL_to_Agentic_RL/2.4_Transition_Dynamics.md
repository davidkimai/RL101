# 2.4 Transition Dynamics

PBRFT uses deterministic, terminal transitions where episodes end immediately after response generation. Agentic RL embraces stochastic, continuing dynamics where agent actions probabilistically update environment state, enabling exploration, adaptation, and multi-step strategies through sequential interaction loops.

## Key Takeaways
- **Deterministic vs Stochastic**: PBRFT has predictable terminal transitions; Agentic RL has probabilistic state evolution
- **Termination Patterns**: Single-step episodes vs continuing multi-step trajectories  
- **Action Effects**: No environment impact vs dynamic state updates from tool use
- **Exploration Enablement**: Static outcomes vs uncertain transitions that reward exploration

## Prerequisites Check

```bash
# Verify probability libraries
python -c "import numpy as np, random; print('Probability tools ready')"
python -c "import matplotlib.pyplot as plt; print('Visualization ready for transition plots')"

# Conceptual check
echo "Do you understand basic probability distributions?"
echo "Are you familiar with state transitions and Markov properties?"
```

## Hands-On: Transition Dynamics Comparison

### PBRFT: Deterministic Terminal Transitions
```python
import random
import numpy as np
from typing import Dict, Tuple, Optional

class PBRFTTransitions:
    """Deterministic single-step transitions for PBRFT"""
    
    def __init__(self):
        self.transition_count = 0
        self.is_deterministic = True
        
    def transition(self, state: str, action: str) -> Tuple[str, bool, Dict]:
        """PBRFT: Always transitions to terminal state"""
        self.transition_count += 1
        
        # Deterministic transition: always terminal after one step
        next_state = "terminal"
        done = True  # Episode always ends
        
        transition_info = {
            'transition_type': 'deterministic_terminal',
            'transition_count': self.transition_count,
            'deterministic': True,
            'episode_length': 1,
            'state_change': state != next_state
        }
        
        return next_state, done, transition_info
    
    def get_transition_matrix(self) -> Dict:
        """PBRFT transition probabilities"""
        return {
            'prompt -> terminal': 1.0,  # Always transitions to terminal
            'terminal -> terminal': 1.0  # Stays terminal (episode over)
        }

# Demo PBRFT transitions
pbrft_env = PBRFTTransitions()
initial_state = "user_prompt"
action = "model_response"

print("=== PBRFT Transition Dynamics ===")
print(f"Initial state: {initial_state}")
print(f"Action: {action}")

next_state, done, info = pbrft_env.transition(initial_state, action)
print(f"Next state: {next_state}")
print(f"Episode done: {done}")
print(f"Transition info: {info}")

print("\nTransition Matrix:")
for transition, prob in pbrft_env.get_transition_matrix().items():
    print(f"  {transition}: {prob}")
```

### Agentic RL: Stochastic Continuing Transitions
```python
import numpy as np
import random
from dataclasses import dataclass
from typing import Dict, List, Any

@dataclass
class EnvironmentState:
    """Rich environment state for agentic transitions"""
    task_progress: float
    user_satisfaction: float
    tool_availability: Dict[str, bool]
    memory_usage: float
    step_count: int
    context_complexity: float

class AgenticTransitions:
    """Stochastic multi-step transitions for Agentic RL"""
    
    def __init__(self, max_episode_length: int = 20):
        self.max_episode_length = max_episode_length
        self.transition_count = 0
        self.is_deterministic = False
        self.state_history = []
        
        # Transition noise parameters
        self.noise_std = 0.1
        self.tool_failure_rate = 0.05
        self.environmental_drift = 0.02
        
    def transition(self, state: EnvironmentState, action: Dict) -> Tuple[EnvironmentState, bool, Dict]:
        """Stochastic environment evolution based on agent action"""
        self.transition_count += 1
        self.state_history.append(state)
        
        # Start with current state
        next_state = EnvironmentState(
            task_progress=state.task_progress,
            user_satisfaction=state.user_satisfaction,
            tool_availability=state.tool_availability.copy(),
            memory_usage=state.memory_usage,
            step_count=state.step_count + 1,
            context_complexity=state.context_complexity
        )
        
        # Apply action effects with stochasticity
        transition_effects = self.compute_action_effects(state, action)
        next_state = self.apply_stochastic_effects(next_state, transition_effects)
        
        # Apply environmental drift (world changes independently)
        next_state = self.apply_environmental_drift(next_state)
        
        # Determine episode termination (probabilistic)
        done = self.check_termination_conditions(next_state)
        
        transition_info = {
            'transition_type': 'stochastic_continuing',
            'transition_count': self.transition_count,
            'deterministic': False,
            'action_type': action.get('type', 'unknown'),
            'noise_applied': True,
            'environmental_drift': True,
            'termination_probability': self.get_termination_probability(next_state)
        }
        
        return next_state, done, transition_info
    
    def compute_action_effects(self, state: EnvironmentState, action: Dict) -> Dict:
        """Compute expected effects of action on state"""
        effects = {
            'task_progress_delta': 0.0,
            'satisfaction_delta': 0.0,
            'memory_delta': 0.0,
            'complexity_delta': 0.0,
            'tool_effects': {}
        }
        
        action_type = action.get('type', 'text')
        
        if action_type == 'tool_use':
            tool_name = action.get('tool', 'unknown')
            
            # Tool use generally advances task progress
            effects['task_progress_delta'] = 0.2
            effects['memory_delta'] = 0.1  # Tools require memory
            
            # But tools can fail stochastically
            if random.random() < self.tool_failure_rate:
                effects['task_progress_delta'] = -0.1  # Setback on failure
                effects['satisfaction_delta'] = -0.1
                effects['tool_effects'][tool_name] = 'failed'
            else:
                effects['satisfaction_delta'] = 0.1
                effects['tool_effects'][tool_name] = 'succeeded'
                
        elif action_type == 'text_response':
            # Text responses provide satisfaction but limited task progress
            effects['satisfaction_delta'] = 0.05
            effects['task_progress_delta'] = 0.05
            
        elif action_type == 'reasoning':
            # Reasoning increases complexity handling but uses resources
            effects['complexity_delta'] = -0.1  # Reduces perceived complexity
            effects['memory_delta'] = 0.15
            effects['task_progress_delta'] = 0.1
            
        return effects
    
    def apply_stochastic_effects(self, state: EnvironmentState, effects: Dict) -> EnvironmentState:
        """Apply effects with stochastic noise"""
        
        # Add Gaussian noise to all continuous effects
        noise = np.random.normal(0, self.noise_std)
        
        state.task_progress += effects['task_progress_delta'] + noise
        state.task_progress = np.clip(state.task_progress, 0.0, 1.0)
        
        state.user_satisfaction += effects['satisfaction_delta'] + noise
        state.user_satisfaction = np.clip(state.user_satisfaction, 0.0, 1.0)
        
        state.memory_usage += effects['memory_delta'] + noise
        state.memory_usage = np.clip(state.memory_usage, 0.0, 1.0)
        
        state.context_complexity += effects['complexity_delta'] + noise
        state.context_complexity = np.clip(state.context_complexity, 0.0, 1.0)
        
        # Update tool availability based on effects
        for tool, effect in effects['tool_effects'].items():
            if effect == 'failed':
                # Failed tools might become temporarily unavailable
                if random.random() < 0.3:
                    state.tool_availability[tool] = False
            
        return state
    
    def apply_environmental_drift(self, state: EnvironmentState) -> EnvironmentState:
        """Environment changes independently of agent actions"""
        
        # Gradual environmental changes
        drift = np.random.normal(0, self.environmental_drift)
        
        # User satisfaction can drift due to external factors
        state.user_satisfaction += drift
        state.user_satisfaction = np.clip(state.user_satisfaction, 0.0, 1.0)
        
        # Context complexity can increase over time
        if random.random() < 0.1:  # 10% chance per step
            state.context_complexity += 0.05
            state.context_complexity = np.clip(state.context_complexity, 0.0, 1.0)
        
        # Tools might randomly become available/unavailable
        for tool in state.tool_availability:
            if random.random() < 0.02:  # 2% chance per step
                state.tool_availability[tool] = not state.tool_availability[tool]
        
        return state
    
    def check_termination_conditions(self, state: EnvironmentState) -> bool:
        """Probabilistic episode termination"""
        
        # Hard termination conditions
        if state.step_count >= self.max_episode_length:
            return True
        
        # Task completion (high probability of termination)
        if state.task_progress >= 0.95:
            return random.random() < 0.8  # 80% chance to terminate
        
        # User frustration (moderate probability of termination)
        if state.user_satisfaction <= 0.1:
            return random.random() < 0.4  # 40% chance to terminate
        
        # Memory overflow (low probability of termination)
        if state.memory_usage >= 0.95:
            return random.random() < 0.2  # 20% chance to terminate
        
        # Random early termination (very low probability)
        return random.random() < 0.01  # 1% chance per step
    
    def get_termination_probability(self, state: EnvironmentState) -> float:
        """Calculate current termination probability"""
        base_prob = 0.01
        
        if state.task_progress >= 0.95:
            base_prob += 0.8
        if state.user_satisfaction <= 0.1:
            base_prob += 0.4
        if state.memory_usage >= 0.95:
            base_prob += 0.2
        if state.step_count >= self.max_episode_length:
            base_prob = 1.0
            
        return min(base_prob, 1.0)

# Demo Agentic transitions
agentic_env = AgenticTransitions()

# Initial state
initial_state = EnvironmentState(
    task_progress=0.1,
    user_satisfaction=0.7,
    tool_availability={'calculator': True, 'web_search': True, 'memory': True},
    memory_usage=0.2,
    step_count=0,
    context_complexity=0.5
)

print("=== Agentic RL Transition Dynamics ===")
print(f"Initial state:")
print(f"  Task progress: {initial_state.task_progress:.2f}")
print(f"  User satisfaction: {initial_state.user_satisfaction:.2f}")
print(f"  Memory usage: {initial_state.memory_usage:.2f}")

# Simulate sequence of transitions
actions = [
    {'type': 'tool_use', 'tool': 'calculator'},
    {'type': 'reasoning', 'content': 'analyzing results'},
    {'type': 'text_response', 'content': 'explaining solution'},
    {'type': 'tool_use', 'tool': 'memory'}
]

current_state = initial_state
for i, action in enumerate(actions):
    print(f"\n--- Step {i+1}: {action['type']} ---")
    next_state, done, info = agentic_env.transition(current_state, action)
    
    print(f"Action: {action}")
    print(f"Task progress: {current_state.task_progress:.2f} → {next_state.task_progress:.2f}")
    print(f"User satisfaction: {current_state.user_satisfaction:.2f} → {next_state.user_satisfaction:.2f}")
    print(f"Memory usage: {current_state.memory_usage:.2f} → {next_state.memory_usage:.2f}")
    print(f"Episode done: {done}")
    print(f"Termination probability: {info['termination_probability']:.3f}")
    
    if done:
        print("Episode terminated!")
        break
        
    current_state = next_state
```

## Stochasticity Patterns and Sources

### Sources of Transition Uncertainty
```python
class TransitionUncertaintySources:
    """Catalog different sources of stochasticity in agentic environments"""
    
    @staticmethod
    def tool_execution_uncertainty():
        """Tools can fail, have latency, return partial results"""
        examples = {
            'api_failures': {
                'description': 'External API calls can timeout or error',
                'probability': 0.05,
                'impact': 'Task progress setback',
                'mitigation': 'Retry logic, fallback tools'
            },
            'rate_limiting': {
                'description': 'API rate limits cause delayed responses',
                'probability': 0.02,
                'impact': 'Increased episode length',
                'mitigation': 'Request throttling, queue management'
            },
            'partial_results': {
                'description': 'Tool returns incomplete information',
                'probability': 0.1,
                'impact': 'Requires additional tool calls',
                'mitigation': 'Result validation, iterative refinement'
            }
        }
        return examples
    
    @staticmethod
    def environment_evolution():
        """Environment changes independently of agent actions"""
        examples = {
            'user_context_drift': {
                'description': 'User needs/preferences change during conversation',
                'probability': 0.08,
                'impact': 'Previous progress becomes less relevant',
                'mitigation': 'Periodic context checking, adaptability'
            },
            'external_information': {
                'description': 'New information becomes available',
                'probability': 0.03,
                'impact': 'Opportunities for better solutions',
                'mitigation': 'Proactive information monitoring'
            },
            'resource_availability': {
                'description': 'Computational resources fluctuate',
                'probability': 0.05,
                'impact': 'Tool performance variability',
                'mitigation': 'Resource-aware planning'
            }
        }
        return examples
    
    @staticmethod
    def measurement_noise():
        """Observation and reward signals contain noise"""
        examples = {
            'sensor_noise': {
                'description': 'Noisy readings from environment sensors',
                'probability': 0.2,
                'impact': 'Inaccurate state estimation',
                'mitigation': 'Filtering, multiple measurements'
            },
            'reward_noise': {
                'description': 'Reward signals vary due to measurement error',
                'probability': 0.15,
                'impact': 'Noisy learning signals',
                'mitigation': 'Reward smoothing, ensemble methods'
            }
        }
        return examples

# Demonstrate uncertainty sources
uncertainty = TransitionUncertaintySources()
print("=== Sources of Transition Uncertainty ===")

for category, method in [
    ('Tool Execution', uncertainty.tool_execution_uncertainty),
    ('Environment Evolution', uncertainty.environment_evolution),
    ('Measurement Noise', uncertainty.measurement_noise)
]:
    print(f"\n{category}:")
    sources = method()
    for source_name, details in sources.items():
        print(f"  {source_name}:")
        print(f"    Description: {details['description']}")
        print(f"    Probability: {details['probability']}")
        print(f"    Impact: {details['impact']}")
```

## Transition Function Design Patterns

### Structured Stochasticity
```python
class StructuredTransitionDesign:
    """Design patterns for principled stochastic transitions"""
    
    def __init__(self):
        self.transition_models = {
            'linear_gaussian': self.linear_gaussian_transition,
            'categorical_discrete': self.categorical_discrete_transition,
            'mixture_model': self.mixture_model_transition,
            'conditional_branching': self.conditional_branching_transition
        }
    
    def linear_gaussian_transition(self, state: Dict, action: Dict) -> Dict:
        """Linear state update with Gaussian noise"""
        # s' = As + Ba + noise
        # Common for continuous state spaces
        
        next_state = {}
        for key, value in state.items():
            if isinstance(value, (int, float)):
                # Linear update with action effect
                action_effect = action.get(f'{key}_effect', 0.0)
                noise = np.random.normal(0, 0.1)
                next_state[key] = value + action_effect + noise
            else:
                next_state[key] = value
                
        return next_state
    
    def categorical_discrete_transition(self, state: Dict, action: Dict) -> Dict:
        """Discrete state transitions with categorical probabilities"""
        # Common for finite state spaces
        
        current_discrete_state = state.get('discrete_state', 'idle')
        action_type = action.get('type', 'wait')
        
        # Define transition probabilities
        transition_probs = {
            ('idle', 'start'): {'working': 0.8, 'failed': 0.2},
            ('working', 'continue'): {'working': 0.9, 'completed': 0.1},
            ('working', 'stop'): {'idle': 0.7, 'failed': 0.3},
            ('completed', 'reset'): {'idle': 1.0}
        }
        
        key = (current_discrete_state, action_type)
        if key in transition_probs:
            probs = transition_probs[key]
            next_discrete_state = np.random.choice(
                list(probs.keys()), 
                p=list(probs.values())
            )
        else:
            next_discrete_state = current_discrete_state
        
        next_state = state.copy()
        next_state['discrete_state'] = next_discrete_state
        return next_state
    
    def mixture_model_transition(self, state: Dict, action: Dict) -> Dict:
        """Multiple possible transition outcomes with mixing"""
        # Useful when action can have qualitatively different effects
        
        outcomes = [
            {'weight': 0.6, 'effect': 'normal_progress'},
            {'weight': 0.3, 'effect': 'accelerated_progress'},
            {'weight': 0.1, 'effect': 'setback'}
        ]
        
        # Sample outcome
        weights = [o['weight'] for o in outcomes]
        chosen_outcome = np.random.choice(outcomes, p=weights)
        
        # Apply corresponding transition
        next_state = state.copy()
        if chosen_outcome['effect'] == 'normal_progress':
            next_state['progress'] = state.get('progress', 0) + 0.1
        elif chosen_outcome['effect'] == 'accelerated_progress':
            next_state['progress'] = state.get('progress', 0) + 0.3
        else:  # setback
            next_state['progress'] = max(0, state.get('progress', 0) - 0.1)
            
        return next_state
    
    def conditional_branching_transition(self, state: Dict, action: Dict) -> Dict:
        """Transition depends on complex state conditions"""
        # Realistic for environments with multiple interacting factors
        
        next_state = state.copy()
        
        # Example: Tool use success depends on multiple conditions
        if action.get('type') == 'tool_use':
            tool_name = action.get('tool', '')
            
            # Success probability depends on state
            base_success_prob = 0.8
            
            # Modifiers based on current state
            if state.get('memory_usage', 0) > 0.9:
                base_success_prob -= 0.3  # High memory usage reduces success
            if state.get('user_satisfaction', 0) < 0.3:
                base_success_prob -= 0.2  # Frustrated user affects performance
            if state.get('step_count', 0) > 15:
                base_success_prob -= 0.1  # Fatigue in long conversations
                
            success = np.random.random() < base_success_prob
            
            if success:
                next_state['tool_results'] = f"{tool_name}_success"
                next_state['progress'] = min(1.0, state.get('progress', 0) + 0.2)
            else:
                next_state['tool_results'] = f"{tool_name}_failure"
                next_state['user_satisfaction'] = max(0, state.get('user_satisfaction', 0.5) - 0.1)
                
        return next_state

# Demo structured transition patterns
transition_designer = StructuredTransitionDesign()

test_state = {
    'progress': 0.3,
    'memory_usage': 0.4,
    'user_satisfaction': 0.7,
    'step_count': 5,
    'discrete_state': 'working'
}

test_action = {
    'type': 'tool_use',
    'tool': 'calculator',
    'progress_effect': 0.1
}

print("=== Structured Transition Patterns ===")
for pattern_name, transition_func in transition_designer.transition_models.items():
    print(f"\n{pattern_name.upper()}:")
    result = transition_func(test_state, test_action)
    print(f"  Progress: {test_state.get('progress', 0):.2f} → {result.get('progress', 0):.2f}")
    if 'discrete_state' in result:
        print(f"  Discrete state: {test_state.get('discrete_state')} → {result.get('discrete_state')}")
```

## ASCII Diagrams: Transition Comparison

```
PBRFT Transition Graph:
┌─────────┐    deterministic    ┌─────────┐
│ Prompt  │ ──── P=1.0 ────────►│Terminal │
│ State   │                     │ State   │
└─────────┘                     └─────────┘
     │                               │
     │            Episode            │
     └──── Length = 1 ──────────────┘

Agentic RL Transition Graph:
                stochastic transitions
┌─────────┐    P₁    ┌─────────┐    P₂    ┌─────────┐
│ State₁  │ ────────►│ State₂  │ ────────►│ State₃  │
└─────────┘          └─────────┘          └─────────┘
     │                     │                     │
     │ P₄                  │ P₃                  │ P₅
     ▼                     ▼                     ▼
┌─────────┐          ┌─────────┐          ┌─────────┐
│ Fail    │          │ Branch  │          │Terminal │
│ State   │          │ State   │          │ State   │
└─────────┘          └─────────┘          └─────────┘

Where: P₁, P₂, P₃, P₄, P₅ ∈ (0,1) and action-dependent

Stochasticity Sources:
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│ Tool        │    │Environment  │    │Measurement  │
│ Failures    │    │ Drift       │    │ Noise       │
├─────────────┤    ├─────────────┤    ├─────────────┤
│• API errors │    │• User drift │    │• Sensor     │
│• Timeouts   │    │• Context    │    │  noise      │
│• Rate limits│    │  change     │    │• Reward     │
└─────────────┘    └─────────────┘    │  noise      │
                                      └─────────────┘
         │               │                     │
         └───────────────┼─────────────────────┘
                         ▼
                ┌─────────────────┐
                │ Stochastic      │
                │ Transitions     │
                │ T(s'|s,a) ~ P   │
                └─────────────────┘
```

## Training Implications

### Credit Assignment in Stochastic Environments
```python
def stochastic_credit_assignment_challenges():
    """How stochasticity affects learning in agentic RL"""
    
    challenges = {
        'variance_in_returns': {
            'problem': 'Same action in same state can yield different rewards',
            'impact': 'High variance gradient estimates',
            'solution': 'Baseline methods (actor-critic), larger batch sizes'
        },
        'exploration_attribution': {
            'problem': 'Hard to distinguish good exploration from lucky outcomes',
            'impact': 'Suboptimal exploration strategies',
            'solution': 'Intrinsic motivation, count-based exploration bonuses'
        },
        'temporal_credit_assignment': {
            'problem': 'Delayed stochastic effects make attribution difficult',
            'impact': 'Slow learning of multi-step strategies',
            'solution': 'Eligibility traces, attention mechanisms, hindsight experience'
        },
        'confounding_factors': {
            'problem': 'Environment changes correlated with actions',
            'impact': 'Learning spurious action-outcome relationships',
            'solution': 'Causal inference methods, controlled baselines'
        }
    }
    
    print("=== Credit Assignment Challenges ===")
    for challenge, details in challenges.items():
        print(f"\n{challenge.upper()}:")
        for aspect in ['problem', 'impact', 'solution']:
            print(f"  {aspect.title()}: {details[aspect]}")

stochastic_credit_assignment_challenges()
```

## Key Differences Summary

| Aspect | PBRFT Transitions | Agentic RL Transitions |
|--------|-------------------|-------------------------|
| **Determinism** | Fully deterministic | Stochastic with multiple sources |
| **Episode Length** | Always 1 step | Variable, probabilistic termination |
| **State Evolution** | No evolution | Rich dynamics with memory |
| **Action Effects** | No environment impact | Probabilistic state updates |
| **Exploration** | Not applicable | Enabled by uncertainty |
| **Credit Assignment** | Trivial (single step) | Complex (multi-step, stochastic) |
| **Learning Challenge** | Response quality only | Sequential decision making |

## Practical Exercises

```python
# Exercise 1: Design transition function
def exercise_transition_function():
    """Design stochastic transitions for your domain"""
    # Your implementation here
    pass

# Exercise 2: Analyze stochasticity sources
def exercise_stochasticity_analysis():
    """Identify and model uncertainty sources in your environment"""
    # Your implementation here
    pass

# Exercise 3: Implement termination conditions
def exercise_termination_conditions():
    """Create probabilistic episode termination logic"""
    # Your implementation here
    pass
```

## Resources

- **Survey Reference**: [Section 2.4, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **Stochastic Processes**: [Probability and Random Processes by Grimmett](https://www.cambridge.org/core/books/probability-and-random-processes/8A98C4AE1FB6B1D5FB7DBF4A64B0CED2)
- **MDP Theory**: [Puterman, Markov Decision Processes](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887)
- **Implementation**: [OpenAI Gym Stochastic Environments](https://gymnasium.farama.org/environments/classic_control/)

## Next Steps

- **[2.5 Reward Function](2.5_Reward_Function.md)**: Multi-step reward design and shaping
- **Practice**: Implement stochastic transitions for a simple environment
- **Deep Dive**: Study advanced stochastic processes for environment modeling

---

*Stochastic transitions are what enable exploration, adaptation, and robust learning in agentic environments. The uncertainty creates challenges but also opportunities for discovering better strategies.*
