# 2.6 Learning Objective

PBRFT optimizes immediate response-level rewards without temporal discounting, treating each interaction independently. Agentic RL maximizes discounted cumulative returns across multi-step trajectories, enabling long-term planning, credit assignment, and strategic decision-making that considers future consequences.

## Key Takeaways
- **Immediate vs Long-term**: Response optimization vs trajectory optimization
- **Discounting**: No temporal weighting vs exponential decay of future rewards
- **Planning Horizon**: Single step vs multi-step strategic thinking
- **Credit Assignment**: Simple attribution vs complex temporal credit distribution

## Prerequisites Check

```bash
# Verify optimization libraries
python -c "import numpy as np, scipy; print('Optimization tools ready')"
python -c "import matplotlib.pyplot as plt; print('Plotting ready for objective visualization')"

# Conceptual check
echo "Do you understand the concept of discounted future rewards?"
echo "Are you familiar with optimization objectives and gradients?"
```

## Hands-On: Learning Objective Comparison

### PBRFT: Immediate Response Optimization
```python
import numpy as np
from typing import List, Dict, Tuple

class PBRFTObjective:
    """Immediate reward optimization for preference-based fine-tuning"""
    
    def __init__(self):
        self.objective_type = "immediate_reward"
        self.discount_factor = None  # No discounting
        
    def compute_objective(self, prompt_responses: List[Tuple[str, str]]) -> Dict:
        """Optimize expected immediate reward"""
        total_reward = 0.0
        evaluations = []
        
        for i, (prompt, response) in enumerate(prompt_responses):
            # Immediate reward for this response
            reward = self.evaluate_response(prompt, response)
            total_reward += reward
            
            evaluations.append({
                'step': i,
                'prompt': prompt[:30] + "...",
                'response': response[:30] + "...",
                'reward': reward,
                'cumulative': total_reward
            })
        
        # Objective is simply average immediate reward
        objective = total_reward / len(prompt_responses) if prompt_responses else 0.0
        
        return {
            'objective_value': objective,
            'total_reward': total_reward,
            'num_evaluations': len(prompt_responses),
            'evaluation_details': evaluations,
            'optimization_type': 'immediate'
        }
    
    def evaluate_response(self, prompt: str, response: str) -> float:
        """Simplified immediate reward evaluation"""
        reward = 0.0
        
        # Length bonus (up to reasonable limit)
        reward += min(len(response.split()) * 0.1, 5.0)
        
        # Helpfulness indicators
        if any(word in response.lower() for word in ['help', 'assist', 'solve']):
            reward += 3.0
            
        # Completeness indicators
        if response.endswith('.') or response.endswith('!'):
            reward += 1.0
            
        return reward
    
    def optimization_gradient(self, current_params: Dict) -> str:
        """PBRFT uses direct preference optimization"""
        return "∇J = E[∇log π(a|s) * R(s,a)] - no temporal dependencies"

# Demo PBRFT objective
pbrft_obj = PBRFTObjective()

sample_interactions = [
    ("What's 2+2?", "2+2 equals 4."),
    ("Explain gravity", "Gravity is a force that pulls objects together."),
    ("How are you?", "I'm doing well, thank you for asking!")
]

print("=== PBRFT Learning Objective ===")
result = pbrft_obj.compute_objective(sample_interactions)
print(f"Objective Value: {result['objective_value']:.3f}")
print(f"Total Reward: {result['total_reward']:.3f}")
print(f"Optimization Type: {result['optimization_type']}")

print("\nPer-Response Breakdown:")
for eval_detail in result['evaluation_details']:
    print(f"  Step {eval_detail['step']}: {eval_detail['reward']:.2f} reward")
```

### Agentic RL: Long-Term Trajectory Optimization
```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class TrajectoryStep:
    """Single step in agentic trajectory"""
    state: str
    action: str
    reward: float
    next_state: str
    done: bool = False
    step_number: int = 0

class AgenticRLObjective:
    """Long-term discounted return optimization"""
    
    def __init__(self, gamma: float = 0.99):
        self.gamma = gamma  # Discount factor
        self.objective_type = "discounted_returns"
        
    def compute_trajectory_return(self, trajectory: List[TrajectoryStep]) -> Dict:
        """Compute discounted return for trajectory"""
        discounted_rewards = []
        cumulative_return = 0.0
        
        # Compute returns working backwards (dynamic programming)
        returns = []
        future_return = 0.0
        
        for step in reversed(trajectory):
            future_return = step.reward + self.gamma * future_return
            returns.append(future_return)
            
        returns = list(reversed(returns))  # Restore forward order
        
        trajectory_details = []
        for i, (step, return_val) in enumerate(zip(trajectory, returns)):
            trajectory_details.append({
                'step': step.step_number,
                'action': step.action,
                'immediate_reward': step.reward,
                'discounted_return': return_val,
                'discount_factor': self.gamma ** i
            })
        
        return {
            'trajectory_return': returns[0] if returns else 0.0,
            'undiscounted_sum': sum(step.reward for step in trajectory),
            'trajectory_length': len(trajectory),
            'discount_factor': self.gamma,
            'step_details': trajectory_details
        }
    
    def compute_value_function(self, trajectories: List[List[TrajectoryStep]]) -> Dict:
        """Compute value function across multiple trajectories"""
        trajectory_returns = []
        all_step_details = []
        
        for traj in trajectories:
            traj_result = self.compute_trajectory_return(traj)
            trajectory_returns.append(traj_result['trajectory_return'])
            all_step_details.extend(traj_result['step_details'])
        
        # Policy objective is expected discounted return
        policy_objective = np.mean(trajectory_returns) if trajectory_returns else 0.0
        
        return {
            'policy_objective': policy_objective,
            'trajectory_returns': trajectory_returns,
            'num_trajectories': len(trajectories),
            'average_length': np.mean([len(traj) for traj in trajectories]),
            'return_variance': np.var(trajectory_returns) if len(trajectory_returns) > 1 else 0.0
        }
    
    def demonstrate_discount_effect(self, base_reward: float, max_steps: int = 10) -> Dict:
        """Show how discounting affects reward weighting"""
        discount_effects = []
        
        for step in range(max_steps):
            discounted_value = base_reward * (self.gamma ** step)
            discount_effects.append({
                'step': step,
                'immediate_reward': base_reward,
                'discounted_value': discounted_value,
                'discount_factor': self.gamma ** step,
                'percentage_of_original': (discounted_value / base_reward) * 100
            })
        
        return {
            'discount_effects': discount_effects,
            'total_discounted_value': sum(effect['discounted_value'] for effect in discount_effects)
        }
    
    def optimization_gradient(self, current_params: Dict) -> str:
        """Agentic RL uses policy gradient with temporal dependencies"""
        return f"∇J = E[Σt γ^t * ∇log π(at|st) * Rt] - temporal credit assignment"

# Demo Agentic RL objective
agentic_obj = AgenticRLObjective(gamma=0.9)

# Example trajectory: multi-step problem solving
trajectory = [
    TrajectoryStep("problem_given", "analyze_problem", 2.0, "understanding", False, 1),
    TrajectoryStep("understanding", "use_calculator", 5.0, "calculation_done", False, 2),
    TrajectoryStep("calculation_done", "verify_result", 3.0, "verified", False, 3),
    TrajectoryStep("verified", "explain_solution", 8.0, "explanation_given", False, 4),
    TrajectoryStep("explanation_given", "complete_task", 20.0, "task_complete", True, 5)
]

print("=== Agentic RL Learning Objective ===")
traj_result = agentic_obj.compute_trajectory_return([trajectory])
print(f"Trajectory Return: {traj_result['trajectory_return']:.3f}")
print(f"Undiscounted Sum: {traj_result['undiscounted_sum']:.3f}")
print(f"Discount Factor: {traj_result['discount_factor']}")

print("\nStep-by-Step Returns:")
for detail in traj_result['step_details']:
    print(f"  Step {detail['step']}: immediate={detail['immediate_reward']:.1f}, "
          f"discounted_return={detail['discounted_return']:.3f}")

# Show discount effect
print("\n=== Discount Effect Demonstration ===")
discount_demo = agentic_obj.demonstrate_discount_effect(10.0, 6)
for effect in discount_demo['discount_effects']:
    print(f"Step {effect['step']}: {effect['discounted_value']:.3f} "
          f"({effect['percentage_of_original']:.1f}% of original)")
```

## Mathematical Formulation Comparison

### Objective Function Analysis
```python
class ObjectiveMathAnalysis:
    """Mathematical analysis of objective functions"""
    
    @staticmethod
    def pbrft_objective_formula():
        """Mathematical formulation of PBRFT objective"""
        return {
            'formula': "J(θ) = E[R(s,a)] where a ~ π_θ(·|s)",
            'components': {
                'J(θ)': "Policy objective to maximize",
                'E[·]': "Expectation over prompt distribution",
                'R(s,a)': "Immediate reward for response",
                'π_θ(·|s)': "Policy (LLM) parameterized by θ",
                's': "Prompt/state",
                'a': "Generated response/action"
            },
            'properties': [
                "No temporal dependencies",
                "Each interaction independent", 
                "No discounting needed",
                "Gradient: ∇J = E[∇log π_θ(a|s) × R(s,a)]"
            ]
        }
    
    @staticmethod
    def agentic_rl_objective_formula():
        """Mathematical formulation of Agentic RL objective"""
        return {
            'formula': "J(θ) = E[Σ_{t=0}^T γ^t R(s_t, a_t)]",
            'components': {
                'J(θ)': "Policy objective to maximize",
                'E[·]': "Expectation over trajectory distribution",
                'T': "Episode length (variable)",
                'γ^t': "Discount factor (exponential decay)",
                'R(s_t, a_t)': "Reward at time step t",
                's_t, a_t': "State and action at time t"
            },
            'properties': [
                "Temporal dependencies crucial",
                "Credit assignment across time",
                "Future rewards discounted",
                "Gradient: ∇J = E[Σ_t γ^t ∇log π_θ(a_t|s_t) × G_t]",
                "G_t = Σ_{k=t}^T γ^{k-t} R(s_k, a_k) (return)"
            ]
        }
    
    @staticmethod
    def compare_convergence_properties():
        """Compare learning dynamics"""
        return {
            'pbrft': {
                'convergence': "Fast (single-step feedback)",
                'sample_efficiency': "High (direct supervision)", 
                'exploration': "Not applicable",
                'stability': "High (no temporal dependencies)"
            },
            'agentic_rl': {
                'convergence': "Slower (multi-step credit assignment)",
                'sample_efficiency': "Lower (sparse rewards, exploration)",
                'exploration': "Critical for learning",
                'stability': "Lower (high variance, temporal correlations)"
            }
        }

# Display mathematical analysis
math_analysis = ObjectiveMathAnalysis()

print("=== PBRFT Mathematical Formulation ===")
pbrft_math = math_analysis.pbrft_objective_formula()
print(f"Formula: {pbrft_math['formula']}")
print("Components:")
for comp, desc in pbrft_math['components'].items():
    print(f"  {comp}: {desc}")

print("\n=== Agentic RL Mathematical Formulation ===")
agentic_math = math_analysis.agentic_rl_objective_formula()
print(f"Formula: {agentic_math['formula']}")
print("Key Properties:")
for prop in agentic_math['properties']:
    print(f"  - {prop}")

print("\n=== Convergence Comparison ===")
convergence = math_analysis.compare_convergence_properties()
for approach, properties in convergence.items():
    print(f"{approach.upper()}:")
    for prop, value in properties.items():
        print(f"  {prop}: {value}")
```

### Discount Factor Impact Analysis
```python
def analyze_discount_factor_impact():
    """Demonstrate how γ affects learning behavior"""
    
    # Test different discount factors
    gammas = [0.1, 0.5, 0.9, 0.99, 1.0]
    
    # Fixed reward sequence: early low, late high
    rewards = [1.0, 1.0, 1.0, 10.0, 10.0]
    
    print("=== Discount Factor Impact Analysis ===")
    print("Reward sequence: [1.0, 1.0, 1.0, 10.0, 10.0]")
    print("Step:            [ 0 ,  1 ,  2 ,   3 ,   4 ]\n")
    
    for gamma in gammas:
        # Compute discounted return
        discounted_return = 0.0
        for t, reward in enumerate(rewards):
            discounted_return += reward * (gamma ** t)
        
        # Compute percentage of total reward from last step
        last_step_contribution = 10.0 * (gamma ** 4)
        percentage_from_last = (last_step_contribution / discounted_return) * 100
        
        print(f"γ = {gamma:4.2f}: Return = {discounted_return:6.2f}, "
              f"Last step = {percentage_from_last:5.1f}% of total")
    
    print("\nInterpretation:")
    print("  γ → 0: Only immediate rewards matter (myopic)")
    print("  γ → 1: All future rewards equally important (far-sighted)")
    print("  γ = 0.9-0.99: Balanced consideration of future")

analyze_discount_factor_impact()
```

## ASCII Diagram: Objective Comparison

```
PBRFT Objective (Immediate):
┌─────────┐    ┌─────────────┐    ┌─────────────┐
│ Prompt  │───►│  Response   │───►│  Reward     │
└─────────┘    └─────────────┘    └─────────────┘
      │               │                   │
      └───────── J = E[R(s,a)] ──────────┘
               No temporal dependencies

Agentic RL Objective (Sequential):
┌─────┐    ┌─────┐    ┌─────┐    ┌─────┐    ┌─────┐
│ S₁  │───►│ S₂  │───►│ S₃  │───►│ S₄  │───►│ S₅  │
└─────┘    └─────┘    └─────┘    └─────┘    └─────┘
   │          │          │          │          │
   ▼          ▼          ▼          ▼          ▼
  R₁         R₂         R₃         R₄         R₅
   │          │          │          │          │
   └────γ⁰───┼────γ¹────┼────γ²────┼────γ³────┘
              └────γ¹────┼────γ²────┘
                         └────γ¹────┘
J = E[Σₜ γᵗ Rₜ] - Temporal credit assignment

Discount Factor Effect:
                Future Reward Weight
γ = 0.1    ████▌
γ = 0.5    ██████████▌  
γ = 0.9    ████████████████████▌
γ = 0.99   ██████████████████████▌
γ = 1.0    ████████████████████████
           0%    25%   50%   75%   100%
```

## Training Algorithms Alignment

### Algorithm Requirements
```python
def algorithm_alignment_analysis():
    """How objective functions align with training algorithms"""
    
    alignments = {
        'pbrft': {
            'suitable_algorithms': [
                'DPO (Direct Preference Optimization)',
                'PPO with single-step episodes', 
                'REINFORCE with immediate rewards',
                'Supervised fine-tuning with preference labels'
            ],
            'key_requirements': [
                'Preference model for reward estimation',
                'Stable gradient estimation',
                'Human or AI feedback collection'
            ],
            'challenges': [
                'Reward model quality',
                'Human labeling consistency',
                'Distribution shift in preferences'
            ]
        },
        'agentic_rl': {
            'suitable_algorithms': [
                'PPO (Proximal Policy Optimization)',
                'A2C/A3C (Actor-Critic methods)',
                'TRPO (Trust Region Policy Optimization)',
                'GRPO (Group Relative Policy Optimization)'
            ],
            'key_requirements': [
                'Value function estimation',
                'Exploration strategy',
                'Credit assignment mechanism',
                'Baseline for variance reduction'
            ],
            'challenges': [
                'High variance gradients',
                'Exploration-exploitation balance',
                'Long episode training stability',
                'Reward sparsity'
            ]
        }
    }
    
    print("=== Algorithm Alignment Analysis ===")
    for objective_type, details in alignments.items():
        print(f"\n{objective_type.upper()} OBJECTIVES:")
        print("Suitable Algorithms:")
        for algo in details['suitable_algorithms']:
            print(f"  - {algo}")
        
        print("Key Requirements:")
        for req in details['key_requirements']:
            print(f"  - {req}")
            
        print("Main Challenges:")
        for challenge in details['challenges']:
            print(f"  - {challenge}")

algorithm_alignment_analysis()
```

## Key Differences Summary

| Aspect | PBRFT Objective | Agentic RL Objective |
|--------|-----------------|----------------------|
| **Formula** | J = E[R(s,a)] | J = E[Σₜ γᵗ R(s_t,a_t)] |
| **Time Horizon** | Single step | Multi-step trajectory |
| **Discounting** | None (γ = N/A) | Exponential (γ < 1) |
| **Dependencies** | Independent responses | Temporal correlations |
| **Credit Assignment** | Direct (response→reward) | Complex (action→future rewards) |
| **Optimization** | Immediate feedback | Sequential planning |
| **Variance** | Low (single measurements) | High (trajectory sampling) |

## Practical Exercises

```python
# Exercise 1: Implement discount factor sweep
def exercise_discount_sweep():
    """Test different γ values on sample trajectory"""
    pass

# Exercise 2: Compare gradient estimators
def exercise_gradient_comparison():
    """PBRFT vs Agentic RL gradient estimation"""
    pass

# Exercise 3: Design value function
def exercise_value_function():
    """Implement value function for your domain"""
    pass
```

## Resources

- **Survey Reference**: [Section 2.6, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **Policy Gradients**: [Sutton et al. - Policy Gradient Methods](https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)
- **Discount Factors**: [Bertsekas - Dynamic Programming and Optimal Control](http://www.athenasc.com/dpbook.html)
- **Credit Assignment**: [Minsky - Steps Toward Artificial Intelligence](https://web.media.mit.edu/~minsky/papers/steps.html)

## Next Steps

- **[2.7 RL Algorithms](2.7_RL_Algorithms.md)**: Algorithm families and implementation details
- **Practice**: Implement trajectory return calculation for your domain
- **Deep Dive**: Study actor-critic methods for long-horizon optimization

---

*Learning objectives shape everything else - the shift from immediate response optimization to long-term trajectory optimization enables the emergence of planning, memory, and strategic behavior in agentic systems.*
