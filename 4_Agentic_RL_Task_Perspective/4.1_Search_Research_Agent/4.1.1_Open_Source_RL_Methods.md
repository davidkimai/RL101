# 4.1.1 Open Source RL Methods

Open source search agent methods reveal a fundamental methodological divide: external knowledge approaches achieve higher information quality but face prohibitive scaling costs and training instability, while internal knowledge methods offer controlled training but suffer systematic hallucination and knowledge staleness. The survey evidence suggests no current approach successfully bridges this divide—requiring architectural innovations that the field hasn't yet developed.

## Key Takeaways
- **External Methods Crisis**: GRPO-trained policies face uncontrolled document quality and API cost explosion
- **Internal Methods Limitations**: Self-search approaches (ZeroSearch, SSRL) transfer poorly to real-world deployment
- **Supervision Paradox**: End-to-end PPO (ReSearch) struggles without trajectories, while supervised methods lack generalization
- **Multi-hop Credit Assignment Failure**: 40+ tool call sequences lack reliable temporal reward attribution

## Prerequisites Check

```bash
# Survey-referenced method implementations
python -c "import torch, transformers; print('GRPO/PPO implementation stack ready')"
python -c "import requests, json; print('External API simulation ready')"
python -c "import numpy as np; print('Credit assignment analysis tools ready')"

# Critical analysis prerequisites
echo "Do you understand why external API training costs explode exponentially?"
echo "Can you identify the reality gap between internal training and deployment?"
echo "Have you analyzed the supervised trajectory bootstrapping problem?"
```

## Hands-On: Open Source Method Analysis

### External Knowledge Method Implementation
```python
import torch
import torch.nn as nn
import numpy as np
import json
import time
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod

@dataclass
class TrainingMetrics:
    """Training metrics for search agent analysis"""
    episodes_completed: int
    total_api_calls: int
    training_cost: float
    quality_variance: float
    convergence_rate: float
    scalability_bottleneck: str

class DeepRetrievalGRPO:
    """Survey: 'framed one-shot query generation as a GRPO-trained policy'"""
    
    def __init__(self, api_budget: float = 500.0):
        self.api_budget = api_budget
        self.cost_per_call = 0.02  # Realistic search API cost
        self.episodes_trained = 0
        self.total_cost = 0.0
        
        # GRPO-specific hyperparameters from survey insights
        self.group_size = 4  # Typical GRPO group size
        self.temperature = 0.8
        self.quality_threshold = 0.7
        
        # Policy network (simplified)
        self.policy_network = nn.Sequential(
            nn.Linear(256, 128),  # Query encoding dimension
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32)     # Action space dimension
        )
        
    def train_grpo_episode(self, query: str, ground_truth: str) -> TrainingMetrics:
        """Survey finding: 'directly rewarded recall and relevance against live search results'"""
        
        episode_cost = 0.0
        episode_calls = 0
        
        # Generate group of candidate queries (GRPO requirement)
        candidate_queries = []
        group_rewards = []
        
        for i in range(self.group_size):
            if self.total_cost + self.cost_per_call > self.api_budget:
                raise RuntimeError(f"Training budget exhausted: {self.total_cost:.2f}/{self.api_budget}")
            
            # Generate query variation
            candidate_query = self._generate_query_variant(query, i)
            candidate_queries.append(candidate_query)
            
            # Simulate API call for search results
            search_results = self._simulate_external_search(candidate_query)
            episode_calls += 1
            episode_cost += self.cost_per_call
            self.total_cost += self.cost_per_call
            
            # Calculate reward: recall and relevance against ground truth
            reward = self._calculate_recall_relevance_reward(search_results, ground_truth)
            group_rewards.append(reward)
        
        # GRPO optimization: rank candidates within group
        group_advantages = self._calculate_group_advantages(group_rewards)
        
        # Policy update based on group ranking
        policy_loss = self._compute_grpo_loss(candidate_queries, group_advantages)
        
        # Update policy (simplified)
        self.policy_network.zero_grad()
        # policy_loss.backward() - simplified for demo
        
        self.episodes_trained += 1
        
        # Calculate training stability metrics
        quality_variance = np.var(group_rewards)
        
        return TrainingMetrics(
            episodes_completed=self.episodes_trained,
            total_api_calls=episode_calls,
            training_cost=episode_cost,
            quality_variance=quality_variance,
            convergence_rate=max(group_rewards) - min(group_rewards),
            scalability_bottleneck="API_COST_EXPLOSION"
        )
    
    def _generate_query_variant(self, base_query: str, variant_idx: int) -> str:
        """Generate query variations for GRPO group"""
        variations = [
            base_query,
            f"detailed {base_query}",
            f"{base_query} comprehensive analysis",
            f"recent research on {base_query}"
        ]
        return variations[variant_idx % len(variations)]
    
    def _simulate_external_search(self, query: str) -> List[Dict]:
        """Simulate external search API with realistic quality variance"""
        # Survey insight: "uncontrolled document quality brings instability"
        results = []
        for i in range(3):  # Typical search result count
            quality_noise = np.random.normal(0.6, 0.3)  # High variance
            result = {
                'content': f"Search result {i+1} for '{query}' - quality varies significantly...",
                'relevance': max(0.1, min(1.0, quality_noise)),
                'credibility': np.random.uniform(0.3, 0.9),
                'timestamp': time.time()
            }
            results.append(result)
        return results
    
    def _calculate_recall_relevance_reward(self, results: List[Dict], ground_truth: str) -> float:
        """Calculate recall and relevance reward against ground truth"""
        if not results:
            return 0.0
        
        # Simplified relevance calculation
        avg_relevance = np.mean([r['relevance'] for r in results])
        avg_credibility = np.mean([r['credibility'] for r in results])
        
        # Ground truth matching (simplified)
        content_overlap = 0.7 + np.random.normal(0, 0.2)  # Simulate matching
        
        return 0.4 * avg_relevance + 0.3 * avg_credibility + 0.3 * content_overlap
    
    def _calculate_group_advantages(self, rewards: List[float]) -> List[float]:
        """GRPO advantage calculation within group"""
        mean_reward = np.mean(rewards)
        advantages = [r - mean_reward for r in rewards]
        return advantages
    
    def _compute_grpo_loss(self, queries: List[str], advantages: List[float]) -> float:
        """Simplified GRPO loss computation"""
        # In reality, this would involve policy gradient calculations
        weighted_loss = sum(abs(adv) for adv in advantages)
        return weighted_loss

class R1SearcherPPO:
    """Survey: 'two-stage, cold-start PPO strategy—first learning when to invoke web search, then how to exploit it'"""
    
    def __init__(self):
        self.stage = 1  # Start with stage 1: when to search
        self.search_invocation_threshold = 0.5
        self.exploitation_strategy = None
        
        # PPO hyperparameters
        self.ppo_clip_epsilon = 0.2
        self.learning_rate = 3e-4
        self.episodes_per_stage = 100
        
    def train_two_stage_ppo(self, queries: List[str]) -> Dict:
        """Two-stage training: when to search → how to exploit"""
        
        stage_1_metrics = self._train_search_invocation(queries)
        
        # Transition to stage 2 after threshold episodes
        if self.episodes_trained >= self.episodes_per_stage:
            self.stage = 2
            stage_2_metrics = self._train_search_exploitation(queries)
            
            return {
                'stage_1': stage_1_metrics,
                'stage_2': stage_2_metrics,
                'training_challenge': 'Cold-start problem: poor initial search decisions create biased exploitation learning'
            }
        
        return {'stage_1': stage_1_metrics, 'stage_2': None}
    
    def _train_search_invocation(self, queries: List[str]) -> Dict:
        """Stage 1: Learn when to invoke search"""
        correct_decisions = 0
        
        for query in queries:
            # Determine if search is needed (ground truth simulation)
            needs_search = self._query_needs_search(query)
            
            # Policy decision
            search_probability = self._policy_search_decision(query)
            will_search = search_probability > self.search_invocation_threshold
            
            # Reward based on correctness
            if will_search == needs_search:
                correct_decisions += 1
        
        accuracy = correct_decisions / len(queries)
        
        return {
            'invocation_accuracy': accuracy,
            'threshold': self.search_invocation_threshold,
            'challenge': 'Learning when to search without knowing how to search effectively'
        }
    
    def _train_search_exploitation(self, queries: List[str]) -> Dict:
        """Stage 2: Learn how to exploit search results"""
        if self.stage != 2:
            return {'error': 'Must complete stage 1 first'}
        
        exploitation_rewards = []
        
        for query in queries:
            if self._policy_search_decision(query) > self.search_invocation_threshold:
                # Simulate search and exploitation
                search_results = self._simulate_search_results(query)
                exploitation_reward = self._calculate_exploitation_reward(query, search_results)
                exploitation_rewards.append(exploitation_reward)
        
        avg_reward = np.mean(exploitation_rewards) if exploitation_rewards else 0.0
        
        return {
            'exploitation_reward': avg_reward,
            'stage_transition_bias': 'Exploitation learning biased by poor stage 1 search decisions',
            'compound_error_risk': 'HIGH - errors compound across stages'
        }
    
    def _query_needs_search(self, query: str) -> bool:
        """Determine if query requires search (ground truth)"""
        # Simplified heuristics
        needs_search_indicators = ['recent', 'current', 'latest', 'news', 'today']
        return any(indicator in query.lower() for indicator in needs_search_indicators)
    
    def _policy_search_decision(self, query: str) -> float:
        """Policy network decision for search invocation"""
        # Simplified policy decision
        complexity_score = len(query.split()) / 20.0
        uncertainty_score = 0.5 + np.random.normal(0, 0.2)
        return min(1.0, complexity_score + uncertainty_score)
    
    def _simulate_search_results(self, query: str) -> List[Dict]:
        """Simulate search results for exploitation training"""
        return [
            {'content': f"Result for {query}", 'relevance': np.random.uniform(0.4, 0.9)}
            for _ in range(3)
        ]
    
    def _calculate_exploitation_reward(self, query: str, results: List[Dict]) -> float:
        """Calculate reward for search exploitation"""
        if not results:
            return 0.0
        avg_relevance = np.mean([r['relevance'] for r in results])
        return avg_relevance

class ReSearchEndToEndPPO:
    """Survey: 'pursues fully end-to-end PPO without supervised tool-use trajectories'"""
    
    def __init__(self):
        self.training_episodes = 0
        self.convergence_struggles = []
        
        # PPO without supervision parameters
        self.exploration_bonus = 0.1
        self.reward_sparsity_factor = 0.8  # High sparsity without supervision
        
    def train_unsupervised_ppo(self, episodes: int = 50) -> Dict:
        """Train end-to-end PPO without supervised trajectories"""
        
        convergence_difficulties = []
        episode_rewards = []
        
        for episode in range(episodes):
            # Generate search trajectory without supervision
            trajectory = self._generate_exploration_trajectory()
            
            # Calculate sparse reward (only at end of trajectory)
            final_reward = self._calculate_sparse_reward(trajectory)
            episode_rewards.append(final_reward)
            
            # Track convergence issues
            if episode > 10:
                recent_variance = np.var(episode_rewards[-10:])
                if recent_variance > 0.5:  # High variance indicates convergence issues
                    convergence_difficulties.append(f"Episode {episode}: High reward variance {recent_variance:.3f}")
        
        self.training_episodes += episodes
        
        # Analyze training challenges
        avg_reward = np.mean(episode_rewards)
        convergence_quality = self._assess_convergence(episode_rewards)
        
        return {
            'average_reward': avg_reward,
            'convergence_quality': convergence_quality,
            'training_challenges': convergence_difficulties,
            'supervision_gap': 'Without supervised trajectories, exploration is inefficient and convergence is slow',
            'sample_efficiency': 'LOW - requires significantly more episodes than supervised approaches'
        }
    
    def _generate_exploration_trajectory(self) -> List[Dict]:
        """Generate search trajectory through exploration (no supervision)"""
        trajectory = []
        max_steps = 5
        
        for step in range(max_steps):
            # Random exploration without supervision
            action_type = np.random.choice(['search', 'synthesize', 'refine'])
            
            trajectory.append({
                'step': step,
                'action_type': action_type,
                'exploration_reward': np.random.uniform(0, 0.3),  # Low exploration rewards
                'quality_uncertainty': 'HIGH - no supervision to guide exploration'
            })
        
        return trajectory
    
    def _calculate_sparse_reward(self, trajectory: List[Dict]) -> float:
        """Calculate sparse reward only at trajectory end"""
        # Sparse reward makes credit assignment very difficult
        final_quality = np.random.uniform(0.2, 0.8)  # Highly variable without supervision
        
        # Survey insight: without supervision, reward signals are sparse and noisy
        noise_factor = 0.3
        noisy_reward = final_quality + np.random.normal(0, noise_factor)
        
        return max(0.0, min(1.0, noisy_reward))
    
    def _assess_convergence(self, rewards: List[float]) -> Dict:
        """Assess convergence quality of unsupervised training"""
        if len(rewards) < 10:
            return {'assessment': 'Insufficient data'}
        
        # Rolling variance as convergence indicator
        recent_variance = np.var(rewards[-10:])
        early_variance = np.var(rewards[:10])
        
        variance_reduction = early_variance - recent_variance
        
        return {
            'variance_reduction': variance_reduction,
            'convergence_stability': 'POOR' if recent_variance > 0.4 else 'MODERATE',
            'supervision_impact': 'Lack of supervised trajectories leads to poor sample efficiency'
        }

# Internal Knowledge Method Analysis
class ZeroSearchInternal:
    """Survey: 'replaces live web retrieval with a pseudo search engine distilled from LLMs themselves'"""
    
    def __init__(self):
        self.internal_knowledge_base = self._initialize_knowledge_base()
        self.hallucination_rate = 0.18  # Realistic internal knowledge limitation
        self.knowledge_staleness = 0.22
        self.pseudo_search_efficiency = 0.95  # High efficiency, low cost
        
    def train_internal_search(self, queries: List[str]) -> Dict:
        """Train on internal knowledge with pseudo search engine"""
        
        training_results = []
        hallucination_incidents = 0
        stale_information_incidents = 0
        
        for query in queries:
            # Pseudo search on internal knowledge
            pseudo_results = self._pseudo_search(query)
            
            # Evaluate result quality
            quality_metrics = self._evaluate_internal_results(pseudo_results, query)
            
            if quality_metrics['hallucinated']:
                hallucination_incidents += 1
            if quality_metrics['stale']:
                stale_information_incidents += 1
            
            training_results.append(quality_metrics)
        
        # Calculate training effectiveness
        avg_quality = np.mean([r['quality_score'] for r in training_results])
        
        return {
            'average_quality': avg_quality,
            'hallucination_rate': hallucination_incidents / len(queries),
            'staleness_rate': stale_information_incidents / len(queries),
            'training_stability': 'HIGH - controlled environment',
            'deployment_risk': 'SEVERE - reality gap between training and live deployment',
            'scalability': 'EXCELLENT - no API costs'
        }
    
    def _initialize_knowledge_base(self) -> Dict:
        """Initialize internal knowledge base (simplified)"""
        return {
            'technical_topics': ['reinforcement learning', 'machine learning', 'neural networks'],
            'general_knowledge': ['history', 'science', 'literature'],
            'knowledge_cutoff': '2024-01',
            'coverage_gaps': ['very recent events', 'specialized domains', 'real-time data']
        }
    
    def _pseudo_search(self, query: str) -> List[Dict]:
        """Perform pseudo search on internal knowledge"""
        results = []
        
        # Match query against internal knowledge
        for topic_category, topics in self.internal_knowledge_base.items():
            if topic_category == 'coverage_gaps':
                continue
                
            for topic in topics:
                if any(word in topic.lower() for word in query.lower().split()):
                    result = {
                        'content': f"Internal knowledge about {topic} related to {query}",
                        'source': 'internal_knowledge',
                        'confidence': np.random.uniform(0.6, 0.9),
                        'category': topic_category
                    }
                    results.append(result)
        
        # Add some hallucinated results
        if np.random.random() < self.hallucination_rate:
            results.append({
                'content': f"Hallucinated information about {query}",
                'source': 'hallucination',
                'confidence': np.random.uniform(0.7, 0.9),  # High confidence in wrong info
                'category': 'hallucinated'
            })
        
        return results[:3]  # Return top 3 results
    
    def _evaluate_internal_results(self, results: List[Dict], query: str) -> Dict:
        """Evaluate quality of internal search results"""
        if not results:
            return {'quality_score': 0.0, 'hallucinated': False, 'stale': False}
        
        # Check for hallucinations
        hallucinated = any(r.get('category') == 'hallucinated' for r in results)
        
        # Check for stale information (time-sensitive queries)
        time_sensitive_indicators = ['recent', 'latest', 'current', '2024', '2025']
        is_time_sensitive = any(indicator in query.lower() for indicator in time_sensitive_indicators)
        stale = is_time_sensitive and np.random.random() < self.knowledge_staleness
        
        # Calculate quality score
        base_quality = 0.75  # Generally higher than external due to consistency
        
        quality_penalties = 0.0
        if hallucinated:
            quality_penalties += 0.4
        if stale:
            quality_penalties += 0.3
        
        final_quality = max(0.1, base_quality - quality_penalties)
        
        return {
            'quality_score': final_quality,
            'hallucinated': hallucinated,
            'stale': stale,
            'consistency_advantage': 'Internal methods provide consistent quality',
            'reality_gap_risk': 'Deployment performance differs significantly from training'
        }

# Demonstration: Method Comparison
def demonstrate_open_source_methods():
    """Compare external vs internal open source approaches"""
    
    print("=== Open Source RL Methods Analysis ===")
    
    # External method analysis - DeepRetrieval GRPO
    try:
        deep_retrieval = DeepRetrievalGRPO(api_budget=50.0)  # Limited budget
        grpo_metrics = deep_retrieval.train_grpo_episode(
            "recent advances in search agents", 
            "RL-based search agents show promise but face scalability challenges"
        )
        
        print(f"DeepRetrieval GRPO Results:")
        print(f"  Training cost: ${grpo_metrics.training_cost:.2f}")
        print(f"  Quality variance: {grpo_metrics.quality_variance:.3f}")
        print(f"  Scalability bottleneck: {grpo_metrics.scalability_bottleneck}")
        
    except RuntimeError as e:
        print(f"DeepRetrieval failed: {e}")
    
    # Two-stage PPO analysis
    r1_searcher = R1SearcherPPO()
    ppo_results = r1_searcher.train_two_stage_ppo([
        "recent research in RL",
        "current state of search agents", 
        "machine learning basics"
    ])
    
    print(f"\nR1-Searcher Two-Stage PPO:")
    print(f"  Stage 1 accuracy: {ppo_results['stage_1']['invocation_accuracy']:.3f}")
    print(f"  Challenge: {ppo_results['stage_1']['challenge']}")
    
    # End-to-end PPO without supervision
    research_agent = ReSearchEndToEndPPO()
    unsupervised_results = research_agent.train_unsupervised_ppo(episodes=20)
    
    print(f"\nReSearch End-to-End PPO:")
    print(f"  Average reward: {unsupervised_results['average_reward']:.3f}")
    print(f"  Sample efficiency: {unsupervised_results['sample_efficiency']}")
    print(f"  Supervision gap: {unsupervised_results['supervision_gap']}")
    
    # Internal knowledge analysis
    zero_search = ZeroSearchInternal()
    internal_results = zero_search.train_internal_search([
        "reinforcement learning for search",
        "recent AI developments",
        "machine learning fundamentals"
    ])
    
    print(f"\nZeroSearch Internal Knowledge:")
    print(f"  Training quality: {internal_results['average_quality']:.3f}")
    print(f"  Hallucination rate: {internal_results['hallucination_rate']:.1%}")
    print(f"  Deployment risk: {internal_results['deployment_risk']}")

# Run analysis
demonstrate_open_source_methods()
```

## Critical Method Analysis

The survey reveals fundamental architectural problems in current open source approaches:

**External Method Failure Modes:**
- **DeepRetrieval GRPO**: Query optimization works but API costs scale exponentially with training episodes
- **R1-Searcher PPO**: Two-stage training creates compound errors—poor stage 1 decisions bias stage 2 exploitation learning  
- **WebDancer/WebSailor**: Human trajectory supervision doesn't transfer to novel domains or tasks
- **ASearcher 40+ tool calls**: Long horizons amplify credit assignment problems rather than solving them

**Internal Method Reality Gap:**
- **ZeroSearch**: Pseudo search engines lack the complexity and noise of real information environments
- **SSRL**: Self-search training creates an optimization target misaligned with deployment realities
- **Hallucination Paradox**: High training consistency leads to overconfidence in incorrect information

**Supervision vs Exploration Contradiction:**
- **Supervised approaches** (WebDancer, R1-Searcher+) rely on human demonstrations that don't generalize
- **Unsupervised approaches** (ReSearch) have poor sample efficiency and struggle with sparse rewards
- **No middle ground**: Current methods either overfit to demonstrations or underperform due to exploration challenges

## Survey Method Taxonomy

| Method | Approach | Innovation | Fatal Limitation |
|--------|----------|------------|------------------|
| **DeepRetrieval** | GRPO one-shot | Direct recall/relevance reward | API cost explosion |
| **Search-R1** | Retrieved-token masking | Query-answer interleaving | Training instability |
| **R1-Searcher** | Two-stage PPO | When→how decomposition | Compound error propagation |
| **ReSearch** | End-to-end PPO | No supervised trajectories | Sample inefficiency crisis |
| **StepSearch** | Step-level rewards | Multi-hop credit assignment | Intermediate reward hacking |
| **ZeroSearch** | Pseudo search engine | Cost-free training | Reality gap at deployment |
| **WebDancer** | Human trajectory + RL | Demonstration grounding | Domain transfer failure |

## Resources

- **Survey Methods**: Table 4 lists 15+ open source implementations with GitHub links
- **Technical Details**: Section 4.1.1 provides algorithmic specifics for each approach
- **GRPO Implementation**: Group Relative Policy Optimization for search query generation
- **Multi-hop Benchmarks**: GAIA, WebWalkerQA referenced for evaluation protocols
- **Cost Analysis**: API pricing models and scalability constraints from survey findings

## Next Steps

- **[4.1.2 Closed Source RL Methods](4.1.2_Closed_Source_RL_Methods.md)**: Industry approaches achieving 51.5% BrowseComp performance
- **Implementation Challenge**: Design hybrid approaches addressing external-internal knowledge trade-offs
- **Critical Evaluation**: Test assumptions about supervised vs unsupervised learning in search domains

---

*Open source methods expose the fundamental scaling and generalization challenges in search agent RL—challenges that existing approaches address through architectural compromises that limit their practical deployment effectiveness.*
