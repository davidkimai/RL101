# 4.2.1 RL for Code Generation

Single-turn code generation represents the most successful application of agentic RL, achieving 60.6% success on LiveCodeBench (DeepCoder-14B). However, the survey reveals a fundamental tension: outcome methods achieve higher peak performance but suffer from sparse rewards, while process methods improve sample efficiency but introduce intermediate reward hacking vulnerabilities. No approach successfully combines both advantages without trade-offs.

## Key Takeaways
- **Outcome Reward Dominance**: Direct optimization for unit-test success achieves highest performance (60.6% LiveCodeBench)
- **Process Reward Trade-off**: Dense intermediate signals improve convergence (+10.5% PRLCoder gains) but enable reward hacking
- **GRPO+ Stability**: Careful clipping and entropy modulation prevent reward hacking and policy collapse in large-scale training
- **Self-Play Limitations**: Absolute Zero self-generated tasks plateau at complexity levels insufficient for real-world problems

## Prerequisites Check

```bash
# Code generation and testing frameworks
python -c "import torch, transformers, datasets; print('ML training stack ready')"
python -c "import subprocess, ast, unittest; print('Code execution tools ready')"
python -c "import numpy as np; print('Reward computation tools ready')"

# Critical understanding check
echo "Do you understand why unit-test rewards achieve higher performance than process rewards?"
echo "Can you identify the reward hacking risks in intermediate supervision approaches?"
echo "Have you analyzed the GRPO+ stability improvements over standard PPO?"
```

## Hands-On: Code Generation Method Analysis

### Outcome vs Process Reward Implementation
```python
import torch
import torch.nn as nn
import subprocess
import ast
import time
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod

@dataclass
class CodeGenerationResult:
    """Results from code generation with different reward types"""
    generated_code: str
    compilation_success: bool
    unit_tests_passed: int
    total_unit_tests: int
    execution_time: float
    outcome_reward: float
    process_rewards: List[float]
    reward_type: str

class OutcomeRewardTrainer:
    """Survey: 'optimize directly for final correctness, typically measured by pass@k or unit-test success'"""
    
    def __init__(self):
        # Survey evidence: outcome methods dominate performance
        self.performance_benchmarks = {
            'DeepCoder-14B': {'livecodeench_pass1': 0.606, 'improvement': 0.08},  # +8% gains
            'AceCoder': {'stability_focus': 'software_maintenance', 'setting': 'single_turn'},
            'RLTF': {'granularity': 'multi_granularity', 'signals': 'coarse_to_fine'},
            'CURE': {'innovation': 'coder_tester_coevolution'}
        }
        
    def train_outcome_reward(self, code_prompt: str, unit_tests: List[Dict]) -> CodeGenerationResult:
        """Train using outcome-only rewards (unit-test success)"""
        
        start_time = time.time()
        
        # Generate code (simplified - in practice uses LLM policy)
        generated_code = self._generate_code_sample(code_prompt)
        
        # Execute and test (outcome reward calculation)
        compilation_success = self._check_compilation(generated_code)
        tests_passed = 0
        
        if compilation_success:
            tests_passed = self._run_unit_tests(generated_code, unit_tests)
        
        # Calculate outcome reward (binary success/failure)
        outcome_reward = tests_passed / len(unit_tests) if unit_tests else 0.0
        
        execution_time = time.time() - start_time
        
        return CodeGenerationResult(
            generated_code=generated_code,
            compilation_success=compilation_success,
            unit_tests_passed=tests_passed,
            total_unit_tests=len(unit_tests),
            execution_time=execution_time,
            outcome_reward=outcome_reward,
            process_rewards=[],  # No intermediate rewards
            reward_type="outcome"
        )
    
    def _generate_code_sample(self, prompt: str) -> str:
        """Simulate code generation (simplified)"""
        if "fibonacci" in prompt.lower():
            return """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
"""
        elif "factorial" in prompt.lower():
            return """
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n-1)
"""
        else:
            return "# Generated code placeholder\npass"
    
    def _check_compilation(self, code: str) -> bool:
        """Check if code compiles"""
        try:
            ast.parse(code)
            return True
        except SyntaxError:
            return False
    
    def _run_unit_tests(self, code: str, tests: List[Dict]) -> int:
        """Run unit tests and return number passed"""
        passed = 0
        try:
            exec_globals = {}
            exec(code, exec_globals)
            
            for test in tests:
                function_name = test['function']
                inputs = test['inputs']
                expected = test['expected']
                
                if function_name in exec_globals:
                    func = exec_globals[function_name]
                    try:
                        result = func(*inputs)
                        if result == expected:
                            passed += 1
                    except Exception:
                        continue
        except Exception:
            pass
        
        return passed

class ProcessRewardTrainer:
    """Survey: 'provide intermediate supervision over compilation, partial execution, or reasoning steps'"""
    
    def __init__(self):
        # Survey evidence: process methods improve sample efficiency
        self.process_improvements = {
            'PRLCoder': {'improvement': 0.105, 'mechanism': 'line_level_supervision'},  # +10.5%
            'StepCoder': {'decomposition': 'compilation_execution_steps'},
            'PSGPO': {'signals': 'error_traces_process_annotations'},
            'CodeBoost': {'unification': 'heterogeneous_execution_signals'}
        }
        
        # Survey insight: "dense shaping at the line-level can guide code synthesis more effectively"
        self.reward_granularity = ['syntax_check', 'semantic_analysis', 'partial_execution', 'full_test']
        
    def train_process_reward(self, code_prompt: str, unit_tests: List[Dict]) -> CodeGenerationResult:
        """Train using process rewards (intermediate supervision)"""
        
        start_time = time.time()
        
        generated_code = self._generate_code_with_process_feedback(code_prompt)
        
        # Calculate process rewards at each step
        process_rewards = self._calculate_process_rewards(generated_code, unit_tests)
        
        # Final outcome reward
        compilation_success = self._check_compilation(generated_code)
        tests_passed = 0
        
        if compilation_success:
            tests_passed = self._run_unit_tests(generated_code, unit_tests)
            
        # Survey insight: process methods risk "speculative reward exploitation"
        outcome_reward = tests_passed / len(unit_tests) if unit_tests else 0.0
        
        # Detect potential reward hacking
        process_outcome_alignment = self._assess_reward_alignment(process_rewards, outcome_reward)
        
        execution_time = time.time() - start_time
        
        result = CodeGenerationResult(
            generated_code=generated_code,
            compilation_success=compilation_success,
            unit_tests_passed=tests_passed,
            total_unit_tests=len(unit_tests),
            execution_time=execution_time,
            outcome_reward=outcome_reward,
            process_rewards=process_rewards,
            reward_type="process"
        )
        
        # Add reward hacking assessment
        result.reward_hacking_risk = 1.0 - process_outcome_alignment
        
        return result
    
    def _generate_code_with_process_feedback(self, prompt: str) -> str:
        """Generate code with intermediate process feedback"""
        # Simulate step-by-step generation with intermediate rewards
        if "fibonacci" in prompt.lower():
            return """
def fibonacci(n):
    # Step 1: Handle base cases (process reward: syntax correct)
    if n <= 1:
        return n
    # Step 2: Recursive implementation (process reward: logic reasonable)
    return fibonacci(n-1) + fibonacci(n-2)
"""
        else:
            return "# Process-guided generation\npass"
    
    def _calculate_process_rewards(self, code: str, tests: List[Dict]) -> List[float]:
        """Calculate intermediate process rewards"""
        rewards = []
        
        # Syntax reward (compilation success)
        syntax_reward = 1.0 if self._check_compilation(code) else 0.0
        rewards.append(syntax_reward)
        
        # Semantic analysis reward (simplified - check for reasonable patterns)
        semantic_reward = 0.8 if 'def ' in code and 'return' in code else 0.3
        rewards.append(semantic_reward)
        
        # Partial execution reward (simplified)
        partial_reward = 0.7 if 'if' in code or 'for' in code or 'while' in code else 0.4
        rewards.append(partial_reward)
        
        # Test alignment reward (how well structure matches test requirements)
        test_alignment = self._assess_test_alignment(code, tests)
        rewards.append(test_alignment)
        
        return rewards
    
    def _assess_test_alignment(self, code: str, tests: List[Dict]) -> float:
        """Assess how well code structure aligns with test requirements"""
        if not tests:
            return 0.5
        
        # Simple heuristic: check if function names in tests appear in code
        alignment_score = 0.0
        for test in tests:
            function_name = test.get('function', '')
            if function_name and function_name in code:
                alignment_score += 1.0
        
        return alignment_score / len(tests)
    
    def _assess_reward_alignment(self, process_rewards: List[float], outcome_reward: float) -> float:
        """Assess alignment between process and outcome rewards"""
        if not process_rewards:
            return 1.0
        
        # Calculate correlation between process reward trend and final outcome
        avg_process_reward = np.mean(process_rewards)
        
        # Perfect alignment would have high process rewards → high outcome reward
        if avg_process_reward > 0.7 and outcome_reward > 0.7:
            return 1.0  # Good alignment
        elif avg_process_reward < 0.3 and outcome_reward < 0.3:
            return 0.8  # Consistent failure
        elif abs(avg_process_reward - outcome_reward) < 0.2:
            return 0.9  # Good correlation
        else:
            return 0.3  # Poor alignment - potential reward hacking
    
    def _check_compilation(self, code: str) -> bool:
        """Check if code compiles"""
        try:
            ast.parse(code)
            return True
        except SyntaxError:
            return False
    
    def _run_unit_tests(self, code: str, tests: List[Dict]) -> int:
        """Run unit tests and return number passed"""
        passed = 0
        try:
            exec_globals = {}
            exec(code, exec_globals)
            
            for test in tests:
                function_name = test['function']
                inputs = test['inputs']
                expected = test['expected']
                
                if function_name in exec_globals:
                    func = exec_globals[function_name]
                    try:
                        result = func(*inputs)
                        if result == expected:
                            passed += 1
                    except Exception:
                        continue
        except Exception:
            pass
        
        return passed

class GRPOPlusStabilityDemo:
    """Survey: DeepCoder-14B 'GRPO+, an improved proximal policy optimization variant'"""
    
    def __init__(self):
        # GRPO+ innovations from survey
        self.stability_features = {
            'careful_clipping': 'mitigates_reward_hacking',
            'entropy_modulation': 'prevents_policy_collapse',
            'distributed_training': 'scales_to_14B_parameters'
        }
        
        # Performance evidence
        self.benchmark_results = {
            'livecodeench_pass1': 0.606,  # 60.6% on LiveCodeBench
            'improvement_over_baseline': 0.08,  # +8% Pass@1 gains
            'comparison': 'matches_much_larger_proprietary_coders'
        }
    
    def demonstrate_grpo_plus_stability(self, training_episodes: int = 100) -> Dict:
        """Demonstrate GRPO+ stability improvements over standard PPO"""
        
        # Simulate training trajectories
        standard_ppo_rewards = []
        grpo_plus_rewards = []
        
        for episode in range(training_episodes):
            # Standard PPO: less stable, prone to collapse
            if episode < 50:
                ppo_reward = 0.3 + 0.2 * np.sin(episode * 0.1) + np.random.normal(0, 0.15)
            else:
                # Simulate policy collapse
                ppo_reward = 0.1 + np.random.normal(0, 0.05)
            
            standard_ppo_rewards.append(max(0, min(1, ppo_reward)))
            
            # GRPO+: more stable due to careful clipping and entropy modulation
            base_improvement = min(episode * 0.005, 0.4)  # Gradual improvement
            stability_bonus = 0.1  # Entropy modulation benefit
            grpo_reward = 0.3 + base_improvement + stability_bonus + np.random.normal(0, 0.08)
            
            grpo_plus_rewards.append(max(0, min(1, grpo_reward)))
        
        # Calculate stability metrics
        ppo_variance = np.var(standard_ppo_rewards)
        grpo_variance = np.var(grpo_plus_rewards)
        
        ppo_final_performance = np.mean(standard_ppo_rewards[-20:])
        grpo_final_performance = np.mean(grpo_plus_rewards[-20:])
        
        return {
            'standard_ppo': {
                'final_performance': ppo_final_performance,
                'variance': ppo_variance,
                'stability': 'LOW - prone to collapse'
            },
            'grpo_plus': {
                'final_performance': grpo_final_performance,
                'variance': grpo_variance,  
                'stability': 'HIGH - careful clipping prevents collapse'
            },
            'improvement_analysis': {
                'performance_gain': grpo_final_performance - ppo_final_performance,
                'stability_gain': ppo_variance - grpo_variance,
                'survey_evidence': '60.6% LiveCodeBench performance validates stability'
            },
            'stability_mechanisms': self.stability_features
        }

class CURECoevolutionDemo:
    """Survey: CURE 'formalizes coder–tester co-evolution'"""
    
    def __init__(self):
        # CURE innovation: joint coder-tester training
        self.coevolution_dynamics = {
            'coder_role': 'iteratively_patches_code',
            'tester_role': 'generates_evolves_unit_tests',
            'joint_objective': 'reward_precision_mitigates_low_quality_tests'
        }
        
    def simulate_coder_tester_coevolution(self, iterations: int = 10) -> Dict:
        """Simulate CURE's coder-tester co-evolution dynamics"""
        
        evolution_history = []
        
        # Initial state
        coder_quality = 0.3
        tester_quality = 0.2
        task_complexity = 0.4
        
        for iteration in range(iterations):
            # Tester generates/evolves tests
            # Survey: "tester generates or evolves unit tests"
            new_test_quality = min(0.95, tester_quality + np.random.normal(0.05, 0.02))
            
            # Test quality affects code generation difficulty
            effective_difficulty = task_complexity * (1 + 0.5 * new_test_quality)
            
            # Coder iteratively patches code
            # Survey: "coder iteratively patches code"
            if effective_difficulty < coder_quality:
                code_success_rate = 0.8 + 0.2 * (coder_quality - effective_difficulty)
            else:
                code_success_rate = 0.3 + 0.5 * (coder_quality / effective_difficulty)
            
            # Co-evolution feedback
            # Survey: "reward-precision objective mitigates low-quality test effects"
            if code_success_rate > 0.6 and new_test_quality > 0.5:
                # Both improve together
                coder_quality = min(0.9, coder_quality + 0.05)
                tester_quality = min(0.9, new_test_quality + 0.03)
            elif code_success_rate < 0.3:
                # Coder needs improvement
                coder_quality = max(0.1, coder_quality + np.random.normal(0.02, 0.01))
            elif new_test_quality < 0.4:
                # Tester generating low-quality tests
                tester_quality = max(0.1, tester_quality - 0.02)
            
            evolution_history.append({
                'iteration': iteration,
                'coder_quality': coder_quality,
                'tester_quality': tester_quality,
                'code_success_rate': code_success_rate,
                'effective_difficulty': effective_difficulty
            })
        
        # Analyze co-evolution dynamics
        final_coder = coder_quality
        final_tester = tester_quality
        improvement_coder = final_coder - 0.3
        improvement_tester = final_tester - 0.2
        
        return {
            'evolution_trajectory': evolution_history,
            'final_performance': {
                'coder_quality': final_coder,
                'tester_quality': final_tester,
                'joint_effectiveness': (final_coder + final_tester) / 2
            },
            'improvement_analysis': {
                'coder_improvement': improvement_coder,
                'tester_improvement': improvement_tester,
                'coevolution_benefit': 'Joint training prevents low-quality test effects'
            },
            'survey_validation': 'CURE demonstrates successful coder-tester joint optimization'
        }

class AbsoluteZeroSelfPlayAnalysis:
    """Survey: Absolute Zero 'applies self-play RL without human data'"""
    
    def __init__(self):
        # Absolute Zero approach
        self.self_play_mechanism = {
            'task_generation': 'generates_coding_tasks_for_itself',
            'verification': 'uses_execution_outcomes_as_verifiable_rewards',
            'bootstrapping': 'bootstrap_reasoning_ability'
        }
        
    def analyze_self_play_limitations(self) -> Dict:
        """Analyze why self-generated tasks limit complexity growth"""
        
        # Simulate self-play task generation over time
        task_complexity_progression = []
        reasoning_ability = 0.3
        
        for round in range(50):
            # Self-generated task complexity limited by current ability
            # Survey insight: tasks must be solvable by current model to provide rewards
            max_generatable_complexity = reasoning_ability * 0.9  # Can't generate significantly harder tasks
            
            # Task generation with slight exploration
            generated_task_complexity = max_generatable_complexity + np.random.normal(0, 0.05)
            generated_task_complexity = max(0.1, min(0.8, generated_task_complexity))
            
            # Learning from self-generated tasks
            if generated_task_complexity <= reasoning_ability * 1.1:
                # Successfully solve → modest improvement
                reasoning_improvement = 0.01 * (generated_task_complexity / reasoning_ability)
                reasoning_ability = min(0.9, reasoning_ability + reasoning_improvement)
            else:
                # Task too hard → no improvement
                reasoning_improvement = 0.0
            
            task_complexity_progression.append({
                'round': round,
                'task_complexity': generated_task_complexity,
                'reasoning_ability': reasoning_ability,
                'improvement': reasoning_improvement
            })
        
        # Analyze plateau behavior
        final_rounds = task_complexity_progression[-10:]
        final_complexity = np.mean([r['task_complexity'] for r in final_rounds])
        final_ability = np.mean([r['reasoning_ability'] for r in final_rounds])
        
        plateau_detection = np.std([r['reasoning_ability'] for r in final_rounds]) < 0.01
        
        return {
            'progression_trajectory': task_complexity_progression,
            'final_metrics': {
                'average_task_complexity': final_complexity,
                'reasoning_ability': final_ability,
                'plateau_reached': plateau_detection
            },
            'limitation_analysis': {
                'complexity_ceiling': 'Self-generated tasks limited by current model capability',
                'bootstrap_paradox': 'Cannot generate tasks significantly harder than current ability',
                'external_curriculum_need': 'Requires external task sources for continued complexity growth'
            },
            'survey_context': 'Absolute Zero demonstrates self-play potential but complexity scaling challenges'
        }

# Comprehensive Method Comparison
def demonstrate_code_generation_methods():
    """Compare outcome vs process rewards with stability and self-play analysis"""
    
    print("=== Code Generation Method Comparison ===")
    
    # Test cases for comparison
    fibonacci_tests = [
        {'function': 'fibonacci', 'inputs': [0], 'expected': 0},
        {'function': 'fibonacci', 'inputs': [1], 'expected': 1},
        {'function': 'fibonacci', 'inputs': [5], 'expected': 5},
        {'function': 'fibonacci', 'inputs': [8], 'expected': 21}
    ]
    
    # 1. Outcome vs Process Reward Comparison
    outcome_trainer = OutcomeRewardTrainer()
    process_trainer = ProcessRewardTrainer()
    
    outcome_result = outcome_trainer.train_outcome_reward("Generate fibonacci function", fibonacci_tests)
    process_result = process_trainer.train_process_reward("Generate fibonacci function", fibonacci_tests)
    
    print(f"Outcome Reward Method:")
    print(f"  Final Success Rate: {outcome_result.outcome_reward:.1%}")
    print(f"  Tests Passed: {outcome_result.unit_tests_passed}/{outcome_result.total_unit_tests}")
    print(f"  Reward Type: Sparse (only final outcome)")
    
    print(f"\nProcess Reward Method:")
    print(f"  Final Success Rate: {process_result.outcome_reward:.1%}")
    print(f"  Tests Passed: {process_result.unit_tests_passed}/{process_result.total_unit_tests}")
    print(f"  Process Rewards: {[f'{r:.2f}' for r in process_result.process_rewards]}")
    print(f"  Reward Hacking Risk: {process_result.reward_hacking_risk:.1%}")
    
    # 2. GRPO+ Stability Analysis
    grpo_demo = GRPOPlusStabilityDemo()
    stability_results = grpo_demo.demonstrate_grpo_plus_stability()
    
    print(f"\nGRPO+ vs Standard PPO Stability:")
    print(f"  Standard PPO Final Performance: {stability_results['standard_ppo']['final_performance']:.1%}")
    print(f"  GRPO+ Final Performance: {stability_results['grpo_plus']['final_performance']:.1%}")
    print(f"  Performance Improvement: {stability_results['improvement_analysis']['performance_gain']:.1%}")
    print(f"  Survey Evidence: {stability_results['improvement_analysis']['survey_evidence']}")
    
    # 3. CURE Co-evolution Analysis
    cure_demo = CURECoevolutionDemo()
    coevo_results = cure_demo.simulate_coder_tester_coevolution()
    
    print(f"\nCURE Coder-Tester Co-evolution:")
    print(f"  Final Coder Quality: {coevo_results['final_performance']['coder_quality']:.1%}")
    print(f"  Final Tester Quality: {coevo_results['final_performance']['tester_quality']:.1%}")
    print(f"  Joint Effectiveness: {coevo_results['final_performance']['joint_effectiveness']:.1%}")
    
    # 4. Absolute Zero Self-Play Limitations
    selfplay_analysis = AbsoluteZeroSelfPlayAnalysis()
    selfplay_results = selfplay_analysis.analyze_self_play_limitations()
    
    print(f"\nAbsolute Zero Self-Play Analysis:")
    print(f"  Final Task Complexity: {selfplay_results['final_metrics']['average_task_complexity']:.1%}")
    print(f"  Reasoning Ability: {selfplay_results['final_metrics']['reasoning_ability']:.1%}")
    print(f"  Plateau Reached: {selfplay_results['final_metrics']['plateau_reached']}")
    print(f"  Key Limitation: {selfplay_results['limitation_analysis']['complexity_ceiling']}")

# Run comprehensive analysis
demonstrate_code_generation_methods()
```

## Critical Analysis: The Reward Signal Paradox

The survey's 21 code generation methods reveal a fundamental paradox: outcome rewards achieve the highest performance (DeepCoder-14B: 60.6% LiveCodeBench) but process rewards show the most promising architectural innovations. This exposes critical tensions in current approaches:

**Outcome Method Dominance:**
- DeepCoder-14B achieves state-of-the-art results through pure unit-test optimization
- CURE's coder-tester co-evolution demonstrates sophisticated joint training
- Absolute Zero shows self-play potential but plateaus due to task generation limitations

**Process Method Innovation vs Performance Gap:**
- PRLCoder achieves +10.5% improvements through line-level supervision
- StepCoder decomposes compilation/execution into learnable steps
- Yet no process method matches outcome method peak performance

**GRPO+ as the Stability Solution:**
The survey identifies GRPO+ as the critical innovation enabling scale: "careful clipping and entropy modulation" prevent the reward hacking and policy collapse that plague standard PPO at 14B parameter scales.

## Survey Method Analysis

| Method | Type | Key Innovation | Performance Evidence | Fundamental Limitation |
|--------|------|----------------|---------------------|----------------------|
| **DeepCoder-14B** | Outcome | GRPO+ stability | 60.6% LiveCodeBench | Sparse rewards limit complex reasoning |
| **PRLCoder** | Process | Line-level supervision | +10.5% improvement | Intermediate reward hacking vulnerability |
| **CURE** | Outcome | Coder-tester co-evolution | Joint optimization | Low-quality test generation affects learning |
| **Absolute Zero** | Outcome | Self-play without human data | Bootstrap reasoning | Task complexity ceiling from self-generation |
| **StepCoder** | Process | Compilation step decomposition | Dense reward signals | Step-level reward hacking risks |

## The Process Reward Hacking Problem

Process methods face a systematic vulnerability: optimizing for intermediate signals that correlate with but don't guarantee final correctness. PRLCoder's +10.5% gains come with the risk that models learn to game line-level rewards without improving actual code quality.

## Resources

- **Primary Survey**: [Section 4.2.1, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **LiveCodeBench**: Benchmark demonstrating DeepCoder-14B's 60.6% performance
- **GRPO+ Technical Details**: Distributed RL training with stability improvements
- **Method Implementations**: 21 GitHub repositories listed in survey Table 5
- **Performance Metrics**: Pass@k evaluation protocols for code generation

## Next Steps

- **[4.2.2 RL for Iterative Code Refinement](4.2.2_RL_for_Iterative_Code_Refinement.md)**: Multi-turn debugging challenges
- **[4.2.3 RL for Automated Software Engineering](4.2.3_RL_for_Automated_SWE.md)**: Full-scale repository management
- **Integration Challenge**: Combine outcome performance with process efficiency without reward hacking

---

*Single-turn code generation achieves the highest success rates in agentic RL but exposes the fundamental tension between outcome performance and process innovation—a tension that GRPO+ stability improvements address at scale but don't resolve architecturally.*
