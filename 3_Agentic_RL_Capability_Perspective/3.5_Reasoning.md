# 3.5 Reasoning

RL enables adaptive reasoning that switches between fast System 1-like responses and deliberate System 2-like multi-step thinking. Test-time scaling and verification integration allow agents to dynamically adjust reasoning depth based on problem complexity, combining speed and accuracy through learned meta-cognitive policies.

## Key Takeaways
- **Dual-System Architecture**: RL learns when to use fast intuitive responses vs deliberate step-by-step reasoning
- **Test-Time Scaling**: Agents adaptively allocate compute during inference based on problem difficulty  
- **Verification Integration**: Reasoning processes incorporate automatic checking and error correction
- **Meta-Cognitive Control**: Learned policies decide reasoning strategy, depth, and termination conditions

## Prerequisites Check

```bash
# Verify reasoning libraries
python -c "import torch, transformers; print('Deep learning stack ready')"
python -c "import time, json; print('Timing and logging tools ready')"
python -c "import numpy as np; print('Numerical computation ready')"

# Conceptual check
echo "Do you understand System 1 vs System 2 thinking?"
echo "Are you familiar with chain-of-thought reasoning?"
echo "Have you completed Module 3.4 (Self-Improvement)?"
```

## Hands-On: Reasoning System Comparison

### Fast Reasoning (System 1)
```python
import torch
import torch.nn as nn
import time
from typing import Dict, List, Any, Tuple

class FastReasoningAgent:
    """System 1: Fast, intuitive, pattern-driven reasoning"""
    
    def __init__(self):
        self.response_templates = {
            'math': "The answer is {result}",
            'logic': "Based on the pattern, {conclusion}",
            'factual': "The key point is {fact}"
        }
        
    def quick_response(self, query: str) -> Dict:
        """Generate immediate response without deliberation"""
        start_time = time.time()
        
        # Pattern matching for quick categorization
        if any(op in query for op in ['+', '-', '*', '/', 'calculate']):
            response_type = 'math'
            result = self.quick_math_pattern(query)
        elif any(word in query.lower() for word in ['if', 'then', 'because', 'therefore']):
            response_type = 'logic' 
            result = self.quick_logic_pattern(query)
        else:
            response_type = 'factual'
            result = self.quick_factual_pattern(query)
        
        response_time = time.time() - start_time
        
        return {
            'response': result,
            'type': response_type,
            'reasoning_steps': 1,
            'response_time': response_time,
            'confidence': 0.7,
            'method': 'fast_pattern_matching'
        }
    
    def quick_math_pattern(self, query: str) -> str:
        """Quick mathematical pattern recognition"""
        # Simplified pattern matching
        if '15%' in query and '240' in query:
            return "36 (15% of 240)"
        elif '+' in query:
            return "sum of the numbers"
        else:
            return "mathematical calculation needed"
    
    def quick_logic_pattern(self, query: str) -> str:
        """Quick logical pattern recognition"""
        return "logical conclusion based on premises"
    
    def quick_factual_pattern(self, query: str) -> str:
        """Quick factual response"""
        return "factual information about the topic"

# Demo fast reasoning
fast_agent = FastReasoningAgent()

test_queries = [
    "What is 15% of 240?",
    "If it rains, then the ground gets wet. It's raining. What happens?",
    "Explain photosynthesis"
]

print("=== Fast Reasoning (System 1) ===")
for query in test_queries:
    result = fast_agent.quick_response(query)
    print(f"Query: {query}")
    print(f"Response: {result['response']}")
    print(f"Time: {result['response_time']:.4f}s, Steps: {result['reasoning_steps']}")
    print()
```

### Slow Reasoning (System 2)
```python
class SlowReasoningAgent:
    """System 2: Deliberate, multi-step, verified reasoning"""
    
    def __init__(self):
        self.max_thinking_steps = 10
        self.verification_threshold = 0.8
        
    def deliberate_response(self, query: str) -> Dict:
        """Generate response through multi-step deliberation"""
        start_time = time.time()
        
        # Step 1: Problem analysis
        analysis = self.analyze_problem(query)
        
        # Step 2: Strategy selection
        strategy = self.select_strategy(analysis)
        
        # Step 3: Multi-step reasoning
        reasoning_trace = self.multi_step_reasoning(query, strategy)
        
        # Step 4: Verification
        verification = self.verify_reasoning(reasoning_trace, query)
        
        # Step 5: Response synthesis
        final_response = self.synthesize_response(reasoning_trace, verification)
        
        response_time = time.time() - start_time
        
        return {
            'response': final_response,
            'reasoning_trace': reasoning_trace,
            'verification': verification,
            'reasoning_steps': len(reasoning_trace),
            'response_time': response_time,
            'confidence': verification['confidence'],
            'method': 'deliberate_multi_step'
        }
    
    def analyze_problem(self, query: str) -> Dict:
        """Analyze problem structure and requirements"""
        analysis = {
            'type': 'unknown',
            'complexity': 'medium',
            'requires_calculation': False,
            'requires_logic': False,
            'requires_knowledge': False
        }
        
        if any(op in query for op in ['+', '-', '*', '/', '%', 'calculate']):
            analysis.update({
                'type': 'mathematical',
                'requires_calculation': True,
                'complexity': 'low' if len(query.split()) < 10 else 'medium'
            })
        elif any(word in query.lower() for word in ['if', 'then', 'because', 'prove']):
            analysis.update({
                'type': 'logical',
                'requires_logic': True,
                'complexity': 'medium'
            })
        else:
            analysis.update({
                'type': 'knowledge_based',
                'requires_knowledge': True,
                'complexity': 'high'
            })
        
        return analysis
    
    def select_strategy(self, analysis: Dict) -> str:
        """Select reasoning strategy based on problem analysis"""
        if analysis['type'] == 'mathematical':
            return 'step_by_step_calculation'
        elif analysis['type'] == 'logical':
            return 'logical_chain_reasoning'
        else:
            return 'structured_explanation'
    
    def multi_step_reasoning(self, query: str, strategy: str) -> List[Dict]:
        """Execute multi-step reasoning process"""
        steps = []
        
        if strategy == 'step_by_step_calculation':
            steps = [
                {'step': 1, 'content': 'Identify the mathematical operation needed', 'type': 'analysis'},
                {'step': 2, 'content': 'Extract numerical values from the problem', 'type': 'extraction'},
                {'step': 3, 'content': 'Apply the mathematical formula', 'type': 'calculation'},
                {'step': 4, 'content': 'Verify the calculation is correct', 'type': 'verification'}
            ]
            
        elif strategy == 'logical_chain_reasoning':
            steps = [
                {'step': 1, 'content': 'Identify premises and conclusion', 'type': 'analysis'},
                {'step': 2, 'content': 'Check logical validity of reasoning', 'type': 'validation'},
                {'step': 3, 'content': 'Apply logical rules and inference', 'type': 'inference'}
            ]
            
        else:  # structured_explanation
            steps = [
                {'step': 1, 'content': 'Break down the concept into components', 'type': 'decomposition'},
                {'step': 2, 'content': 'Explain each component clearly', 'type': 'explanation'},
                {'step': 3, 'content': 'Show relationships between components', 'type': 'synthesis'}
            ]
        
        return steps
    
    def verify_reasoning(self, reasoning_trace: List[Dict], query: str) -> Dict:
        """Verify the reasoning chain for correctness"""
        verification_checks = []
        
        # Check completeness
        if len(reasoning_trace) >= 3:
            verification_checks.append({'check': 'completeness', 'passed': True})
        else:
            verification_checks.append({'check': 'completeness', 'passed': False})
        
        # Check logical flow
        has_analysis = any(step['type'] == 'analysis' for step in reasoning_trace)
        verification_checks.append({'check': 'logical_flow', 'passed': has_analysis})
        
        # Check specificity to query
        if any(word in query.lower() for word in ['calculate', 'math']) and \
           any(step['type'] == 'calculation' for step in reasoning_trace):
            verification_checks.append({'check': 'query_relevance', 'passed': True})
        else:
            verification_checks.append({'check': 'query_relevance', 'passed': True})  # Default pass
        
        passed_checks = sum(1 for check in verification_checks if check['passed'])
        confidence = passed_checks / len(verification_checks)
        
        return {
            'checks': verification_checks,
            'confidence': confidence,
            'verified': confidence >= self.verification_threshold
        }
    
    def synthesize_response(self, reasoning_trace: List[Dict], verification: Dict) -> str:
        """Synthesize final response from reasoning trace"""
        if verification['verified']:
            return f"After {len(reasoning_trace)} steps of analysis, the answer is derived through systematic reasoning."
        else:
            return f"Based on {len(reasoning_trace)} reasoning steps, though verification suggests further analysis needed."

# Demo slow reasoning
slow_agent = SlowReasoningAgent()

print("=== Slow Reasoning (System 2) ===")
for query in test_queries:
    result = slow_agent.deliberate_response(query)
    print(f"Query: {query}")
    print(f"Response: {result['response']}")
    print(f"Time: {result['response_time']:.4f}s, Steps: {result['reasoning_steps']}")
    print(f"Confidence: {result['confidence']:.3f}")
    print()
```

### Adaptive Meta-Reasoning
```python
class MetaReasoningPolicy(nn.Module):
    """RL policy for adaptive reasoning strategy selection"""
    
    def __init__(self, input_dim: int = 64):
        super().__init__()
        self.query_encoder = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16)
        )
        
        # Strategy selection head
        self.strategy_head = nn.Linear(16, 3)  # fast, slow, adaptive
        
        # Depth control head  
        self.depth_head = nn.Linear(16, 5)  # 1-5 reasoning steps
        
    def forward(self, query_features):
        encoded = self.query_encoder(query_features)
        strategy_logits = self.strategy_head(encoded)
        depth_logits = self.depth_head(encoded)
        return strategy_logits, depth_logits

class AdaptiveReasoningAgent:
    """Agent that learns when to use fast vs slow reasoning"""
    
    def __init__(self):
        self.meta_policy = MetaReasoningPolicy()
        self.fast_agent = FastReasoningAgent()
        self.slow_agent = SlowReasoningAgent()
        
        # Strategy mapping
        self.strategies = ['fast', 'slow', 'adaptive']
        
    def encode_query(self, query: str) -> torch.Tensor:
        """Encode query for meta-reasoning decision"""
        features = torch.zeros(64)
        
        # Basic features
        words = query.split()
        features[0] = min(len(words) / 20.0, 1.0)  # Length complexity
        
        # Content type indicators
        features[1] = 1.0 if any(op in query for op in ['+', '-', '*', '/']) else 0.0  # Math
        features[2] = 1.0 if any(w in query.lower() for w in ['explain', 'why', 'how']) else 0.0  # Explanation
        features[3] = 1.0 if any(w in query.lower() for w in ['if', 'then', 'because']) else 0.0  # Logic
        
        # Difficulty indicators
        features[4] = len(set(words)) / len(words) if words else 0  # Vocabulary diversity
        features[5] = 1.0 if '?' in query else 0.0  # Question format
        
        return features
    
    def adaptive_reasoning(self, query: str, time_budget: float = 5.0) -> Dict:
        """Adaptively select and execute reasoning strategy"""
        query_features = self.encode_query(query)
        
        with torch.no_grad():
            strategy_logits, depth_logits = self.meta_policy(query_features.unsqueeze(0))
            
            # Sample strategy
            strategy_probs = torch.softmax(strategy_logits, dim=-1)
            strategy_idx = torch.multinomial(strategy_probs, 1).item()
            selected_strategy = self.strategies[strategy_idx]
            
            # Sample depth
            depth_probs = torch.softmax(depth_logits, dim=-1) 
            reasoning_depth = torch.multinomial(depth_probs, 1).item() + 1
        
        # Execute based on strategy and time budget
        if selected_strategy == 'fast' or time_budget < 1.0:
            result = self.fast_agent.quick_response(query)
            result['selected_strategy'] = 'fast'
            
        elif selected_strategy == 'slow':
            result = self.slow_agent.deliberate_response(query)
            result['selected_strategy'] = 'slow'
            
        else:  # adaptive
            # Try fast first, then slow if confidence is low
            fast_result = self.fast_agent.quick_response(query)
            
            if fast_result['confidence'] > 0.8 or time_budget < 2.0:
                result = fast_result
                result['selected_strategy'] = 'adaptive_fast'
            else:
                result = self.slow_agent.deliberate_response(query)
                result['selected_strategy'] = 'adaptive_slow'
                result['fast_confidence'] = fast_result['confidence']
        
        result['reasoning_depth'] = reasoning_depth
        result['time_budget'] = time_budget
        
        return result

# Demo adaptive reasoning
adaptive_agent = AdaptiveReasoningAgent()

print("=== Adaptive Meta-Reasoning ===")
test_scenarios = [
    ("2 + 3 = ?", 1.0),  # Simple, low time budget
    ("Explain the theory of relativity", 10.0),  # Complex, high time budget  
    ("If all birds can fly, and penguins are birds, can penguins fly?", 5.0)  # Logic, medium budget
]

for query, budget in test_scenarios:
    result = adaptive_agent.adaptive_reasoning(query, budget)
    print(f"Query: {query}")
    print(f"Time budget: {budget}s")
    print(f"Strategy: {result['selected_strategy']}")
    print(f"Steps: {result['reasoning_steps']}, Time: {result['response_time']:.4f}s")
    print(f"Response: {result['response'][:50]}...")
    print()
```

## ASCII Diagram: Reasoning Architecture

```
Adaptive Reasoning System Architecture:

                    ┌─────────────────────┐
                    │   Query Input       │
                    └─────────────────────┘
                               │
                               ▼
                    ┌─────────────────────┐
                    │  Meta-Reasoning     │
                    │  Policy π(s|q,c)    │
                    └─────────────────────┘
                               │
                    ┌──────────┼──────────┐
                    │          │          │
                    ▼          ▼          ▼
            ┌─────────────┐ ┌─────────────┐ ┌─────────────┐
            │Fast System 1│ │Slow System 2│ │  Adaptive   │
            │ Pattern     │ │Multi-step   │ │ Hybrid      │
            │ Matching    │ │Deliberation │ │ Strategy    │
            └─────────────┘ └─────────────┘ └─────────────┘
                    │          │          │
                    └──────────┼──────────┘
                               ▼
                    ┌─────────────────────┐
                    │   Response +        │
                    │ Reasoning Trace     │
                    └─────────────────────┘

System 1 vs System 2 Trade-offs:
┌─────────────┬─────────────┬─────────────┐
│   Aspect    │  System 1   │  System 2   │
├─────────────┼─────────────┼─────────────┤
│   Speed     │    Fast     │    Slow     │
│ Accuracy    │   Medium    │    High     │
│ Compute     │    Low      │    High     │
│Verification │   Minimal   │ Extensive   │
│Use Cases    │Simple Q&A   │Complex Math │
└─────────────┴─────────────┴─────────────┘
```

## Training Meta-Reasoning Policies

```python
def train_meta_reasoning():
    """Train meta-reasoning policy to select optimal strategies"""
    
    agent = AdaptiveReasoningAgent()
    optimizer = torch.optim.Adam(agent.meta_policy.parameters(), lr=0.001)
    
    # Training scenarios with ground truth optimal strategies
    training_data = [
        ("2+2", "fast", 1, 0.9),  # query, optimal_strategy, steps, expected_confidence
        ("Explain quantum physics", "slow", 5, 0.8),
        ("What's the capital of France?", "fast", 1, 0.95),
        ("Prove that the sum of angles in a triangle is 180 degrees", "slow", 4, 0.85)
    ]
    
    print("=== Training Meta-Reasoning Policy ===")
    
    for epoch in range(10):
        epoch_loss = 0
        
        for query, optimal_strategy, optimal_steps, expected_confidence in training_data:
            # Get current policy predictions
            query_features = agent.encode_query(query)
            strategy_logits, depth_logits = agent.meta_policy(query_features.unsqueeze(0))
            
            # Compute targets
            strategy_target = torch.tensor([agent.strategies.index(optimal_strategy)])
            depth_target = torch.tensor([optimal_steps - 1])  # 0-indexed
            
            # Loss computation
            strategy_loss = nn.CrossEntropyLoss()(strategy_logits, strategy_target)
            depth_loss = nn.CrossEntropyLoss()(depth_logits, depth_target)
            
            total_loss = strategy_loss + depth_loss
            
            # Backpropagate
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            epoch_loss += total_loss.item()
        
        if epoch % 2 == 0:
            print(f"Epoch {epoch}: Loss = {epoch_loss:.4f}")
    
    print("Meta-reasoning policy training complete")
    
    return agent

# Demo training
trained_agent = train_meta_reasoning()
```

## Key Differences Summary

| Aspect | Fast Reasoning | Slow Reasoning | Adaptive Reasoning |
|--------|----------------|----------------|--------------------|
| **Speed** | <0.1s | 1-10s | Context-dependent |
| **Steps** | 1 | 3-10 | Variable |
| **Accuracy** | Medium | High | Optimized |
| **Use Cases** | Simple Q&A | Complex problems | General purpose |
| **Compute** | Minimal | High | Adaptive allocation |

## Practical Exercises

```python
# Exercise 1: Build domain-specific reasoning
def exercise_domain_reasoning():
    """Create specialized reasoning for your domain"""
    pass

# Exercise 2: Implement verification systems  
def exercise_verification():
    """Build automatic reasoning verification"""
    pass

# Exercise 3: Design test-time scaling
def exercise_test_time_scaling():
    """Implement compute scaling during inference"""
    pass
```

## Resources

- **Survey Reference**: [Section 3.5, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **System 1/2 Theory**: [Kahneman - Thinking, Fast and Slow](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)
- **Chain-of-Thought**: [Wei et al. - Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)
- **OpenAI o1**: [Learning to Reason with LLMs](https://openai.com/blog/learning-to-reason-with-llms)
- **DeepSeek-R1**: [Incentivizing Reasoning with RL](https://arxiv.org/abs/2501.12948)

## Next Steps

- **[3.6 Perception](3.6_Perception.md)**: Learn multimodal reasoning and vision-language integration
- **Integration Practice**: Combine reasoning with planning, memory, and tool use
- **Advanced Topics**: Study test-time scaling and verification optimization

---

*Reasoning becomes adaptive when agents learn not just how to think, but when to think fast vs slow. Meta-cognitive policies enable optimal compute allocation based on problem complexity and time constraints.*
