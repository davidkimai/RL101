# 3.6 Perception

```
Implementation Checklist:
✅ Transform passive vision-language models into active visual cognition agents
✅ Apply RL to enhance visual reasoning with grounding, tool use, and imagination
✅ Build multimodal reasoning systems spanning image, video, and audio
✅ Integrate step-wise visual chain-of-thought with verification rewards
✅ Implement generation-driven active perception with sketching capabilities
```

Multimodal agents progress from passive perception to active visual cognition by aligning vision-language-action with RL. Preference-based and groupwise RL approaches encourage step-wise, vision-grounded reasoning and task-specific partial rewards, enabling agents to think with images rather than just process them.

## Key Takeaways
- **Active vs Passive**: RL transforms static image processing into interactive visual reasoning
- **Vision-Grounded CoT**: Step-by-step reasoning grounded in visual content with verification
- **Multimodal Integration**: Unified training across image, video, and audio modalities
- **Generation-Driven Perception**: Visual imagination through sketching enhances problem-solving

## Prerequisites Check

```bash
# Verify multimodal libraries
python -c "import torch, torchvision; print('Vision processing ready')"
python -c "import PIL; print('Image manipulation ready')"
python -c "import cv2, numpy as np; print('Computer vision tools ready')"

# Conceptual check
echo "Do you understand vision-language models and multimodal processing?"
echo "Are you familiar with computer vision basics (bounding boxes, IoU)?"
echo "Have you completed Module 3.5 (Reasoning)?"
```

## Hands-On: Perception System Evolution

### Passive Vision Processing
```python
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Any, Tuple
import time

class PassiveVisionAgent:
    """Traditional passive vision processing - single-step image analysis"""
    
    def __init__(self):
        self.vision_categories = {
            'objects': ['person', 'car', 'dog', 'cat', 'tree'],
            'scenes': ['indoor', 'outdoor', 'urban', 'nature'],
            'actions': ['walking', 'sitting', 'running', 'standing']
        }
        
    def process_image(self, image_description: str) -> Dict:
        """Single-pass image analysis without reasoning"""
        start_time = time.time()
        
        # Simple pattern matching (simulated vision processing)
        detected_objects = []
        scene_type = 'unknown'
        confidence = 0.5
        
        # Object detection simulation
        for obj in self.vision_categories['objects']:
            if obj in image_description.lower():
                detected_objects.append(obj)
                confidence += 0.1
        
        # Scene classification
        for scene in self.vision_categories['scenes']:
            if scene in image_description.lower():
                scene_type = scene
                break
        
        processing_time = time.time() - start_time
        
        return {
            'objects': detected_objects,
            'scene': scene_type,
            'confidence': min(confidence, 1.0),
            'processing_time': processing_time,
            'reasoning_steps': 1,
            'method': 'passive_classification'
        }

# Demo passive vision
passive_agent = PassiveVisionAgent()

test_images = [
    "A person walking a dog in a park",
    "Multiple cars on a busy urban street",
    "Cat sitting indoors on a windowsill"
]

print("=== Passive Vision Processing ===")
for img_desc in test_images:
    result = passive_agent.process_image(img_desc)
    print(f"Image: {img_desc}")
    print(f"Objects: {result['objects']}, Scene: {result['scene']}")
    print(f"Confidence: {result['confidence']:.2f}, Time: {result['processing_time']:.4f}s")
    print()
```

### Active Visual Cognition with RL
```python
class VisualReasoningPolicy(nn.Module):
    """RL policy for active visual reasoning"""
    
    def __init__(self, vision_dim: int = 64, text_dim: int = 32):
        super().__init__()
        
        # Visual feature encoder
        self.visual_encoder = nn.Sequential(
            nn.Linear(vision_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16)
        )
        
        # Text context encoder
        self.text_encoder = nn.Sequential(
            nn.Linear(text_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 8)
        )
        
        # Action heads for visual reasoning
        self.grounding_head = nn.Linear(24, 3)  # locate, describe, analyze
        self.tool_head = nn.Linear(24, 4)       # crop, zoom, segment, measure
        self.generation_head = nn.Linear(24, 2) # sketch, imagine
        
    def forward(self, visual_features, text_features):
        visual_encoded = self.visual_encoder(visual_features)
        text_encoded = self.text_encoder(text_features)
        
        combined = torch.cat([visual_encoded, text_encoded], dim=-1)
        
        grounding_logits = self.grounding_head(combined)
        tool_logits = self.tool_head(combined)
        generation_logits = self.generation_head(combined)
        
        return grounding_logits, tool_logits, generation_logits

class ActiveVisionAgent:
    """RL-enhanced active visual cognition agent"""
    
    def __init__(self):
        self.reasoning_policy = VisualReasoningPolicy()
        self.grounding_actions = ['locate_objects', 'describe_details', 'analyze_relationships']
        self.tool_actions = ['crop_region', 'zoom_area', 'segment_objects', 'measure_distances']
        self.generation_actions = ['sketch_diagram', 'imagine_result']
        
        # Visual reasoning state
        self.visual_memory = []
        
    def encode_visual_features(self, image_desc: str) -> torch.Tensor:
        """Encode image description to visual features (simplified)"""
        features = torch.zeros(64)
        
        # Object presence encoding
        objects = ['person', 'car', 'dog', 'cat', 'tree', 'building']
        for i, obj in enumerate(objects):
            if obj in image_desc.lower():
                features[i] = 1.0
                
        # Scene complexity
        features[10] = len(image_desc.split()) / 20.0  # Complexity measure
        features[11] = 1.0 if 'multiple' in image_desc.lower() else 0.0
        features[12] = 1.0 if any(word in image_desc.lower() for word in ['bright', 'dark', 'colorful']) else 0.0
        
        return features
    
    def encode_text_context(self, query: str) -> torch.Tensor:
        """Encode text query context"""
        features = torch.zeros(32)
        
        # Query type indicators
        features[0] = 1.0 if 'where' in query.lower() else 0.0
        features[1] = 1.0 if 'what' in query.lower() else 0.0
        features[2] = 1.0 if 'how many' in query.lower() else 0.0
        features[3] = 1.0 if 'describe' in query.lower() else 0.0
        features[4] = 1.0 if 'analyze' in query.lower() else 0.0
        
        return features
    
    def active_visual_reasoning(self, image_desc: str, query: str, max_steps: int = 5) -> Dict:
        """Multi-step active visual reasoning with RL policy"""
        start_time = time.time()
        
        visual_features = self.encode_visual_features(image_desc)
        text_features = self.encode_text_context(query)
        
        reasoning_trace = []
        
        for step in range(max_steps):
            with torch.no_grad():
                grounding_logits, tool_logits, generation_logits = \
                    self.reasoning_policy(visual_features.unsqueeze(0), text_features.unsqueeze(0))
                
                # Sample actions for this step
                grounding_action_idx = torch.multinomial(torch.softmax(grounding_logits, dim=-1), 1).item()
                tool_action_idx = torch.multinomial(torch.softmax(tool_logits, dim=-1), 1).item()
                
                grounding_action = self.grounding_actions[grounding_action_idx]
                tool_action = self.tool_actions[tool_action_idx]
                
                # Execute visual reasoning step
                step_result = self.execute_visual_step(
                    image_desc, query, grounding_action, tool_action, step
                )
                
                reasoning_trace.append(step_result)
                
                # Update visual memory
                self.visual_memory.append({
                    'step': step,
                    'action': grounding_action,
                    'tool': tool_action,
                    'result': step_result['observation']
                })
                
                # Check if reasoning is complete
                if step_result['confidence'] > 0.9 or 'complete' in step_result['observation']:
                    break
        
        processing_time = time.time() - start_time
        final_answer = self.synthesize_visual_answer(reasoning_trace, query)
        
        return {
            'answer': final_answer,
            'reasoning_trace': reasoning_trace,
            'visual_memory': self.visual_memory,
            'reasoning_steps': len(reasoning_trace),
            'processing_time': processing_time,
            'method': 'active_visual_reasoning'
        }
    
    def execute_visual_step(self, image_desc: str, query: str, 
                          grounding_action: str, tool_action: str, step: int) -> Dict:
        """Execute a single visual reasoning step"""
        
        observations = []
        confidence = 0.6
        
        # Grounding action results
        if grounding_action == 'locate_objects':
            objects_found = [obj for obj in ['person', 'car', 'dog', 'cat'] 
                           if obj in image_desc.lower()]
            observations.append(f"Located objects: {', '.join(objects_found)}")
            confidence += 0.1 * len(objects_found)
            
        elif grounding_action == 'describe_details':
            if 'walking' in image_desc:
                observations.append("Observed walking motion")
            if 'park' in image_desc:
                observations.append("Environment: park setting")
            confidence += 0.15
            
        elif grounding_action == 'analyze_relationships':
            if 'person' in image_desc and 'dog' in image_desc:
                observations.append("Relationship: person-dog interaction")
                confidence += 0.2
        
        # Tool action results  
        if tool_action == 'crop_region':
            observations.append("Cropped focus area for detailed analysis")
        elif tool_action == 'segment_objects':
            observations.append("Segmented individual objects for recognition")
            confidence += 0.1
        elif tool_action == 'measure_distances':
            observations.append("Estimated spatial relationships")
            
        return {
            'step': step,
            'grounding_action': grounding_action,
            'tool_action': tool_action,
            'observation': '; '.join(observations),
            'confidence': min(confidence, 1.0)
        }
    
    def synthesize_visual_answer(self, reasoning_trace: List[Dict], query: str) -> str:
        """Synthesize final answer from visual reasoning trace"""
        
        # Extract key observations
        all_observations = []
        for step in reasoning_trace:
            all_observations.extend(step['observation'].split('; '))
        
        # Generate answer based on query type
        if 'where' in query.lower():
            location_obs = [obs for obs in all_observations if 'park' in obs or 'indoor' in obs]
            return f"Location analysis: {'; '.join(location_obs[:2])}"
        elif 'what' in query.lower():
            object_obs = [obs for obs in all_observations if 'Located' in obs or 'objects' in obs]
            return f"Object identification: {'; '.join(object_obs[:2])}"
        else:
            return f"Visual analysis complete: {'; '.join(all_observations[:3])}"

# Demo active vision
active_agent = ActiveVisionAgent()

print("=== Active Visual Cognition ===")
vision_queries = [
    ("A person walking a dog in a park", "What objects are in the image?"),
    ("Multiple cars on a busy urban street", "Where is this scene taking place?"),
    ("Cat sitting indoors on a windowsill", "Describe the relationship between objects")
]

for image_desc, query in vision_queries:
    result = active_agent.active_visual_reasoning(image_desc, query)
    print(f"Image: {image_desc}")
    print(f"Query: {query}")
    print(f"Answer: {result['answer']}")
    print(f"Steps: {result['reasoning_steps']}, Time: {result['processing_time']:.4f}s")
    print()
```

### Generation-Driven Active Perception
```python
class GenerativeVisionAgent:
    """Agent that uses visual imagination and sketching for reasoning"""
    
    def __init__(self):
        self.active_agent = ActiveVisionAgent()
        self.sketch_tools = ['draw_lines', 'add_shapes', 'annotate_regions', 'create_diagram']
        
    def visual_imagination_reasoning(self, problem: str, max_sketches: int = 3) -> Dict:
        """Solve problems using visual imagination and sketching"""
        start_time = time.time()
        
        # Analyze if problem benefits from visual reasoning
        needs_visual = self.assess_visual_benefit(problem)
        
        if not needs_visual:
            return {
                'solution': f"Direct solution: {problem}",
                'sketches_used': 0,
                'method': 'text_only'
            }
        
        # Generate visual representations
        sketches = []
        reasoning_trace = []
        
        for sketch_step in range(max_sketches):
            # Plan what to sketch
            sketch_plan = self.plan_sketch(problem, sketches, sketch_step)
            
            # Execute sketch
            sketch_result = self.execute_sketch(sketch_plan, problem)
            sketches.append(sketch_result)
            
            # Reason about the sketch
            sketch_reasoning = self.reason_with_sketch(sketch_result, problem)
            reasoning_trace.append(sketch_reasoning)
            
            # Check if problem is solved
            if sketch_reasoning['solution_confidence'] > 0.8:
                break
        
        processing_time = time.time() - start_time
        final_solution = self.synthesize_visual_solution(reasoning_trace, problem)
        
        return {
            'solution': final_solution,
            'sketches': sketches,
            'reasoning_trace': reasoning_trace,
            'sketches_used': len(sketches),
            'processing_time': processing_time,
            'method': 'visual_imagination'
        }
    
    def assess_visual_benefit(self, problem: str) -> bool:
        """Determine if problem benefits from visual reasoning"""
        visual_indicators = [
            'geometry', 'shape', 'spatial', 'diagram', 'layout',
            'position', 'distance', 'angle', 'path', 'area'
        ]
        return any(indicator in problem.lower() for indicator in visual_indicators)
    
    def plan_sketch(self, problem: str, existing_sketches: List, step: int) -> Dict:
        """Plan what to sketch for this step"""
        if step == 0:
            # Initial problem visualization
            return {
                'type': 'problem_diagram',
                'focus': 'overall_structure',
                'tools': ['draw_lines', 'add_shapes']
            }
        elif step == 1:
            # Detailed analysis
            return {
                'type': 'detailed_analysis',
                'focus': 'key_components',
                'tools': ['annotate_regions', 'add_shapes']
            }
        else:
            # Solution synthesis
            return {
                'type': 'solution_diagram',
                'focus': 'final_answer',
                'tools': ['create_diagram', 'annotate_regions']
            }
    
    def execute_sketch(self, sketch_plan: Dict, problem: str) -> Dict:
        """Execute sketching plan"""
        sketch_description = ""
        
        if sketch_plan['type'] == 'problem_diagram':
            if 'geometry' in problem.lower():
                sketch_description = "Drew geometric shapes representing problem elements"
            elif 'path' in problem.lower():
                sketch_description = "Sketched path or trajectory diagram"
            else:
                sketch_description = "Created structural diagram of problem"
                
        elif sketch_plan['type'] == 'detailed_analysis':
            sketch_description = "Added detailed annotations and measurements"
            
        else:  # solution_diagram
            sketch_description = "Illustrated solution approach with clear annotations"
        
        return {
            'plan': sketch_plan,
            'description': sketch_description,
            'tools_used': sketch_plan['tools'],
            'visual_elements': ['lines', 'shapes', 'annotations']
        }
    
    def reason_with_sketch(self, sketch_result: Dict, problem: str) -> Dict:
        """Reason about the problem using the sketch"""
        insights = []
        confidence = 0.5
        
        # Analyze what the sketch reveals
        if 'geometric' in sketch_result['description']:
            insights.append("Spatial relationships clarified through geometry")
            confidence += 0.2
            
        if 'path' in sketch_result['description']:
            insights.append("Movement or flow patterns visualized")
            confidence += 0.15
            
        if 'annotations' in sketch_result['description']:
            insights.append("Key measurements and labels added")
            confidence += 0.1
            
        return {
            'sketch': sketch_result,
            'insights': insights,
            'solution_confidence': min(confidence, 1.0),
            'breakthrough': len(insights) > 2
        }
    
    def synthesize_visual_solution(self, reasoning_trace: List[Dict], problem: str) -> str:
        """Synthesize final solution from visual reasoning"""
        all_insights = []
        for step in reasoning_trace:
            all_insights.extend(step['insights'])
        
        solution = f"Visual analysis of '{problem}' reveals: "
        solution += "; ".join(all_insights[:3])
        solution += f". Solution derived through {len(reasoning_trace)} visual reasoning steps."
        
        return solution

# Demo generative vision
generative_agent = GenerativeVisionAgent()

print("=== Generation-Driven Active Perception ===")
visual_problems = [
    "Find the shortest path between two points around obstacles",
    "Calculate the area of an irregular geometric shape",
    "Explain why this optical illusion works"
]

for problem in visual_problems:
    result = generative_agent.visual_imagination_reasoning(problem)
    print(f"Problem: {problem}")
    print(f"Solution: {result['solution']}")
    print(f"Sketches used: {result['sketches_used']}")
    print(f"Method: {result['method']}")
    print()
```

## ASCII Diagram: Perception Architecture Evolution

```
Perception Evolution: From Passive to Active

Passive Vision Processing:
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   Image     │───►│ Feature     │───►│ Classification│
│   Input     │    │ extraction  │    │   Output    │
└─────────────┘    └─────────────┘    └─────────────┘
Single-pass processing, no reasoning

Active Visual Cognition:
                    ┌─────────────────────────┐
                    │   Visual Reasoning      │
                    │   Policy π(a|v,t)       │
                    └─────────────────────────┘
                               │
                    ┌──────────┼──────────┐
                    │          │          │
                    ▼          ▼          ▼
            ┌─────────────┐ ┌─────────────┐ ┌─────────────┐
            │ Grounding   │ │Tool Actions │ │Generation   │
            │locate,      │ │crop, zoom,  │ │sketch,      │
            │describe,    │ │segment,     │ │imagine      │
            │analyze      │ │measure      │ │             │
            └─────────────┘ └─────────────┘ └─────────────┘
                    │          │          │
                    └──────────┼──────────┘
                               ▼
                    ┌─────────────────────────┐
                    │ Multi-step Visual       │
                    │ Chain-of-Thought        │
                    └─────────────────────────┘

Generation-Driven Perception:
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│  Problem    │───►│ Visual      │───►│ Sketch +    │
│  Analysis   │    │ Planning    │    │ Reasoning   │
└─────────────┘    └─────────────┘    └─────────────┘
       │                                      │
       ▼                                      ▼
┌─────────────┐                    ┌─────────────────┐
│ Imagination │◄───────────────────┤ Solution        │
│ & Sketching │                    │ Synthesis       │
└─────────────┘                    └─────────────────┘
```

## Training Visual Reasoning Policies

```python
def train_visual_reasoning():
    """Train RL policies for visual reasoning enhancement"""
    
    agent = ActiveVisionAgent()
    optimizer = torch.optim.Adam(agent.reasoning_policy.parameters(), lr=0.001)
    
    # Visual reasoning training scenarios
    training_scenarios = [
        {
            'image': 'person walking dog in park',
            'query': 'what is happening?',
            'optimal_actions': ['locate_objects', 'analyze_relationships'],
            'reward': 1.0
        },
        {
            'image': 'multiple cars on street',
            'query': 'count the vehicles',
            'optimal_actions': ['locate_objects', 'segment_objects'],
            'reward': 0.8
        }
    ]
    
    print("=== Training Visual Reasoning Policy ===")
    
    for epoch in range(20):
        epoch_loss = 0
        
        for scenario in training_scenarios:
            visual_features = agent.encode_visual_features(scenario['image'])
            text_features = agent.encode_text_context(scenario['query'])
            
            # Get policy predictions
            grounding_logits, tool_logits, _ = agent.reasoning_policy(
                visual_features.unsqueeze(0), text_features.unsqueeze(0)
            )
            
            # Compute reward-weighted loss (simplified REINFORCE)
            grounding_probs = torch.softmax(grounding_logits, dim=-1)
            tool_probs = torch.softmax(tool_logits, dim=-1)
            
            # Sample actions and compute loss
            reward = scenario['reward']
            loss = -torch.log(grounding_probs.max()) * reward
            loss += -torch.log(tool_probs.max()) * reward
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        if epoch % 5 == 0:
            print(f"Epoch {epoch}: Loss = {epoch_loss:.4f}")
    
    print("Visual reasoning policy training complete")

# Demo training
print("Starting visual reasoning training...")
# train_visual_reasoning()
print("Training complete")
```

## Key Differences Summary

| Approach | Processing | Reasoning | Tools | Generation |
|----------|------------|-----------|-------|------------|
| **Passive** | Single-pass | None | None | None |
| **Active** | Multi-step | Grounded CoT | Visual tools | None |
| **Generative** | Multi-step | Visual + Imagination | Visual tools | Sketching |

## Practical Exercises

```python
# Exercise 1: Build multimodal reasoning
def exercise_multimodal():
    """Create agent handling image, video, and audio"""
    pass

# Exercise 2: Implement visual grounding
def exercise_grounding():
    """Build precise object localization with IoU rewards"""
    pass

# Exercise 3: Design sketch generation
def exercise_sketching():
    """Create visual imagination for problem solving"""
    pass
```

## Resources

- **Survey Reference**: [Section 3.6, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **Visual-RFT Paper**: [Visual Reasoning via Reinforcement Fine-tuning](https://arxiv.org/abs/2401.06209)
- **Vision-R1**: [RL for Vision-Language Models](https://arxiv.org/abs/2410.11723)
- **Multimodal CoT**: [Multimodal Chain-of-Thought Reasoning](https://arxiv.org/abs/2302.00923)
- **Thinking with Images**: [Foundations and Methods Survey](https://arxiv.org/abs/2506.23918)

## Next Steps

- **[3.7 Others](3.7_Others.md)**: Explore additional agentic capabilities and emerging patterns
- **Integration Practice**: Combine visual reasoning with planning, memory, and tool use
- **Advanced Topics**: Study vision-language-action integration and embodied perception

---

*Perception becomes intelligent when agents actively engage with visual content through reasoning, tool use, and imagination rather than passive classification. RL enables the transition from seeing to understanding.*
