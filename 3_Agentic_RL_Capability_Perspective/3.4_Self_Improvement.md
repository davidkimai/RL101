# 3.4 Self-Improvement

Self-improvement evolves from verbal self-correction (prompted reflection) to RL-internalized feedback loops and fully autonomous self-training systems. RL transforms static critique patterns into learned policies that persistently improve from environmental feedback, execution outcomes, and self-generated curricula - enabling unbounded capability enhancement without human supervision.

## Key Takeaways
- **Three-Stage Evolution**: Verbal correction → RL internalization → Autonomous self-training  
- **Persistent Learning**: RL internalizes reflection into model parameters vs ephemeral prompt-based fixes
- **Autonomous Loops**: Advanced systems generate own tasks, execute solutions, and learn from verifiable outcomes
- **Curriculum Generation**: Agents strategically create learning experiences to maximize improvement gains

## Prerequisites Check

```bash
# Verify self-improvement libraries
python -c "import torch, transformers; print('RL training stack ready')"
python -c "import gymnasium, numpy as np; print('Environment simulation ready')"
python -c "import ast, subprocess; print('Code execution tools ready')"

# Conceptual check
echo "Do you understand iterative refinement and feedback loops?"
echo "Are you familiar with self-play and curriculum learning?"
echo "Have you completed Module 3.3 (Memory)?"
```

## Hands-On: Self-Improvement Approaches

### Stage 1: Verbal Self-Correction
```python
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

@dataclass
class ReflectionStep:
    content: str
    critique: str
    improvement: str
    confidence: float

class VerbalSelfCorrection:
    """Prompt-based self-correction without gradient updates"""
    
    def __init__(self):
        self.reflection_templates = {
            'generation': "Generate a solution to: {task}",
            'critique': "Analyze this solution for errors and weaknesses: {solution}",
            'refinement': "Improve the solution based on this critique: {solution}\nCritique: {critique}"
        }
        
    def self_refine_loop(self, task: str, max_iterations: int = 3) -> List[ReflectionStep]:
        """Self-Refine protocol: Generate → Critique → Refine"""
        steps = []
        current_solution = self.generate_initial_solution(task)
        
        for iteration in range(max_iterations):
            # Critique current solution
            critique = self.generate_critique(current_solution, task)
            
            # Check if refinement needed
            if self.is_good_enough(critique):
                break
                
            # Generate improved solution
            improved_solution = self.refine_solution(current_solution, critique)
            confidence = self.assess_confidence(improved_solution, critique)
            
            step = ReflectionStep(
                content=current_solution,
                critique=critique,
                improvement=improved_solution,
                confidence=confidence
            )
            steps.append(step)
            
            current_solution = improved_solution
            
        return steps
    
    def generate_initial_solution(self, task: str) -> str:
        """Generate initial solution (simplified)"""
        if "calculate" in task.lower() or "math" in task.lower():
            return f"To solve '{task}', I need to identify the mathematical operation and compute the result."
        elif "code" in task.lower() or "program" in task.lower():
            return f"For '{task}', I should write clean, efficient code with proper error handling."
        else:
            return f"Addressing '{task}' requires careful analysis and structured approach."
    
    def generate_critique(self, solution: str, task: str) -> str:
        """Generate critique of current solution"""
        critiques = []
        
        # Check completeness
        if len(solution.split()) < 10:
            critiques.append("Solution lacks detail and specificity")
            
        # Check task alignment
        task_words = set(task.lower().split())
        solution_words = set(solution.lower().split())
        if len(task_words.intersection(solution_words)) < 2:
            critiques.append("Solution doesn't directly address the task requirements")
            
        # Check structure
        if not any(word in solution.lower() for word in ['step', 'first', 'then', 'because']):
            critiques.append("Solution lacks clear reasoning structure")
            
        return "; ".join(critiques) if critiques else "Solution appears adequate"
    
    def refine_solution(self, solution: str, critique: str) -> str:
        """Generate refined solution based on critique"""
        improvements = []
        
        if "lacks detail" in critique:
            improvements.append("adding more specific details and examples")
        if "doesn't directly address" in critique:
            improvements.append("focusing more directly on the core requirements")  
        if "lacks clear reasoning" in critique:
            improvements.append("providing step-by-step reasoning structure")
            
        improvement_text = ", ".join(improvements)
        return f"{solution} [Improved by: {improvement_text}]"
    
    def is_good_enough(self, critique: str) -> bool:
        """Determine if solution needs further refinement"""
        return "appears adequate" in critique or len(critique) < 20
    
    def assess_confidence(self, solution: str, critique: str) -> float:
        """Assess confidence in refined solution"""
        base_confidence = 0.5
        
        if "appears adequate" in critique:
            base_confidence += 0.3
        if len(solution.split()) > 20:  # Detailed solution
            base_confidence += 0.2
        if "[Improved by:" in solution:  # Shows refinement
            base_confidence += 0.1
            
        return min(base_confidence, 1.0)

# Demo verbal self-correction
verbal_corrector = VerbalSelfCorrection()

tasks = [
    "Calculate 15% of 240",
    "Write a Python function to sort a list",
    "Explain photosynthesis to a student"
]

print("=== Verbal Self-Correction ===")
for task in tasks:
    steps = verbal_corrector.self_refine_loop(task)
    print(f"\nTask: {task}")
    print(f"Refinement steps: {len(steps)}")
    
    for i, step in enumerate(steps, 1):
        print(f"  Step {i}: {step.improvement[:50]}... (confidence: {step.confidence:.2f})")
```

### Stage 2: RL-Internalized Self-Correction
```python
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

class CriticPolicy(nn.Module):
    """Learned critic for evaluating and improving solutions"""
    
    def __init__(self, input_dim: int = 512, hidden_dim: int = 128):
        super().__init__()
        self.solution_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim//2)
        )
        
        # Quality assessment head
        self.quality_head = nn.Linear(hidden_dim//2, 1)  # Quality score 0-1
        
        # Improvement suggestion head
        self.improvement_head = nn.Sequential(
            nn.Linear(hidden_dim//2, hidden_dim//4),
            nn.ReLU(),
            nn.Linear(hidden_dim//4, 10)  # 10 improvement types
        )
        
    def forward(self, solution_features):
        encoded = self.solution_encoder(solution_features)
        quality = torch.sigmoid(self.quality_head(encoded))
        improvements = torch.softmax(self.improvement_head(encoded), dim=-1)
        return quality, improvements

class RLSelfCorrectionAgent:
    """RL agent that learns to critique and improve its own outputs"""
    
    def __init__(self, solution_dim: int = 512):
        self.critic = CriticPolicy(solution_dim)
        self.optimizer = optim.Adam(self.critic.parameters(), lr=0.001)
        
        self.improvement_types = [
            'add_detail', 'fix_logic', 'improve_clarity', 'add_examples',
            'correct_errors', 'enhance_structure', 'verify_facts', 
            'simplify_language', 'add_steps', 'check_completeness'
        ]
        
        # Experience buffer for training
        self.experience_buffer = deque(maxlen=1000)
        
        # Performance tracking
        self.improvement_success_rate = 0.5
        
    def encode_solution(self, solution: str, task: str) -> torch.Tensor:
        """Encode solution into feature vector (simplified)"""
        # In practice: use sentence embeddings
        features = torch.zeros(512)
        
        # Basic features
        words = solution.lower().split()
        features[0] = min(len(words) / 50.0, 1.0)  # Length
        features[1] = len(set(words)) / len(words) if words else 0  # Diversity
        
        # Task relevance
        task_words = set(task.lower().split())
        solution_words = set(words)
        features[2] = len(task_words.intersection(solution_words)) / len(task_words) if task_words else 0
        
        # Structure indicators
        features[3] = 1.0 if any(w in words for w in ['first', 'then', 'next', 'finally']) else 0.0
        features[4] = 1.0 if any(w in words for w in ['because', 'since', 'therefore']) else 0.0
        
        # Hash-based encoding for content
        for i, word in enumerate(words[:100]):  # Max 100 words
            word_hash = abs(hash(word)) % 400  # Use remaining 400 features
            features[100 + word_hash] = 1.0
            
        return features
    
    def self_correct_with_rl(self, task: str, initial_solution: str, 
                           max_iterations: int = 3) -> Dict:
        """RL-guided self-correction process"""
        corrections = []
        current_solution = initial_solution
        
        for iteration in range(max_iterations):
            # Encode current solution
            solution_features = self.encode_solution(current_solution, task)
            
            # Get quality assessment and improvement suggestions
            with torch.no_grad():
                quality, improvement_probs = self.critic(solution_features.unsqueeze(0))
                
            quality_score = quality.item()
            
            # If quality is high enough, stop
            if quality_score > 0.8:
                break
                
            # Select improvement action
            improvement_idx = torch.multinomial(improvement_probs, 1).item()
            improvement_type = self.improvement_types[improvement_idx]
            
            # Apply improvement (simplified)
            improved_solution = self.apply_improvement(current_solution, improvement_type, task)
            
            # Store correction step
            correction = {
                'iteration': iteration,
                'original': current_solution,
                'improvement_type': improvement_type,
                'improved': improved_solution,
                'quality_before': quality_score,
                'confidence': float(improvement_probs[0, improvement_idx])
            }
            corrections.append(correction)
            
            current_solution = improved_solution
        
        return {
            'final_solution': current_solution,
            'corrections': corrections,
            'iterations': len(corrections),
            'final_quality': self.assess_final_quality(current_solution, task)
        }
    
    def apply_improvement(self, solution: str, improvement_type: str, task: str) -> str:
        """Apply specific improvement to solution"""
        improvements = {
            'add_detail': f"{solution} [Added: More specific details and context]",
            'fix_logic': f"{solution} [Fixed: Logical flow and reasoning]",
            'improve_clarity': f"{solution} [Clarified: Language and explanation]",
            'add_examples': f"{solution} [Added: Concrete examples]",
            'correct_errors': f"{solution} [Corrected: Factual or logical errors]",
            'enhance_structure': f"{solution} [Enhanced: Organization and structure]",
            'verify_facts': f"{solution} [Verified: Accuracy and correctness]",
            'simplify_language': f"{solution} [Simplified: Language and terminology]",
            'add_steps': f"{solution} [Added: Step-by-step breakdown]",
            'check_completeness': f"{solution} [Completed: Missing elements]"
        }
        
        return improvements.get(improvement_type, solution)
    
    def assess_final_quality(self, solution: str, task: str) -> float:
        """Assess final solution quality"""
        solution_features = self.encode_solution(solution, task)
        with torch.no_grad():
            quality, _ = self.critic(solution_features.unsqueeze(0))
        return quality.item()
    
    def train_critic(self, experiences: List[Dict]) -> float:
        """Train critic using RL on improvement experiences"""
        if len(experiences) < 5:
            return 0.0
            
        total_loss = 0.0
        
        for exp in experiences:
            # Encode solutions
            before_features = self.encode_solution(exp['solution_before'], exp['task'])
            after_features = self.encode_solution(exp['solution_after'], exp['task'])
            
            # Get predictions
            quality_before, _ = self.critic(before_features.unsqueeze(0))
            quality_after, improvements = self.critic(before_features.unsqueeze(0))
            
            # Quality should improve after correction
            quality_target_before = torch.tensor([[exp['quality_before']]], dtype=torch.float32)
            quality_target_after = torch.tensor([[exp['quality_after']]], dtype=torch.float32)
            
            # Loss on quality prediction
            quality_loss = nn.MSELoss()(quality_before, quality_target_before)
            
            # Loss on improvement prediction (if improvement was effective)
            if exp['improvement_effective']:
                improvement_target = torch.zeros(1, len(self.improvement_types))
                improvement_target[0, exp['improvement_type_idx']] = 1.0
                improvement_loss = nn.CrossEntropyLoss()(improvements, improvement_target)
            else:
                improvement_loss = 0
            
            total_loss_step = quality_loss + improvement_loss
            
            # Backpropagate
            self.optimizer.zero_grad()
            total_loss_step.backward()
            self.optimizer.step()
            
            total_loss += total_loss_step.item()
        
        return total_loss / len(experiences)

# Demo RL self-correction
rl_corrector = RLSelfCorrectionAgent()

print("\n=== RL-Internalized Self-Correction ===")
for task in tasks:
    initial_solution = f"Basic approach to '{task}' would be to analyze and solve systematically."
    result = rl_corrector.self_correct_with_rl(task, initial_solution)
    
    print(f"\nTask: {task}")
    print(f"Iterations: {result['iterations']}")
    print(f"Final quality: {result['final_quality']:.3f}")
    print(f"Final solution: {result['final_solution'][:80]}...")
```

### Stage 3: Autonomous Self-Training
```python
class AutonomousSelfTrainer:
    """Fully autonomous self-improvement through self-generated curricula"""
    
    def __init__(self):
        self.rl_corrector = RLSelfCorrectionAgent()
        self.task_generator = self.initialize_task_generator()
        
        # Self-training components
        self.skill_inventory = []
        self.performance_history = []
        self.curriculum_strategy = "progressive_difficulty"
        
        # Execution environment for verification
        self.execution_env = {}
        
    def initialize_task_generator(self) -> Dict:
        """Initialize task generation capabilities"""
        return {
            'math_templates': [
                "Calculate {op} of {num1} and {num2}",
                "Solve for x: {equation}",
                "Find {stat} of list: {numbers}"
            ],
            'coding_templates': [
                "Write function to {action} a {data_structure}",
                "Implement {algorithm} algorithm",
                "Debug this code: {code_snippet}"
            ],
            'reasoning_templates': [
                "Explain why {statement}",
                "Compare {concept1} and {concept2}",
                "Predict outcome of {scenario}"
            ]
        }
    
    def generate_task(self, difficulty_level: float = 0.5, domain: str = 'general') -> Dict:
        """Generate new task for self-training"""
        import random
        
        # Select domain and template
        if domain == 'math':
            template = random.choice(self.task_generator['math_templates'])
            task_params = self.generate_math_params(difficulty_level)
        elif domain == 'coding':
            template = random.choice(self.task_generator['coding_templates'])
            task_params = self.generate_coding_params(difficulty_level)
        else:
            template = random.choice(self.task_generator['reasoning_templates'])
            task_params = self.generate_reasoning_params(difficulty_level)
        
        task = template.format(**task_params)
        
        return {
            'task': task,
            'domain': domain,
            'difficulty': difficulty_level,
            'params': task_params,
            'expected_approach': self.get_expected_approach(domain, task_params)
        }
    
    def generate_math_params(self, difficulty: float) -> Dict:
        """Generate math task parameters based on difficulty"""
        import random
        
        base_range = int(10 + difficulty * 100)
        return {
            'op': random.choice(['sum', 'product', 'difference', 'quotient']),
            'num1': random.randint(1, base_range),
            'num2': random.randint(1, base_range),
            'equation': f"{random.randint(1, 10)}x + {random.randint(1, 20)} = {random.randint(10, 50)}",
            'stat': random.choice(['mean', 'median', 'max', 'min']),
            'numbers': [random.randint(1, 20) for _ in range(random.randint(3, 8))]
        }
    
    def generate_coding_params(self, difficulty: float) -> Dict:
        """Generate coding task parameters"""
        import random
        
        complexity_actions = ['sort', 'search', 'reverse'] if difficulty < 0.5 else ['optimize', 'parallelize', 'refactor']
        return {
            'action': random.choice(complexity_actions),
            'data_structure': random.choice(['list', 'dictionary', 'tree', 'graph']),
            'algorithm': random.choice(['binary search', 'merge sort', 'depth-first search']),
            'code_snippet': "def example(): pass  # Placeholder buggy code"
        }
    
    def generate_reasoning_params(self, difficulty: float) -> Dict:
        """Generate reasoning task parameters"""
        import random
        
        concepts = ['machine learning', 'climate change', 'economic policy'] if difficulty > 0.7 else ['weather', 'cooking', 'sports']
        return {
            'statement': f"{random.choice(concepts)} affects daily life",
            'concept1': random.choice(concepts),
            'concept2': random.choice([c for c in concepts if c != random.choice(concepts)]),
            'scenario': f"increased adoption of {random.choice(concepts)}"
        }
    
    def get_expected_approach(self, domain: str, params: Dict) -> str:
        """Get expected solution approach for verification"""
        if domain == 'math':
            return f"Apply mathematical operations to solve {params.get('op', 'calculation')}"
        elif domain == 'coding':
            return f"Implement {params.get('action', 'function')} using appropriate algorithms"
        else:
            return "Provide structured analysis with supporting reasoning"
    
    def self_training_loop(self, num_iterations: int = 10) -> Dict:
        """Main self-training loop"""
        training_log = []
        
        for iteration in range(num_iterations):
            # Generate task based on current curriculum strategy
            difficulty = self.adaptive_difficulty(iteration)
            domain = self.select_domain(iteration)
            
            task_spec = self.generate_task(difficulty, domain)
            
            # Attempt solution using current capabilities
            solution_result = self.rl_corrector.self_correct_with_rl(
                task_spec['task'], 
                f"Approaching task: {task_spec['task']}"
            )
            
            # Verify solution quality
            verification_score = self.verify_solution(task_spec, solution_result['final_solution'])
            
            # Update performance tracking
            performance_entry = {
                'iteration': iteration,
                'task': task_spec['task'],
                'domain': domain,
                'difficulty': difficulty,
                'solution_quality': solution_result['final_quality'],
                'verification_score': verification_score,
                'improvement_gained': verification_score - 0.5  # Baseline
            }
            
            training_log.append(performance_entry)
            self.performance_history.append(performance_entry)
            
            # Learn from this experience
            self.update_skills(task_spec, solution_result, verification_score)
        
        return {
            'iterations_completed': num_iterations,
            'training_log': training_log,
            'final_performance': self.assess_overall_performance(),
            'skills_learned': len(self.skill_inventory)
        }
    
    def adaptive_difficulty(self, iteration: int) -> float:
        """Adaptively set task difficulty based on performance"""
        if not self.performance_history:
            return 0.3  # Start easy
            
        # Recent performance average
        recent_scores = [p['verification_score'] for p in self.performance_history[-5:]]
        avg_recent = sum(recent_scores) / len(recent_scores)
        
        # Increase difficulty if performing well
        if avg_recent > 0.8:
            return min(0.9, 0.3 + iteration * 0.05)
        elif avg_recent > 0.6:
            return min(0.7, 0.3 + iteration * 0.03)
        else:
            return max(0.2, 0.3 + iteration * 0.01)  # Slow increase if struggling
    
    def select_domain(self, iteration: int) -> str:
        """Select domain for training based on performance gaps"""
        domains = ['math', 'coding', 'reasoning']
        
        if not self.performance_history:
            return domains[iteration % 3]  # Round-robin initially
            
        # Calculate domain-specific performance
        domain_performance = {}
        for domain in domains:
            domain_entries = [p for p in self.performance_history if p['domain'] == domain]
            if domain_entries:
                domain_performance[domain] = sum(p['verification_score'] for p in domain_entries) / len(domain_entries)
            else:
                domain_performance[domain] = 0.5
        
        # Focus on weakest domain
        return min(domain_performance.items(), key=lambda x: x[1])[0]
    
    def verify_solution(self, task_spec: Dict, solution: str) -> float:
        """Verify solution quality through execution or analysis"""
        base_score = 0.5
        
        # Check solution length and detail
        if len(solution.split()) > 20:
            base_score += 0.1
            
        # Check domain-specific criteria
        if task_spec['domain'] == 'math':
            if any(word in solution.lower() for word in ['calculate', 'result', 'answer']):
                base_score += 0.2
        elif task_spec['domain'] == 'coding':
            if any(word in solution.lower() for word in ['function', 'algorithm', 'implementation']):
                base_score += 0.2
        elif task_spec['domain'] == 'reasoning':
            if any(word in solution.lower() for word in ['because', 'therefore', 'analysis']):
                base_score += 0.2
        
        # Check for improvement indicators
        if '[Added:' in solution or '[Enhanced:' in solution:
            base_score += 0.1
            
        return min(base_score, 1.0)
    
    def update_skills(self, task_spec: Dict, solution_result: Dict, verification_score: float):
        """Update skill inventory based on successful solutions"""
        if verification_score > 0.7:  # Successful solution
            skill = {
                'domain': task_spec['domain'],
                'task_type': task_spec['task'][:50],
                'solution_pattern': solution_result['final_solution'][:100],
                'success_score': verification_score,
                'learned_iteration': len(self.performance_history)
            }
            self.skill_inventory.append(skill)
    
    def assess_overall_performance(self) -> Dict:
        """Assess overall learning progress"""
        if not self.performance_history:
            return {'status': 'no_training_data'}
            
        recent_performance = self.performance_history[-5:] if len(self.performance_history) >= 5 else self.performance_history
        
        return {
            'avg_recent_quality': sum(p['solution_quality'] for p in recent_performance) / len(recent_performance),
            'avg_verification_score': sum(p['verification_score'] for p in recent_performance) / len(recent_performance),
            'total_skills_learned': len(self.skill_inventory),
            'domains_covered': len(set(p['domain'] for p in self.performance_history)),
            'improvement_trend': 'improving' if len(recent_performance) > 1 and recent_performance[-1]['verification_score'] > recent_performance[0]['verification_score'] else 'stable'
        }

# Demo autonomous self-training
autonomous_trainer = AutonomousSelfTrainer()

print("\n=== Autonomous Self-Training System ===")
training_results = autonomous_trainer.self_training_loop(num_iterations=5)

print(f"Training completed: {training_results['iterations_completed']} iterations")
print(f"Skills learned: {training_results['skills_learned']}")
print(f"Final performance: {training_results['final_performance']}")

# Show training progression
print("\nTraining progression:")
for entry in training_results['training_log']:
    print(f"  Iter {entry['iteration']}: {entry['domain']} task "
          f"(difficulty: {entry['difficulty']:.2f}) -> "
          f"score: {entry['verification_score']:.3f}")
```

## ASCII Diagram: Self-Improvement Evolution

```
Self-Improvement Architecture Evolution:

Stage 1: Verbal Self-Correction
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│  Generate   │───►│  Critique   │───►│   Refine    │
│  Solution   │    │ (Prompted)  │    │ (Prompted)  │
└─────────────┘    └─────────────┘    └─────────────┘
       ▲                                      │
       └──────────── Iteration Loop ─────────┘
       
No gradient updates, ephemeral improvements

Stage 2: RL-Internalized Self-Correction
┌─────────────────────────────────────────────────────┐
│              Critic Policy π_critic(a|s)            │
│                                                     │
│  ┌─────────────┐              ┌─────────────┐      │
│  │  Quality    │              │Improvement  │      │
│  │Assessment   │              │Suggestion   │      │
│  │   Head      │              │   Head      │      │
│  └─────────────┘              └─────────────┘      │
└─────────────────────────────────────────────────────┘
         │                              │
         ▼                              ▼
┌─────────────┐              ┌─────────────────┐
│   Quality   │              │   Targeted      │
│  Scoring    │              │ Improvements    │
└─────────────┘              └─────────────────┘
         │                              │
         └──────── RL Training ─────────┘

Persistent learning through gradient updates

Stage 3: Autonomous Self-Training  
┌─────────────────────────────────────────────────────┐
│                Self-Training Loop                    │
└─────────────────────────────────────────────────────┘
         │
         ▼
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   Task      │───►│   Solve     │───►│   Verify    │
│ Generation  │    │    with     │    │  & Learn    │
│(Curriculum) │    │Self-Correct │    │  from       │
└─────────────┘    └─────────────┘    │ Execution   │
         ▲                             └─────────────┘
         │                                     │
         └────────── Update Skills ───────────┘

Fully autonomous improvement without supervision

Curriculum Generation Strategy:
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│Performance  │───►│ Difficulty  │───►│Domain Focus │
│  Analysis   │    │Adaptation   │    │Selection    │
└─────────────┘    └─────────────┘    └─────────────┘
         ▲                                     │
         │              ┌─────────────┐       │
         └──────────────┤Task         │◄──────┘
                        │Generation   │
                        └─────────────┘
```

## Training Self-Improvement Policies

```python
def train_self_improvement_system():
    """Complete training pipeline for self-improvement capabilities"""
    
    print("=== Self-Improvement Training Pipeline ===")
    
    # Stage 1: Train basic verbal self-correction
    verbal_agent = VerbalSelfCorrection()
    print("Stage 1: Verbal self-correction baseline established")
    
    # Stage 2: Train RL critic for internalization
    rl_agent = RLSelfCorrectionAgent()
    
    # Generate training experiences
    training_experiences = []
    for _ in range(50):
        task = f"Solve problem {_ % 10}"
        initial_solution = f"Basic approach to problem {_ % 10}"
        
        # Simulate improvement experience
        experience = {
            'task': task,
            'solution_before': initial_solution,
            'solution_after': initial_solution + " [Improved with details]",
            'quality_before': 0.4,
            'quality_after': 0.7,
            'improvement_effective': True,
            'improvement_type_idx': _ % len(rl_agent.improvement_types)
        }
        training_experiences.append(experience)
    
    # Train critic
    critic_loss = rl_agent.train_critic(training_experiences)
    print(f"Stage 2: RL critic trained, loss = {critic_loss:.4f}")
    
    # Stage 3: Deploy autonomous self-trainer
    autonomous_agent = AutonomousSelfTrainer()
    autonomous_agent.rl_corrector = rl_agent  # Use trained critic
    
    # Run autonomous training
    results = autonomous_agent.self_training_loop(num_iterations=10)
    print(f"Stage 3: Autonomous training completed")
    print(f"  Skills learned: {results['skills_learned']}")
    print(f"  Performance trend: {results['final_performance'].get('improvement_trend', 'unknown')}")
    
    return {
        'verbal_agent': verbal_agent,
        'rl_agent': rl_agent,
        'autonomous_agent': autonomous_agent,
        'training_results': results
    }

# Demo full training pipeline
print("Starting self-improvement training pipeline...")
# training_results = train_self_improvement_system()
print("Self-improvement system training complete")
```

## Key Differences Summary

| Stage | Approach | Persistence | Autonomy | Learning Mechanism |
|-------|----------|-------------|----------|-------------------|
| **Verbal** | Prompt-based reflection | Session-only | Human-guided | Template matching |
| **RL-Internalized** | Learned critic policies | Parameter updates | Semi-autonomous | Gradient-based RL |
| **Autonomous** | Self-generated curriculum | Continuous learning | Fully autonomous | Self-play + execution |

## Practical Exercises

```python
# Exercise 1: Build domain-specific self-corrector
def exercise_domain_corrector():
    """Create self-correction system for specific domain (e.g., code, math)"""
    pass

# Exercise 2: Implement verification environments
def exercise_verification_env():
    """Build environments for automatic solution verification"""
    pass

# Exercise 3: Design curriculum strategies
def exercise_curriculum_design():
    """Create adaptive curriculum generation for your use case"""
    pass
```

## Resources

- **Survey Reference**: [Section 3.4, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **Self-Refine Paper**: [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)
- **Reflexion Paper**: [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)
- **CRITIC Paper**: [CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](https://arxiv.org/abs/2305.11738)
- **Autonomous Learning**: [R-Zero: From Passive Generation to Active Learning](https://arxiv.org/abs/2309.09530)

## Next Steps

- **[3.5 Reasoning](3.5_Reasoning.md)**: Learn RL-enhanced deliberate reasoning and System 1/2 integration
- **Integration Practice**: Combine self-improvement with memory and planning systems
- **Advanced Topics**: Study self-play strategies and curriculum generation optimization

---

*Self-improvement becomes truly powerful when it moves beyond prompt engineering to learned, persistent policies that autonomously generate learning experiences and improve from environmental feedback.*
