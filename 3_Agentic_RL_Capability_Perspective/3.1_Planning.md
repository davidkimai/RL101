# 3.1 Planning

Planning transforms from static heuristic search to learned policies that decide *how to deliberate*. RL enables agents to learn when to plan deeply vs act quickly, which search strategies work best for different problems, and how to integrate planning with tool use and memory - creating adaptive meta-planning capabilities.

## Key Takeaways
- **Meta-Planning Learning**: RL optimizes the planning process itself, not just individual plans
- **Dynamic Depth Control**: Agents learn when to think fast vs slow based on problem complexity
- **Search Strategy Selection**: Learned policies choose between tree search, linear reasoning, or direct action
- **Integration Architecture**: Planning coordinates with tools, memory, and reasoning through shared optimization

## Prerequisites Check

```bash
# Verify planning libraries
python -c "import torch, transformers; print('Deep learning ready')"
python -c "import numpy as np; print('Search algorithms ready')"
python -c "import json, time; print('Planning data structures ready')"

# Conceptual check
echo "Do you understand basic search algorithms (BFS, DFS, A*)?"
echo "Are you familiar with tree search and Monte Carlo methods?"
echo "Have you completed Module 2 (RL foundations)?"
```

## Hands-On: Planning Approaches

### Traditional Heuristic Planning
```python
import json
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class PlanStep:
    action: str
    description: str
    expected_outcome: str
    confidence: float

class TraditionalPlanner:
    """Fixed heuristic planning - static search algorithms"""
    
    def __init__(self):
        self.search_depth = 5
        self.search_algorithm = "breadth_first"
        
    def create_plan(self, goal: str, current_state: Dict) -> List[PlanStep]:
        """Static planning with fixed heuristics"""
        
        # Fixed decomposition heuristics
        if "math" in goal.lower():
            return self.math_problem_template(goal)
        elif "research" in goal.lower():
            return self.research_template(goal)
        else:
            return self.generic_template(goal)
    
    def math_problem_template(self, goal: str) -> List[PlanStep]:
        """Fixed template for math problems"""
        return [
            PlanStep("understand", "Read and understand the problem", "Clear problem comprehension", 0.9),
            PlanStep("identify_operations", "Identify required mathematical operations", "Know what math to do", 0.8),
            PlanStep("calculate", "Perform calculations", "Get numerical answer", 0.7),
            PlanStep("verify", "Check the answer", "Confirmed correct result", 0.8)
        ]
    
    def research_template(self, goal: str) -> List[PlanStep]:
        """Fixed template for research tasks"""
        return [
            PlanStep("search", "Search for relevant information", "Find source materials", 0.6),
            PlanStep("analyze", "Analyze gathered information", "Understand key points", 0.7),
            PlanStep("synthesize", "Combine information into answer", "Complete response", 0.5)
        ]
    
    def generic_template(self, goal: str) -> List[PlanStep]:
        """Generic fallback template"""
        return [
            PlanStep("analyze", "Analyze the request", "Understand what's needed", 0.6),
            PlanStep("execute", "Take appropriate action", "Address the request", 0.5)
        ]

# Demo traditional planning
traditional = TraditionalPlanner()

problems = [
    "Solve: What is 15% of 240?",
    "Research the causes of climate change",
    "Help me learn Python"
]

print("=== Traditional Heuristic Planning ===")
for problem in problems:
    plan = traditional.create_plan(problem, {})
    print(f"\nProblem: {problem}")
    for i, step in enumerate(plan, 1):
        print(f"  {i}. {step.action}: {step.description} (confidence: {step.confidence})")
```

### Agentic RL Planning
```python
import torch
import torch.nn as nn
import numpy as np
from typing import Tuple

class PlanningPolicy(nn.Module):
    """Learned policy for planning decisions"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Planning depth policy
        self.depth_head = nn.Linear(hidden_dim, 5)  # 1-5 steps lookahead
        
        # Search strategy policy  
        self.strategy_head = nn.Linear(hidden_dim, 3)  # direct, linear, tree
        
        # Action selection policy
        self.action_head = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        features = self.state_encoder(state)
        
        depth_logits = self.depth_head(features)
        strategy_logits = self.strategy_head(features)
        action_logits = self.action_head(features)
        
        return depth_logits, strategy_logits, action_logits

class AgenticPlanner:
    """RL-based adaptive planning agent"""
    
    def __init__(self, state_dim: int = 10, action_dim: int = 8):
        self.policy = PlanningPolicy(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=0.001)
        
        self.action_vocab = [
            'understand', 'search', 'calculate', 'verify', 
            'synthesize', 'reason', 'tool_use', 'complete'
        ]
        
        self.strategy_vocab = ['direct', 'linear_chain', 'tree_search']
        
        # Experience buffer for RL training
        self.experiences = []
        
    def encode_state(self, goal: str, context: Dict) -> torch.Tensor:
        """Convert goal and context to state vector"""
        # Simplified encoding - in practice use embeddings
        features = [
            len(goal) / 100.0,  # Goal complexity
            1.0 if 'math' in goal.lower() else 0.0,
            1.0 if 'research' in goal.lower() else 0.0,
            1.0 if 'calculate' in goal.lower() else 0.0,
            context.get('time_pressure', 0.0),
            context.get('accuracy_importance', 0.5),
            len(context.get('available_tools', [])) / 10.0,
            context.get('memory_items', 0) / 20.0,
            context.get('user_expertise', 0.5),
            context.get('task_complexity', 0.5)
        ]
        return torch.tensor(features, dtype=torch.float32)
    
    def create_adaptive_plan(self, goal: str, context: Dict) -> Dict:
        """Create plan using learned planning policy"""
        state = self.encode_state(goal, context)
        
        with torch.no_grad():
            depth_logits, strategy_logits, action_logits = self.policy(state)
            
            # Sample planning decisions
            depth = torch.multinomial(torch.softmax(depth_logits, dim=0), 1).item() + 1
            strategy_idx = torch.multinomial(torch.softmax(strategy_logits, dim=0), 1).item()
            strategy = self.strategy_vocab[strategy_idx]
            
        # Generate plan based on learned decisions
        if strategy == 'direct':
            plan_steps = self.direct_action_plan(goal, action_logits)
        elif strategy == 'linear_chain':
            plan_steps = self.linear_chain_plan(goal, depth, action_logits)
        else:  # tree_search
            plan_steps = self.tree_search_plan(goal, depth, action_logits)
        
        return {
            'steps': plan_steps,
            'strategy': strategy,
            'depth': depth,
            'confidence': float(torch.max(torch.softmax(action_logits, dim=0)).item())
        }
    
    def direct_action_plan(self, goal: str, action_logits: torch.Tensor) -> List[PlanStep]:
        """Direct action without multi-step planning"""
        action_idx = torch.multinomial(torch.softmax(action_logits, dim=0), 1).item()
        action = self.action_vocab[action_idx]
        
        return [PlanStep(action, f"Directly {action} to address goal", "Goal completion", 0.8)]
    
    def linear_chain_plan(self, goal: str, depth: int, action_logits: torch.Tensor) -> List[PlanStep]:
        """Linear sequence of actions"""
        probs = torch.softmax(action_logits, dim=0)
        
        plan_steps = []
        for i in range(depth):
            # Sample without replacement for diversity
            action_idx = torch.multinomial(probs, 1).item()
            action = self.action_vocab[action_idx]
            
            plan_steps.append(PlanStep(
                action, 
                f"Step {i+1}: {action} towards goal",
                f"Progress after {action}",
                float(probs[action_idx].item())
            ))
            
            # Reduce probability of selected action
            probs[action_idx] *= 0.1
            probs = probs / probs.sum()  # Renormalize
            
        return plan_steps
    
    def tree_search_plan(self, goal: str, depth: int, action_logits: torch.Tensor) -> List[PlanStep]:
        """Tree search with backtracking possibilities"""
        # Simplified tree search - create main path with alternatives
        main_path = self.linear_chain_plan(goal, depth, action_logits)
        
        # Add branching step for complex goals
        if depth > 2:
            branch_step = PlanStep(
                'evaluate_and_branch',
                'Assess progress and consider alternative approaches',
                'Optimal path selection',
                0.7
            )
            main_path.insert(depth//2, branch_step)
            
        return main_path
    
    def train_planning_policy(self, experiences: List[Dict]) -> float:
        """Train planning policy using RL"""
        if len(experiences) < 5:
            return 0.0
            
        states = torch.stack([exp['state'] for exp in experiences])
        actions = torch.tensor([exp['action_taken'] for exp in experiences])
        rewards = torch.tensor([exp['reward'] for exp in experiences])
        
        # Policy gradient update
        depth_logits, strategy_logits, action_logits = self.policy(states)
        
        # Compute log probabilities  
        log_probs = torch.log_softmax(action_logits, dim=1)
        selected_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze()
        
        # Policy gradient loss
        loss = -(selected_log_probs * rewards).mean()
        
        # Update policy
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()

# Demo agentic planning
agentic = AgenticPlanner()

contexts = [
    {'time_pressure': 0.1, 'accuracy_importance': 0.9, 'available_tools': ['calculator'], 'task_complexity': 0.3},
    {'time_pressure': 0.8, 'accuracy_importance': 0.6, 'available_tools': ['search', 'memory'], 'task_complexity': 0.7},
    {'time_pressure': 0.5, 'accuracy_importance': 0.8, 'available_tools': [], 'task_complexity': 0.9}
]

print("\n=== Agentic RL Planning ===")
for i, (problem, context) in enumerate(zip(problems, contexts)):
    plan = agentic.create_adaptive_plan(problem, context)
    print(f"\nProblem {i+1}: {problem}")
    print(f"Strategy: {plan['strategy']}, Depth: {plan['depth']}, Confidence: {plan['confidence']:.3f}")
    for j, step in enumerate(plan['steps'], 1):
        print(f"  {j}. {step.action}: {step.description}")
```

## Planning Integration Patterns

### Tool-Integrated Planning
```python
class ToolAwarePlanner(AgenticPlanner):
    """Planning that coordinates with tool availability and costs"""
    
    def __init__(self, state_dim: int = 10, action_dim: int = 8):
        super().__init__(state_dim, action_dim)
        self.tool_costs = {
            'calculator': 0.1,
            'web_search': 0.5,
            'memory_retrieval': 0.2,
            'code_execution': 0.8
        }
        
    def plan_with_tools(self, goal: str, available_tools: List[str], budget: float = 1.0) -> Dict:
        """Create cost-aware plan with tool integration"""
        
        context = {
            'available_tools': available_tools,
            'tool_budget': budget,
            'task_complexity': self.estimate_complexity(goal)
        }
        
        # Get base plan from learned policy
        base_plan = self.create_adaptive_plan(goal, context)
        
        # Optimize tool usage within budget
        tool_optimized_plan = self.optimize_tool_usage(base_plan, available_tools, budget)
        
        return tool_optimized_plan
    
    def optimize_tool_usage(self, base_plan: Dict, available_tools: List[str], budget: float) -> Dict:
        """Optimize tool selection within budget constraints"""
        steps = base_plan['steps']
        optimized_steps = []
        remaining_budget = budget
        
        for step in steps:
            # Determine if step needs tool use
            tool_needed = self.determine_tool_need(step.action, step.description)
            
            if tool_needed and available_tools:
                # Select best tool within budget
                best_tool = self.select_best_tool(step, available_tools, remaining_budget)
                if best_tool:
                    remaining_budget -= self.tool_costs.get(best_tool, 0.1)
                    step.action = f"{step.action}_with_{best_tool}"
                    step.description += f" (using {best_tool})"
                    
            optimized_steps.append(step)
            
        base_plan['steps'] = optimized_steps
        base_plan['budget_used'] = budget - remaining_budget
        return base_plan
    
    def determine_tool_need(self, action: str, description: str) -> bool:
        """Determine if action requires tool use"""
        tool_indicators = {
            'calculate': True,
            'search': True,
            'verify': True,
            'understand': False,
            'synthesize': False
        }
        return tool_indicators.get(action, False)
    
    def select_best_tool(self, step: PlanStep, available_tools: List[str], budget: float) -> Optional[str]:
        """Select best tool for step within budget"""
        suitable_tools = []
        
        for tool in available_tools:
            cost = self.tool_costs.get(tool, 0.1)
            if cost <= budget:
                effectiveness = self.estimate_tool_effectiveness(step, tool)
                suitable_tools.append((tool, effectiveness, cost))
        
        if not suitable_tools:
            return None
            
        # Select highest effectiveness tool
        return max(suitable_tools, key=lambda x: x[1])[0]
    
    def estimate_tool_effectiveness(self, step: PlanStep, tool: str) -> float:
        """Estimate tool effectiveness for step"""
        effectiveness_map = {
            'calculate': {'calculator': 0.9, 'web_search': 0.3, 'memory_retrieval': 0.2},
            'search': {'web_search': 0.9, 'memory_retrieval': 0.7, 'calculator': 0.1},
            'verify': {'calculator': 0.8, 'memory_retrieval': 0.6, 'web_search': 0.7}
        }
        return effectiveness_map.get(step.action, {}).get(tool, 0.1)
    
    def estimate_complexity(self, goal: str) -> float:
        """Estimate task complexity"""
        complexity_indicators = {
            'calculate': 0.3,
            'research': 0.7,
            'analyze': 0.8,
            'multiple': 0.9
        }
        
        goal_lower = goal.lower()
        for indicator, complexity in complexity_indicators.items():
            if indicator in goal_lower:
                return complexity
        return 0.5

# Demo tool-integrated planning
tool_planner = ToolAwarePlanner()

print("\n=== Tool-Integrated Planning ===")
available_tools = ['calculator', 'web_search', 'memory_retrieval']
budget = 1.5

for problem in problems:
    plan = tool_planner.plan_with_tools(problem, available_tools, budget)
    print(f"\nProblem: {problem}")
    print(f"Strategy: {plan['strategy']}, Budget used: {plan['budget_used']:.2f}")
    for i, step in enumerate(plan['steps'], 1):
        print(f"  {i}. {step.action}: {step.description}")
```

## ASCII Diagram: Planning Architecture Evolution

```
Traditional Fixed Planning:
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│    Goal     │───►│ Fixed       │───►│  Static     │
│    Input    │    │ Template    │    │  Plan       │
└─────────────┘    │ Selection   │    │ Output      │
                   └─────────────┘    └─────────────┘
                          │
                   Limited Adaptation

Agentic RL Planning:
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│ Goal +      │───►│ Planning    │───►│ Adaptive    │
│ Context     │    │ Policy      │    │ Plan        │
│ State       │    │ π(a|s)      │    │ Strategy    │
└─────────────┘    └─────────────┘    └─────────────┘
                          │                   │
                          ▼                   │
                   ┌─────────────┐           │
                   │    RL       │           │
                   │  Training   │           │
                   │ Experience  │◄──────────┘
                   └─────────────┘

Meta-Planning Decisions:
           ┌─────────────────────────┐
           │    Planning Policy      │
           │       π_plan(a|s)       │
           └─────────────────────────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
        ▼               ▼               ▼
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   Depth     │ │  Strategy   │ │   Tool      │
│ Selection   │ │ Selection   │ │ Selection   │ 
│  (1-5)      │ │(direct/tree)│ │ (budget)    │
└─────────────┘ └─────────────┘ └─────────────┘
```

## Training Planning Policies

### Planning RL Training Loop
```python
def train_planning_system():
    """Complete training loop for planning policies"""
    
    planner = ToolAwarePlanner()
    
    # Training configuration
    episodes = 100
    batch_size = 10
    
    training_tasks = [
        "Calculate compound interest on $1000 at 5% for 3 years",
        "Research the impact of AI on job markets",
        "Debug this Python code that's not working",
        "Plan a 7-day vacation to Japan under $3000",
        "Explain quantum computing to a high school student"
    ]
    
    print("=== Planning Policy Training ===")
    
    for episode in range(episodes):
        episode_experiences = []
        
        for task in training_tasks:
            # Sample context variation
            context = {
                'time_pressure': np.random.uniform(0.1, 0.9),
                'accuracy_importance': np.random.uniform(0.3, 1.0),
                'available_tools': np.random.choice(['calculator', 'web_search', 'memory'], 
                                                  size=np.random.randint(1, 4), replace=False).tolist(),
                'task_complexity': np.random.uniform(0.2, 0.8)
            }
            
            # Generate plan
            state = planner.encode_state(task, context)
            plan = planner.create_adaptive_plan(task, context)
            
            # Simulate plan execution and reward
            reward = simulate_plan_execution(plan, context)
            
            # Store experience
            experience = {
                'state': state,
                'action_taken': 0,  # Simplified for demo
                'reward': reward,
                'task': task,
                'context': context
            }
            episode_experiences.append(experience)
        
        # Train on batch of experiences
        if len(episode_experiences) >= batch_size:
            loss = planner.train_planning_policy(episode_experiences)
            
            if episode % 20 == 0:
                print(f"Episode {episode}: Loss = {loss:.4f}")

def simulate_plan_execution(plan: Dict, context: Dict) -> float:
    """Simulate plan execution and return reward"""
    base_reward = 1.0
    
    # Reward based on plan quality
    if plan['strategy'] == 'tree_search' and context['task_complexity'] > 0.7:
        base_reward += 2.0  # Good strategy for complex tasks
    
    if plan['depth'] >= 3 and context['accuracy_importance'] > 0.8:
        base_reward += 1.5  # Deep planning for high accuracy needs
        
    if 'budget_used' in plan and plan['budget_used'] <= context.get('tool_budget', 1.0):
        base_reward += 1.0  # Budget compliance
    
    # Add noise for realism
    return base_reward + np.random.normal(0, 0.3)

# Run training demonstration
print("Starting planning policy training...")
# train_planning_system()  # Uncommented for demo
print("Training complete - planning policies optimized for task context")
```

## Key Differences Summary

| Aspect | Traditional Planning | Agentic RL Planning |
|--------|---------------------|---------------------|
| **Strategy Selection** | Fixed heuristics | Learned policies |
| **Depth Control** | Static preset | Adaptive based on context |
| **Tool Integration** | Rule-based | Cost-optimized selection |
| **Adaptation** | Template matching | Continuous learning |
| **Context Sensitivity** | Limited | Full state consideration |
| **Meta-Planning** | None | Learns how to plan |

## Practical Exercises

```python
# Exercise 1: Implement planning depth optimization
def exercise_depth_optimization():
    """Train policy to select optimal planning depth"""
    pass

# Exercise 2: Build tree search integration
def exercise_tree_search():
    """Integrate MCTS with learned planning policies"""
    pass

# Exercise 3: Design domain-specific planner
def exercise_domain_planner():
    """Create specialized planner for your application domain"""
    pass
```

## Resources

- **Survey Reference**: [Section 3.1, arXiv:2509.02547](https://arxiv.org/abs/2509.02547)
- **RAP Paper**: [Reasoning via Planning](https://arxiv.org/abs/2305.14992)
- **LATS Paper**: [Language Agent Tree Search](https://arxiv.org/abs/2310.04406)
- **Tree of Thoughts**: [Deliberate Problem Solving with LLMs](https://arxiv.org/abs/2305.10601)
- **MCTS Integration**: [Monte Carlo Tree Search with Neural Networks](https://www.nature.com/articles/nature16961)

## Next Steps

- **[3.2 Tool Using](3.2_Tool_Using.md)**: Learn RL-optimized tool selection and execution
- **Integration Practice**: Combine planning with tool policies from next module
- **Advanced Topics**: Study tree search integration and multi-step verification

---

*Planning becomes truly intelligent when it learns how to plan. RL transforms static search heuristics into adaptive policies that consider context, resources, and goals to determine optimal deliberation strategies.*
